"""
System Router - ì‹œìŠ¤í…œ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import Dict, Any, List
from pydantic import BaseModel

from app.api_server.dependencies import get_chatbot_service
from app.api_server.services.chatbot_service import ChatbotService
from app.utils.config import OLLAMA_MODEL, DEBUG_MODE
import httpx
import asyncio
from app.utils.ollama_manager import ensure_single_model_running
from app.utils.prompt_manager import get_prompt_manager
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# ê³µí†µ í•¨ìˆ˜
async def _get_ollama_models() -> List[str]:
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        async with httpx.AsyncClient(timeout=3.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            response.raise_for_status()
            data = response.json()
            
            # ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ ì¶”ê°€
            filtered_models = []
            for model in data.get("models", []):
                model_name = model["name"]
                model_lower = model_name.lower()
                
                # ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ (embed, embedding, mxbai í‚¤ì›Œë“œ í¬í•¨ ëª¨ë¸ ì œì™¸)
                if not any(keyword in model_lower for keyword in ['embed', 'embedding', 'mxbai']):
                    filtered_models.append(model_name)
            
            return filtered_models
    except Exception:
        # ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ìœ¼ë¡œ í´ë°±
        return ["gemma3:latest", "llama3.2:latest", "mistral:latest"]

async def _get_ollama_running_models() -> List[Dict[str, Any]]:
    """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        async with httpx.AsyncClient(timeout=3.0) as client:
            response = await client.get("http://localhost:11434/api/ps")
            response.raise_for_status()
            data = response.json()
            return data.get("models", [])
    except Exception:
        return []

async def _get_ollama_model_details() -> Dict[str, Any]:
    """ì „ì²´ Ollama ëª¨ë¸ ì •ë³´ ì¡°íšŒ (ì„¤ì¹˜ëœ ëª¨ë¸ + ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)"""
    try:
        available_models = []
        running_models = []
        available_embedding_models = []
        running_embedding_models = []
        
        # ì„ë² ë”© ëª¨ë¸ íŒë³„ í•¨ìˆ˜
        def is_embedding_model(model_name: str) -> bool:
            model_lower = model_name.lower()
            return any(keyword in model_lower for keyword in ['embed', 'embedding', 'mxbai'])
        
        # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
        async with httpx.AsyncClient(timeout=3.0) as client:
            # ì„¤ì¹˜ëœ ëª¨ë¸
            tags_response = await client.get("http://localhost:11434/api/tags")
            if tags_response.status_code == 200:
                tags_data = tags_response.json()
                for model in tags_data.get("models", []):
                    model_name = model["name"]
                    
                    model_info = {
                        "name": model_name,
                        "size": model.get("size", 0),
                        "modified_at": model.get("modified_at", ""),
                        "digest": model.get("digest", "")[:12] + "...",
                        "parameter_size": model.get("details", {}).get("parameter_size", "Unknown")
                    }
                    
                    # ì„ë² ë”© ëª¨ë¸ê³¼ ì±„íŒ… ëª¨ë¸ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
                    if is_embedding_model(model_name):
                        available_embedding_models.append(model_info)
                    else:
                        available_models.append(model_info)
            
            # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸
            ps_response = await client.get("http://localhost:11434/api/ps")
            if ps_response.status_code == 200:
                ps_data = ps_response.json()
                for model in ps_data.get("models", []):
                    model_name = model["name"]
                    
                    model_info = {
                        "name": model_name,
                        "size_vram": model.get("size_vram", 0),
                        "expires_at": model.get("expires_at", ""),
                        "is_running": True
                    }
                    
                    # ì„ë² ë”© ëª¨ë¸ê³¼ ì±„íŒ… ëª¨ë¸ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
                    if is_embedding_model(model_name):
                        running_embedding_models.append(model_info)
                    else:
                        running_models.append(model_info)
        
        return {
            "available": available_models,
            "running": running_models,
            "available_embedding_models": available_embedding_models,
            "running_embedding_models": running_embedding_models,
            "status": "success",
            "available_models": available_models,  # í˜¸í™˜ì„± ìœ ì§€
            "running_models": running_models,  # í˜¸í™˜ì„± ìœ ì§€
            "total_available": len(available_models) + len(available_embedding_models),
            "total_running": len(running_models) + len(running_embedding_models),
            "chat_models_running": len(running_models),
            "embedding_models_running": len(running_embedding_models)
        }
    except Exception as e:
        return {
            "available": [],
            "running": [],
            "available_embedding_models": [],
            "running_embedding_models": [],
            "status": "error",
            "error": str(e),
            "available_models": [],  # í˜¸í™˜ì„± ìœ ì§€
            "running_models": [],  # í˜¸í™˜ì„± ìœ ì§€
            "total_available": 0,
            "total_running": 0,
            "chat_models_running": 0,
            "embedding_models_running": 0
        }

class SystemInfo(BaseModel):
    """ì‹œìŠ¤í…œ ì •ë³´ ëª¨ë¸"""
    version: str = "2.0.0"
    model: str
    mode: str
    mcp_enabled: bool
    debug: bool
    available_models: List[str]
    available_prompts: List[str]

class ModelChangeRequest(BaseModel):
    """ëª¨ë¸ ë³€ê²½ ìš”ì²­"""
    model: str
    session_id: str = "default"

class PromptChangeRequest(BaseModel):
    """í”„ë¡¬í”„íŠ¸ ë³€ê²½ ìš”ì²­"""
    prompt_type: str
    session_id: str = "default"

@router.get("/info", 
    response_model=SystemInfo,
    summary="ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ",
    description="""
## ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ

GAIA-BT API ì„œë²„ì˜ í˜„ì¬ ìƒíƒœì™€ ì„¤ì •ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

### ë°˜í™˜ ì •ë³´
- **version**: API ë²„ì „
- **model**: í˜„ì¬ í™œì„± AI ëª¨ë¸
- **mode**: ì‘ë™ ëª¨ë“œ (normal/deep_research)
- **mcp_enabled**: Database ê²€ìƒ‰ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
- **debug**: ë””ë²„ê·¸ ëª¨ë“œ ìƒíƒœ
- **available_models**: ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡
- **available_prompts**: ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

### ì‚¬ìš© ì˜ˆì‹œ
ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ í˜„ì¬ ì„¤ì •ì„ íŒŒì•…í•˜ê³ , ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""",
    responses={
        200: {
            "description": "ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "version": "3.60.0",
                        "model": "gemma3-12b:latest",
                        "mode": "normal",
                        "mcp_enabled": True,
                        "debug": False,
                        "available_models": [
                            "gemma3-12b:latest",
                            "txgemma-chat:latest",
                            "llama3.2:latest"
                        ],
                        "available_prompts": [
                            "default",
                            "clinical",
                            "research",
                            "chemistry",
                            "regulatory"
                        ]
                    }
                }
            }
        }
    }
)
async def get_system_info(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    prompt_manager = get_prompt_manager()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë™ì  ì¡°íšŒ
    available_models = await _get_ollama_models()
    
    return {
        "version": "2.0.0",
        "model": service.current_model,
        "mode": service.current_mode,
        "mcp_enabled": service.mcp_enabled,
        "debug": service.debug_mode,
        "available_models": available_models,
        # templates í”„ë¡œí¼í‹° AttributeError ë“± ëª¨ë“  ì˜ˆì™¸ ë°©ì§€
        "available_prompts": (lambda: (
            list(prompt_manager.templates.keys()) if hasattr(prompt_manager, 'templates') and isinstance(prompt_manager.templates, dict)
            else []
        ))()
    }

@router.post("/model",
    summary="ğŸ¤– AI ëª¨ë¸ ë³€ê²½",
    description="""
## AI ëª¨ë¸ ë³€ê²½

í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ AI ëª¨ë¸ì„ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

### ì‘ë™ ë°©ì‹
1. ìš”ì²­ëœ ëª¨ë¸ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
2. ê¸°ì¡´ ëª¨ë¸ì„ ì¤‘ì§€í•˜ê³  ìƒˆ ëª¨ë¸ ì‹œì‘
3. ChatbotServiceì˜ ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸
4. ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ í™•ì¸ ë° ë°˜í™˜

### ì§€ì› ëª¨ë¸
- **gemma3-12b:latest**: 12B íŒŒë¼ë¯¸í„°, ê¸°ë³¸ ì¶”ì²œ ëª¨ë¸
- **txgemma-chat:latest**: ì±„íŒ… ìµœì í™” ëª¨ë¸
- **llama3.2:latest**: Metaì˜ ìµœì‹  LLaMA ëª¨ë¸
- **mistral:latest**: Mistral AI ëª¨ë¸

### âš ï¸ ì£¼ì˜ì‚¬í•­
- ëª¨ë¸ ë³€ê²½ ì‹œ ê¸°ì¡´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì´ˆê¸°í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- í° ëª¨ë¸ì€ ë¡œë”©ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- GPU ë©”ëª¨ë¦¬ ì œí•œì— ë”°ë¼ ì¼ë¶€ ëª¨ë¸ì€ ì‹¤í–‰ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
""",
    responses={
        200: {
            "description": "ëª¨ë¸ ë³€ê²½ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "success": True,
                        "model": "gemma3-12b:latest",
                        "model_running": True,
                        "message": "ëª¨ë¸ì´ 'gemma3-12b:latest'ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"
                    }
                }
            }
        },
        400: {
            "description": "ì˜ëª»ëœ ìš”ì²­",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "ëª¨ë¸ 'invalid-model'ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"
                    }
                }
            }
        },
        500: {
            "description": "ì„œë²„ ì˜¤ë¥˜",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "Ollama ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: Connection error"
                    }
                }
            }
        }
    }
)
async def change_model(
    request: ModelChangeRequest,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """AI ëª¨ë¸ ë³€ê²½ ë° Ollama ì‹¤í–‰ ë³´ì¥"""
    chatbot = service.get_session(request.session_id)
    if not chatbot:
        raise HTTPException(404, f"ì„¸ì…˜ {request.session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        # ë¨¼ì € ìš”ì²­ëœ ëª¨ë¸ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        available_models = await _get_ollama_models()
        if request.model not in available_models:
            raise HTTPException(400, f"ëª¨ë¸ '{request.model}'ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì¹˜ëœ ëª¨ë¸: {available_models}")
        
        # 1) Ollamaì—ì„œ ëª¨ë¸ ì‹¤í–‰ ë³´ì¥ (ê¸°ì¡´ ëª¨ë¸ ìœ ì§€)
        try:
            from app.utils.ollama_manager import start_model, list_running_models
            
            # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸
            running_models = await list_running_models()
            logger.info(f"í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_models}")
            
            # ìš”ì²­ëœ ëª¨ë¸ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            if request.model not in running_models:
                logger.info(f"ëª¨ë¸ '{request.model}' ì‹œì‘ ì¤‘... (ë‹¤ë¥¸ ëª¨ë¸ì€ ìœ ì§€)")
                await start_model(request.model)
            else:
                logger.info(f"ëª¨ë¸ '{request.model}' ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                
        except Exception as proc_err:
            logger.error(f"ëª¨ë¸ ì‹œì‘ ì‹¤íŒ¨: {proc_err}")
            raise HTTPException(500, f"Ollama ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {proc_err}")
        
        # 2) Chatbot ë‚´ë¶€ ëª¨ë¸ êµì²´
        result = await chatbot.change_model(request.model)
        
        # 3) ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ ì¬í™•ì¸
        running_models = await _get_ollama_running_models()
        model_running = any(model["name"] == request.model for model in running_models)
        
        return {
            "success": True,
            "model": chatbot.client.model_name,
            "model_running": model_running,
            "message": result
        }
    except Exception as e:
        raise HTTPException(400, str(e))

@router.post("/prompt")
async def change_prompt(
    request: PromptChangeRequest,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """í”„ë¡¬í”„íŠ¸ íƒ€ì… ë³€ê²½"""
    chatbot = service.get_session(request.session_id)
    if not chatbot:
        raise HTTPException(404, f"ì„¸ì…˜ {request.session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        # ë‘ ê°€ì§€ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ëª¨ë‘ ë¦¬ë¡œë“œ
        # 1. ë ˆê±°ì‹œ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ë¦¬ë¡œë“œ
        from app.utils.prompt_manager import get_prompt_manager
        prompt_manager = get_prompt_manager()
        prompt_manager.reload_prompts()
        
        # 2. ì‹ ê·œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§¤ë‹ˆì € ë¦¬ë¡œë“œ
        from app.utils.prompt_template_manager import reload_prompt_manager
        template_manager = reload_prompt_manager()
        template_manager.reload_all_prompts()
        
        result = await chatbot.change_prompt(request.prompt_type)
        return {
            "success": True,
            "prompt_type": chatbot.current_prompt_type,
            "message": result,
            "reload_status": "í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¦¬ë¡œë“œ ì™„ë£Œ"
        }
    except Exception as e:
        raise HTTPException(400, str(e))

@router.post("/prompts/reload",
    summary="ğŸ”„ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¦¬ë¡œë“œ",
    description="""
## í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì‹¤ì‹œê°„ ë¦¬ë¡œë“œ

ëª¨ë“  í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ë””ìŠ¤í¬ì—ì„œ ë‹¤ì‹œ ì½ì–´ì™€ ë³€ê²½ì‚¬í•­ì„ ì¦‰ì‹œ ë°˜ì˜í•©ë‹ˆë‹¤.

### ê¸°ëŠ¥
- ë ˆê±°ì‹œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ (prompt_*.txt) ë¦¬ë¡œë“œ
- ì‹ ê·œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¡°í•© íŒŒì¼ ë¦¬ë¡œë“œ
- í”„ë¡¬í”„íŠ¸ ìºì‹œ ë¬´íš¨í™”

### ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
- í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ìˆ˜ì •í•œ í›„ ì„œë²„ ì¬ì‹œì‘ ì—†ì´ ì ìš©í•˜ê³  ì‹¶ì„ ë•Œ
- ìƒˆë¡œìš´ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•œ í›„ ì¦‰ì‹œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ
- í”„ë¡¬í”„íŠ¸ ë³€ê²½ì‚¬í•­ì´ ë°˜ì˜ë˜ì§€ ì•Šì„ ë•Œ ìˆ˜ë™ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨í•  ë•Œ
""",
    responses={
        200: {
            "description": "í”„ë¡¬í”„íŠ¸ ë¦¬ë¡œë“œ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "success": True,
                        "message": "ëª¨ë“  í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¦¬ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤",
                        "reloaded_files": 15,
                        "template_manager_reloaded": True,
                        "legacy_manager_reloaded": True
                    }
                }
            }
        }
    }
)
async def reload_prompts(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ëª¨ë“  í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¦¬ë¡œë“œ"""
    try:
        reload_results = {}
        
        # 1. ë ˆê±°ì‹œ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ë¦¬ë¡œë“œ
        try:
            from app.utils.prompt_manager import get_prompt_manager
            prompt_manager = get_prompt_manager()
            prompt_manager.reload_prompts()
            reload_results["legacy_manager_reloaded"] = True
            reload_results["legacy_prompts"] = len(prompt_manager.prompts)
        except Exception as e:
            logger.error(f"ë ˆê±°ì‹œ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ë¦¬ë¡œë“œ ì‹¤íŒ¨: {e}")
            reload_results["legacy_manager_reloaded"] = False
            reload_results["legacy_error"] = str(e)
        
        # 2. ì‹ ê·œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§¤ë‹ˆì € ë¦¬ë¡œë“œ
        try:
            from app.utils.prompt_template_manager import reload_prompt_manager
            template_manager = reload_prompt_manager()
            template_manager.reload_all_prompts()
            reload_results["template_manager_reloaded"] = True
        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§¤ë‹ˆì € ë¦¬ë¡œë“œ ì‹¤íŒ¨: {e}")
            reload_results["template_manager_reloaded"] = False
            reload_results["template_error"] = str(e)
        
        # 3. ChatbotServiceì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§¤ë‹ˆì €ë„ ì—…ë°ì´íŠ¸
        try:
            service.prompt_template_manager = template_manager
            reload_results["service_updated"] = True
        except Exception as e:
            logger.error(f"ChatbotService í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            reload_results["service_updated"] = False
            reload_results["service_error"] = str(e)
        
        success = reload_results.get("legacy_manager_reloaded", False) or reload_results.get("template_manager_reloaded", False)
        
        return {
            "success": success,
            "message": "í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¦¬ë¡œë“œ ì™„ë£Œ" if success else "í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¦¬ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            **reload_results
        }
        
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ë¦¬ë¡œë“œ API ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": f"í”„ë¡¬í”„íŠ¸ ë¦¬ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        }

@router.post("/debug")
async def toggle_debug(
    session_id: str = "default",
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€"""
    result = await service.process_command("/debug", session_id)
    return result

@router.get("/models/detailed",
    summary="ğŸ¤– ëª¨ë¸ ìƒíƒœ ìƒì„¸ ì¡°íšŒ ë° ìë™ ì‹œì‘",
    description="""
## Ollama ëª¨ë¸ ìƒíƒœ ìƒì„¸ ì¡°íšŒ

ì„¤ì¹˜ëœ ëª¨ë¸ê³¼ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

### ë°˜í™˜ ì •ë³´
- **available_models**: ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ (ì´ë¦„, í¬ê¸°, íŒŒë¼ë¯¸í„° ìˆ˜ ë“±)
- **running_models**: í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡
- **default_model**: ê¸°ë³¸ ì„¤ì • ëª¨ë¸
- **auto_started**: ìë™ìœ¼ë¡œ ì‹œì‘ëœ ëª¨ë¸ ì •ë³´
- **error**: ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€
""",
    responses={
        200: {
            "description": "ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "available_models": [
                            {
                                "name": "gemma3-12b:latest",
                                "size": 7825332736,
                                "modified_at": "2024-11-20T08:15:00Z",
                                "digest": "e7cdf2014791...",
                                "parameter_size": "12B"
                            }
                        ],
                        "running_models": [
                            {
                                "name": "gemma3-12b:latest",
                                "size_vram": 7825332736,
                                "expires_at": "2024-12-30T10:00:00Z"
                            }
                        ],
                        "default_model": "gemma3-12b:latest",
                        "auto_started": "gemma3-12b:latest"
                    }
                }
            }
        }
    }
)
async def get_models_detailed(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ëª¨ë¸ ìƒíƒœ ìƒì„¸ ì¡°íšŒ ë° ìë™ ì‹œì‘"""
    try:
        # ëª¨ë¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        model_info = await _get_ollama_model_details()
        logger.info(f"_get_ollama_model_details ë°˜í™˜ê°’: {model_info}")
        
        # apiClientê°€ ì‚¬ìš©í•˜ëŠ” í‚¤ë¡œ ë³€í™˜
        available_models = model_info.get("available_models", model_info.get("available", []))
        running_models = model_info.get("running_models", model_info.get("running", []))
        available_embedding_models = model_info.get("available_embedding_models", [])
        running_embedding_models = model_info.get("running_embedding_models", [])
        
        response = {
            "available": available_models,
            "running": running_models,
            "available_embedding_models": available_embedding_models,
            "running_embedding_models": running_embedding_models,
            "default_model": OLLAMA_MODEL,
            "total_available": model_info.get("total_available", len(available_models)),
            "total_running": model_info.get("total_running", len(running_models)),
            "chat_models_running": model_info.get("chat_models_running", len(running_models)),
            "embedding_models_running": model_info.get("embedding_models_running", len(running_embedding_models)),
            "current_model_running": any(m["name"] == OLLAMA_MODEL for m in running_models)
        }
        
        # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ìë™ ì‹œì‘
        if not response["running"]:
            logger.info(f"ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìŒ. ê¸°ë³¸ ëª¨ë¸ '{OLLAMA_MODEL}' ìë™ ì‹œì‘")
            try:
                # Ollama ëª¨ë¸ ì‹œì‘
                from app.utils.ollama_manager import start_model
                await start_model(OLLAMA_MODEL)
                
                # ì ì‹œ ëŒ€ê¸° í›„ ë‹¤ì‹œ í™•ì¸
                await asyncio.sleep(2)
                updated_info = await _get_ollama_model_details()
                updated_running = updated_info.get("running_models", updated_info.get("running", []))
                response["running"] = updated_running
                response["total_running"] = len(updated_running)
                response["current_model_running"] = any(m["name"] == OLLAMA_MODEL for m in updated_running)
                response["auto_started"] = OLLAMA_MODEL
                
                logger.info(f"ëª¨ë¸ '{OLLAMA_MODEL}' ìë™ ì‹œì‘ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ìë™ ì‹œì‘ ì‹¤íŒ¨: {e}")
                response["auto_start_error"] = str(e)
        
        logger.info(f"ìµœì¢… ë°˜í™˜ response: {response}")
        return response
        
    except Exception as e:
        import traceback
        logger.error(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {
            "available": [],
            "running": [],
            "default_model": OLLAMA_MODEL,
            "total_available": 0,
            "total_running": 0,
            "current_model_running": False,
            "error": str(e),
            "available_models": [],  # í˜¸í™˜ì„±
            "running_models": []  # í˜¸í™˜ì„±
        }

@router.post("/mode/{mode}")
async def change_mode(
    mode: str,
    session_id: str = "default",
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ëª¨ë“œ ë³€ê²½ (normal/deep_research)"""
    if mode not in ["normal", "deep_research"]:
        raise HTTPException(400, "ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë“œì…ë‹ˆë‹¤")
    
    if mode == "normal":
        result = await service.process_command("/normal", session_id)
    else:
        result = await service.process_command("/mcp start", session_id)
    
    return result

@router.get("/models")
async def get_available_models(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    models = await _get_ollama_models()
    
    return {
        "models": models,
        "default": service.current_model,
        "status": "success"
    }

# ì¤‘ë³µ ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨ - ìœ„ì˜ get_models_detailed í•¨ìˆ˜ê°€ ë™ì¼í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

@router.get("/models/running")
async def get_running_models() -> Dict[str, Any]:
    """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    running_models = await _get_ollama_running_models()
    
    return {
        "running_models": running_models,
        "count": len(running_models),
        "status": "success"
    }

@router.post("/models/{model_name}/start")
async def start_model(
    model_name: str,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """íŠ¹ì • ëª¨ë¸ ì‹œì‘ (ê¸°ì¡´ ëª¨ë¸ë“¤ ìë™ ì¤‘ì§€)"""
    try:
        from urllib.parse import unquote
        from app.utils.ollama_manager import ensure_single_model_running, list_running_models
        
        # URL ë””ì½”ë”© (FastAPIê°€ ìë™ìœ¼ë¡œ í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬)
        decoded_model_name = unquote(model_name)
        logger.info(f"ğŸš€ ëª¨ë¸ ì‹œì‘ ìš”ì²­: ì›ë³¸={model_name}, ë””ì½”ë”©={decoded_model_name}")
        
        # ì‹œì‘ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤ í™•ì¸
        running_before = await list_running_models()
        logger.info(f"ğŸ“‹ ì‹œì‘ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_before}")
        
        # ê¸°ì¡´ ëª¨ë¸ë“¤ ì¤‘ì§€í•˜ê³  ìƒˆ ëª¨ë¸ë§Œ ì‹¤í–‰ (ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰ ë³´ì¥)
        await ensure_single_model_running(decoded_model_name)
        
        # ê²°ê³¼ í™•ì¸
        running_after = await list_running_models()
        logger.info(f"âœ… ì‹œì‘ í›„ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_after}")
        
        # ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if decoded_model_name in running_after:
            # ChatbotServiceì˜ í˜„ì¬ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if hasattr(service, 'update_current_model'):
                service.update_current_model(decoded_model_name)
            
            return {
                "success": True,
                "message": f"ëª¨ë¸ '{decoded_model_name}' ì‹œì‘ ì™„ë£Œ (ê¸°ì¡´ ëª¨ë¸ë“¤ ì¤‘ì§€ë¨)",
                "model": decoded_model_name,
                "running_models": running_after,
                "stopped_models": [m for m in running_before if m != decoded_model_name]
            }
        else:
            return {
                "success": False,
                "error": f"ëª¨ë¸ '{decoded_model_name}' ì‹œì‘ë˜ì§€ ì•ŠìŒ",
                "model": decoded_model_name,
                "running_models": running_after
            }
                
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": f"ëª¨ë¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "model": model_name
        }

@router.post("/models/{model_name}/stop")
async def stop_model(
    model_name: str,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """íŠ¹ì • ëª¨ë¸ ì¤‘ì§€"""
    try:
        from urllib.parse import unquote
        from app.utils.ollama_manager import list_running_models
        import subprocess
        import shlex
        
        # URL ë””ì½”ë”©
        decoded_model_name = unquote(model_name)
        logger.info(f"ğŸ›‘ ëª¨ë¸ ì¤‘ì§€ ìš”ì²­: ì›ë³¸={model_name}, ë””ì½”ë”©={decoded_model_name}")
        
        # ì¤‘ì§€ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤ í™•ì¸
        running_before = await list_running_models()
        logger.info(f"ğŸ“‹ ì¤‘ì§€ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_before}")
        
        if decoded_model_name not in running_before:
            return {
                "success": True,
                "message": f"ëª¨ë¸ '{decoded_model_name}'ì€ ì´ë¯¸ ì¤‘ì§€ëœ ìƒíƒœì…ë‹ˆë‹¤",
                "model": decoded_model_name,
                "running_models": running_before
            }
        
        # Ollama CLIë¥¼ í†µí•´ ëª¨ë¸ ì¤‘ì§€
        cmd = f"ollama stop {shlex.quote(decoded_model_name)}"
        logger.info(f"ğŸ”§ ì¤‘ì§€ ëª…ë ¹ ì‹¤í–‰: {cmd}")
        
        process = await asyncio.create_subprocess_shell(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        # ê²°ê³¼ í™•ì¸
        running_after = await list_running_models()
        logger.info(f"âœ… ì¤‘ì§€ í›„ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_after}")
        
        if decoded_model_name not in running_after:
            return {
                "success": True,
                "message": f"ëª¨ë¸ '{decoded_model_name}' ì¤‘ì§€ ì™„ë£Œ",
                "model": decoded_model_name,
                "running_models": running_after
            }
        else:
            return {
                "success": False,
                "error": f"ëª¨ë¸ '{decoded_model_name}' ì¤‘ì§€ ì‹¤íŒ¨",
                "model": decoded_model_name,
                "running_models": running_after,
                "stdout": stdout.decode() if stdout else "",
                "stderr": stderr.decode() if stderr else ""
            }
                
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": f"ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "model": model_name
        }

@router.post("/models/switch/{model_name}", 
    summary="ğŸ”„ ë¹ ë¥¸ ëª¨ë¸ ì „í™˜",
    description="""
## ë¹ ë¥¸ ëª¨ë¸ ì „í™˜

ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ëª…ì„ ë°›ì•„ ë¹ ë¥´ê²Œ ëª¨ë¸ì„ ì „í™˜í•©ë‹ˆë‹¤.
`POST /api/system/models/switch/{model_name}` í˜•íƒœë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.

### ì‚¬ìš©ë²•
```bash
curl -X POST http://localhost:8000/api/system/models/switch/gemma3-12b%3Alatest
```

### íŠ¹ì§•
- URL ê²½ë¡œë¡œ ëª¨ë¸ëª… ì „ë‹¬ (ê°„í¸í•œ í˜¸ì¶œ)
- ì•ˆì „í•œ ëª¨ë¸ ì „í™˜ ë¡œì§ ì‚¬ìš©
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶”ì 
- ì™„ì „í•œ ì˜¤ë¥˜ ë³µêµ¬ ì§€ì›
"""
)
async def switch_model_by_path(
    model_name: str,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ì „í™˜"""
    from urllib.parse import unquote
    decoded_model_name = unquote(model_name)
    return await service.switch_model_safely(decoded_model_name)

@router.post("/models/{model_name}/start-multiple")
async def start_model_multiple(
    model_name: str,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """íŠ¹ì • ëª¨ë¸ ì‹œì‘ (ë‹¤ì¤‘ ëª¨ë¸ ì‹¤í–‰ í—ˆìš©) - í–¥í›„ í™•ì¥ìš©"""
    try:
        from urllib.parse import unquote
        from app.utils.ollama_manager import start_model, list_running_models
        
        # URL ë””ì½”ë”©
        decoded_model_name = unquote(model_name)
        logger.info(f"ğŸš€ ë‹¤ì¤‘ ëª¨ë¸ ì‹œì‘ ìš”ì²­: ì›ë³¸={model_name}, ë””ì½”ë”©={decoded_model_name}")
        
        # ì‹œì‘ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤ í™•ì¸
        running_before = await list_running_models()
        logger.info(f"ğŸ“‹ ì‹œì‘ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_before}")
        
        # ê¸°ì¡´ ëª¨ë¸ë“¤ì„ ì¤‘ì§€í•˜ì§€ ì•Šê³  ìƒˆ ëª¨ë¸ë§Œ ì‹œì‘
        await start_model(decoded_model_name)
        
        # ê²°ê³¼ í™•ì¸
        running_after = await list_running_models()
        logger.info(f"âœ… ì‹œì‘ í›„ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_after}")
        
        # ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if decoded_model_name in running_after:
            return {
                "success": True,
                "message": f"ëª¨ë¸ '{decoded_model_name}' ì‹œì‘ ì™„ë£Œ (ê¸°ì¡´ ëª¨ë¸ë“¤ ìœ ì§€)",
                "model": decoded_model_name,
                "running_models": running_after,
                "newly_started": decoded_model_name not in running_before
            }
        else:
            return {
                "success": False,
                "error": f"ëª¨ë¸ '{decoded_model_name}' ì‹œì‘ë˜ì§€ ì•ŠìŒ",
                "model": decoded_model_name,
                "running_models": running_after
            }
                
    except Exception as e:
        logger.error(f"âŒ ë‹¤ì¤‘ ëª¨ë¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": f"ë‹¤ì¤‘ ëª¨ë¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "model": model_name
        }

@router.post("/models/stop-all")
async def stop_all_models() -> Dict[str, Any]:
    """ëª¨ë“  ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ì¤‘ì§€"""
    try:
        from app.utils.ollama_manager import stop_all_models, list_running_models
        
        # ì¤‘ì§€ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤ í™•ì¸
        running_before = await list_running_models()
        logger.info(f"ğŸ“‹ ì „ì²´ ì¤‘ì§€ ì „ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_before}")
        
        # ëª¨ë“  ëª¨ë¸ ì¤‘ì§€
        await stop_all_models()
        
        # ê²°ê³¼ í™•ì¸
        running_after = await list_running_models()
        logger.info(f"âœ… ì „ì²´ ì¤‘ì§€ í›„ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_after}")
        
        return {
            "success": True,
            "message": f"ëª¨ë“  ëª¨ë¸ ì¤‘ì§€ ì™„ë£Œ ({len(running_before)}ê°œ ëª¨ë¸ ì¤‘ì§€ë¨)",
            "stopped_models": running_before,
            "running_models": running_after
        }
                
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": f"ì „ì²´ ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        }

@router.get("/health")
async def health_check(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    prompt_manager = get_prompt_manager()
    
    return {
        "status": "ok",
        "model": service.current_model,
        "mode": service.current_mode,
        "mcp_enabled": service.mcp_enabled,
        "debug": service.debug_mode
    }

@router.post("/startup")
async def system_startup(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ê¸°ë³¸ ëª¨ë¸ ìë™ ì‹œì‘"""
    try:
        # ê¸°ë³¸ ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
        chatbot = service.get_session("default")
        if not chatbot:
            session_result = await service.create_session("default")
            if "error" in session_result:
                return {"success": False, "error": session_result["error"]}
            chatbot = service.get_session("default")
        
        # Ollama ì—°ê²° ìƒíƒœ í™•ì¸ ë° ëª¨ë¸ ì‹œì‘
        if chatbot:
            connection_status = await chatbot.client.check_ollama_connection()
            if connection_status["connected"]:
                if not connection_status.get("current_model_running", False):
                    # ëª¨ë¸ì´ ì‹¤í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ì‹œì‘
                    start_result = await chatbot.client.ensure_model_running()
                    if start_result["success"]:
                        return {
                            "success": True,
                            "message": f"ê¸°ë³¸ ëª¨ë¸ '{service.current_model}' ì‹œì‘ ì™„ë£Œ",
                            "model": service.current_model,
                            "model_started": start_result["model_started"]
                        }
                    else:
                        return {
                            "success": False,
                            "error": start_result["message"]
                        }
                else:
                    return {
                        "success": True,
                        "message": f"ëª¨ë¸ '{service.current_model}' ì´ë¯¸ ì‹¤í–‰ ì¤‘",
                        "model": service.current_model,
                        "model_started": False
                    }
            else:
                return {
                    "success": False,
                    "error": connection_status["error"]
                }
        
        return {"success": False, "error": "ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨"}
    
    except Exception as e:
        return {"success": False, "error": str(e)}

@router.get("/startup-validation")
async def get_startup_validation() -> Dict[str, Any]:
    """API ì„œë²„ ì‹œì‘ ì‹œ Ollama ê²€ì¦ ê²°ê³¼ ì¡°íšŒ"""
    from fastapi import Request
    from app.api_server.main import app
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœì—ì„œ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    validation_result = getattr(app.state, 'ollama_validation', None)
    
    if validation_result:
        return {
            "status": "success",
            "validation_result": validation_result,
            "message": "ì‹œì‘ ì‹œ Ollama ê²€ì¦ ì •ë³´ ì¡°íšŒ ì„±ê³µ"
        }
    else:
        return {
            "status": "not_available",
            "message": "ì‹œì‘ ì‹œ ê²€ì¦ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        }

@router.get("/startup-banner")
async def get_startup_banner(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œì‘ ë°°ë„ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    prompt_manager = get_prompt_manager()
    default_prompt = prompt_manager.get_prompt_template("default")
    prompt_desc = default_prompt.description if default_prompt else "ì‹ ì•½ê°œë°œ ì „ë¬¸ AI"
    
    return {
        "version": "2.0.0",
        "model": service.current_model,
        "prompt_type": "default",
        "prompt_description": prompt_desc,
        "features": [
            "ğŸ’Š ì‹ ì•½ê°œë°œ ì „ë¬¸ AI - ë¶„ìë¶€í„° ì„ìƒê¹Œì§€ ì „ ê³¼ì • ì§€ì›",
            "ğŸ§¬ ê³¼í•™ì  ê·¼ê±° ê¸°ë°˜ ë‹µë³€ - ì°¸ê³ ë¬¸í—Œê³¼ í•¨ê»˜ ì œê³µ",
            "ğŸ¯ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ - ëª©ì ì— ë§ëŠ” ì „ë¬¸í™”ëœ ì‘ë‹µ"
        ]
    }