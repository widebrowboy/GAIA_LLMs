"""
System Router - ì‹œìŠ¤í…œ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import Dict, Any, List
from pydantic import BaseModel

from app.api_server.dependencies import get_chatbot_service
from app.api_server.services.chatbot_service import ChatbotService
from app.utils.config import OLLAMA_MODEL, DEBUG_MODE
import httpx
from app.utils.ollama_manager import ensure_single_model_running
from app.utils.prompt_manager import get_prompt_manager

router = APIRouter()

# ê³µí†µ í•¨ìˆ˜
async def _get_ollama_models() -> List[str]:
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        async with httpx.AsyncClient(timeout=3.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            response.raise_for_status()
            data = response.json()
            return [model["name"] for model in data.get("models", [])]
    except Exception:
        # ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ìœ¼ë¡œ í´ë°±
        return ["gemma3:latest", "llama3.2:latest", "mistral:latest"]

async def _get_ollama_running_models() -> List[Dict[str, Any]]:
    """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        async with httpx.AsyncClient(timeout=3.0) as client:
            response = await client.get("http://localhost:11434/api/ps")
            response.raise_for_status()
            data = response.json()
            return data.get("models", [])
    except Exception:
        return []

async def _get_ollama_model_details() -> Dict[str, Any]:
    """ì „ì²´ Ollama ëª¨ë¸ ì •ë³´ ì¡°íšŒ (ì„¤ì¹˜ëœ ëª¨ë¸ + ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)"""
    try:
        available_models = []
        running_models = []
        
        # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
        async with httpx.AsyncClient(timeout=3.0) as client:
            # ì„¤ì¹˜ëœ ëª¨ë¸
            tags_response = await client.get("http://localhost:11434/api/tags")
            if tags_response.status_code == 200:
                tags_data = tags_response.json()
                for model in tags_data.get("models", []):
                    available_models.append({
                        "name": model["name"],
                        "size": model.get("size", 0),
                        "modified_at": model.get("modified_at", ""),
                        "digest": model.get("digest", "")[:12] + "...",
                        "parameter_size": model.get("details", {}).get("parameter_size", "Unknown")
                    })
            
            # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸
            ps_response = await client.get("http://localhost:11434/api/ps")
            if ps_response.status_code == 200:
                ps_data = ps_response.json()
                for model in ps_data.get("models", []):
                    running_models.append({
                        "name": model["name"],
                        "size_vram": model.get("size_vram", 0),
                        "expires_at": model.get("expires_at", ""),
                        "is_running": True
                    })
        
        return {
            "available": available_models,
            "running": running_models,
            "status": "success"
        }
    except Exception as e:
        return {
            "available": [],
            "running": [],
            "status": "error",
            "error": str(e)
        }

class SystemInfo(BaseModel):
    """ì‹œìŠ¤í…œ ì •ë³´ ëª¨ë¸"""
    version: str = "2.0.0"
    model: str
    mode: str
    mcp_enabled: bool
    debug: bool
    available_models: List[str]
    available_prompts: List[str]

class ModelChangeRequest(BaseModel):
    """ëª¨ë¸ ë³€ê²½ ìš”ì²­"""
    model: str
    session_id: str = "default"

class PromptChangeRequest(BaseModel):
    """í”„ë¡¬í”„íŠ¸ ë³€ê²½ ìš”ì²­"""
    prompt_type: str
    session_id: str = "default"

@router.get("/info", response_model=SystemInfo)
async def get_system_info(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    prompt_manager = get_prompt_manager()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë™ì  ì¡°íšŒ
    available_models = await _get_ollama_models()
    
    return {
        "version": "2.0.0",
        "model": service.current_model,
        "mode": service.current_mode,
        "mcp_enabled": service.mcp_enabled,
        "debug": service.debug_mode,
        "available_models": available_models,
        # templates í”„ë¡œí¼í‹° AttributeError ë“± ëª¨ë“  ì˜ˆì™¸ ë°©ì§€
        "available_prompts": (lambda: (
            list(prompt_manager.templates.keys()) if hasattr(prompt_manager, 'templates') and isinstance(prompt_manager.templates, dict)
            else []
        ))()
    }

@router.post("/model")
async def change_model(
    request: ModelChangeRequest,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """AI ëª¨ë¸ ë³€ê²½ ë° Ollama ì‹¤í–‰ ë³´ì¥"""
    chatbot = service.get_session(request.session_id)
    if not chatbot:
        raise HTTPException(404, f"ì„¸ì…˜ {request.session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        # ë¨¼ì € ìš”ì²­ëœ ëª¨ë¸ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        available_models = await _get_ollama_models()
        if request.model not in available_models:
            raise HTTPException(400, f"ëª¨ë¸ '{request.model}'ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì¹˜ëœ ëª¨ë¸: {available_models}")
        
        # 1) Ollamaì—ì„œ ëª¨ë¸ ì‹¤í–‰ ë³´ì¥ (ë¨¼ì € ì‹¤í–‰)
        try:
            from app.utils.ollama_manager import ensure_single_model_running
            await ensure_single_model_running(request.model)
        except Exception as proc_err:
            raise HTTPException(500, f"Ollama ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {proc_err}")
        
        # 2) Chatbot ë‚´ë¶€ ëª¨ë¸ êµì²´
        result = await chatbot.change_model(request.model)
        
        # 3) ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ ì¬í™•ì¸
        running_models = await _get_ollama_running_models()
        model_running = any(model["name"] == request.model for model in running_models)
        
        return {
            "success": True,
            "model": chatbot.client.model_name,
            "model_running": model_running,
            "message": result
        }
    except Exception as e:
        raise HTTPException(400, str(e))

@router.post("/prompt")
async def change_prompt(
    request: PromptChangeRequest,
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """í”„ë¡¬í”„íŠ¸ íƒ€ì… ë³€ê²½"""
    chatbot = service.get_session(request.session_id)
    if not chatbot:
        raise HTTPException(404, f"ì„¸ì…˜ {request.session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ìµœì‹  í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì ìš©
        from app.utils.prompt_manager import get_prompt_manager
        prompt_manager = get_prompt_manager()
        prompt_manager.reload_prompts()
        
        result = await chatbot.change_prompt(request.prompt_type)
        return {
            "success": True,
            "prompt_type": chatbot.current_prompt_type,
            "message": result
        }
    except Exception as e:
        raise HTTPException(400, str(e))

@router.post("/debug")
async def toggle_debug(
    session_id: str = "default",
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€"""
    result = await service.process_command("/debug", session_id)
    return result

@router.post("/mode/{mode}")
async def change_mode(
    mode: str,
    session_id: str = "default",
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ëª¨ë“œ ë³€ê²½ (normal/deep_research)"""
    if mode not in ["normal", "deep_research"]:
        raise HTTPException(400, "ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë“œì…ë‹ˆë‹¤")
    
    if mode == "normal":
        result = await service.process_command("/normal", session_id)
    else:
        result = await service.process_command("/mcp start", session_id)
    
    return result

@router.get("/models")
async def get_available_models(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    models = await _get_ollama_models()
    
    return {
        "models": models,
        "default": service.current_model,
        "status": "success"
    }

@router.get("/models/detailed")
async def get_detailed_models(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ìƒì„¸í•œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì„¤ì¹˜ëœ ëª¨ë¸ + ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸)"""
    model_details = await _get_ollama_model_details()
    current_model = service.current_model
    
    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    current_model_running = any(
        model["name"] == current_model 
        for model in model_details.get("running", [])
    )
    
    return {
        **model_details,
        "current_model": current_model,
        "current_model_running": current_model_running
    }

@router.get("/models/running")
async def get_running_models() -> Dict[str, Any]:
    """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    running_models = await _get_ollama_running_models()
    
    return {
        "running_models": running_models,
        "count": len(running_models),
        "status": "success"
    }

@router.get("/health")
async def health_check(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    prompt_manager = get_prompt_manager()
    
    return {
        "status": "ok",
        "model": service.current_model,
        "mode": service.current_mode,
        "mcp_enabled": service.mcp_enabled,
        "debug": service.debug_mode
    }

@router.post("/startup")
async def system_startup(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ê¸°ë³¸ ëª¨ë¸ ìë™ ì‹œì‘"""
    try:
        # ê¸°ë³¸ ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
        chatbot = service.get_session("default")
        if not chatbot:
            session_result = await service.create_session("default")
            if "error" in session_result:
                return {"success": False, "error": session_result["error"]}
            chatbot = service.get_session("default")
        
        # Ollama ì—°ê²° ìƒíƒœ í™•ì¸ ë° ëª¨ë¸ ì‹œì‘
        if chatbot:
            connection_status = await chatbot.client.check_ollama_connection()
            if connection_status["connected"]:
                if not connection_status.get("current_model_running", False):
                    # ëª¨ë¸ì´ ì‹¤í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ì‹œì‘
                    start_result = await chatbot.client.ensure_model_running()
                    if start_result["success"]:
                        return {
                            "success": True,
                            "message": f"ê¸°ë³¸ ëª¨ë¸ '{service.current_model}' ì‹œì‘ ì™„ë£Œ",
                            "model": service.current_model,
                            "model_started": start_result["model_started"]
                        }
                    else:
                        return {
                            "success": False,
                            "error": start_result["message"]
                        }
                else:
                    return {
                        "success": True,
                        "message": f"ëª¨ë¸ '{service.current_model}' ì´ë¯¸ ì‹¤í–‰ ì¤‘",
                        "model": service.current_model,
                        "model_started": False
                    }
            else:
                return {
                    "success": False,
                    "error": connection_status["error"]
                }
        
        return {"success": False, "error": "ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨"}
    
    except Exception as e:
        return {"success": False, "error": str(e)}

@router.get("/startup-validation")
async def get_startup_validation() -> Dict[str, Any]:
    """API ì„œë²„ ì‹œì‘ ì‹œ Ollama ê²€ì¦ ê²°ê³¼ ì¡°íšŒ"""
    from fastapi import Request
    from app.api_server.main import app
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœì—ì„œ ê²€ì¦ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    validation_result = getattr(app.state, 'ollama_validation', None)
    
    if validation_result:
        return {
            "status": "success",
            "validation_result": validation_result,
            "message": "ì‹œì‘ ì‹œ Ollama ê²€ì¦ ì •ë³´ ì¡°íšŒ ì„±ê³µ"
        }
    else:
        return {
            "status": "not_available",
            "message": "ì‹œì‘ ì‹œ ê²€ì¦ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        }

@router.get("/startup-banner")
async def get_startup_banner(
    service: ChatbotService = Depends(get_chatbot_service)
) -> Dict[str, Any]:
    """ì‹œì‘ ë°°ë„ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    prompt_manager = get_prompt_manager()
    default_prompt = prompt_manager.get_prompt_template("default")
    prompt_desc = default_prompt.description if default_prompt else "ì‹ ì•½ê°œë°œ ì „ë¬¸ AI"
    
    return {
        "version": "2.0.0",
        "model": service.current_model,
        "prompt_type": "default",
        "prompt_description": prompt_desc,
        "features": [
            "ğŸ’Š ì‹ ì•½ê°œë°œ ì „ë¬¸ AI - ë¶„ìë¶€í„° ì„ìƒê¹Œì§€ ì „ ê³¼ì • ì§€ì›",
            "ğŸ§¬ ê³¼í•™ì  ê·¼ê±° ê¸°ë°˜ ë‹µë³€ - ì°¸ê³ ë¬¸í—Œê³¼ í•¨ê»˜ ì œê³µ",
            "ğŸ¯ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ - ëª©ì ì— ë§ëŠ” ì „ë¬¸í™”ëœ ì‘ë‹µ"
        ]
    }