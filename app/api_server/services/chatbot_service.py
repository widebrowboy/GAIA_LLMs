"""
ChatbotService - í•µì‹¬ ì±—ë´‡ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
ê¸°ì¡´ DrugDevelopmentChatbotì˜ ê¸°ëŠ¥ì„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬
"""

import asyncio
import json
from typing import Dict, Any, Optional, List, AsyncGenerator
from datetime import datetime
import logging

from app.cli.chatbot import DrugDevelopmentChatbot, Config
from app.utils.config import OLLAMA_MODEL, MAX_CONVERSATION_CONTEXT, MAX_CONVERSATION_HISTORY
from app.utils.prompt_manager import get_prompt_manager

logger = logging.getLogger(__name__)

class ChatbotService:
    """ì±—ë´‡ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.sessions: Dict[str, DrugDevelopmentChatbot] = {}
        self.prompt_manager = get_prompt_manager()
        self.initialized = False
        
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if not self.initialized:
            logger.info("ChatbotService ì´ˆê¸°í™” ì¤‘...")
            # ê¸°ë³¸ ì„¸ì…˜ ìƒì„±
            await self.create_session("default")
            self.initialized = True
            logger.info("ChatbotService ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def shutdown(self):
        """ì„œë¹„ìŠ¤ ì¢…ë£Œ"""
        logger.info("ChatbotService ì¢…ë£Œ ì¤‘...")
        # ëª¨ë“  ì„¸ì…˜ì˜ MCP ì„œë²„ ì¢…ë£Œ
        for session_id, chatbot in self.sessions.items():
            if chatbot.mcp_commands and chatbot.config.mcp_enabled:
                try:
                    await chatbot.mcp_commands.stop_mcp()
                except Exception as e:
                    logger.error(f"ì„¸ì…˜ {session_id} MCP ì¢…ë£Œ ì˜¤ë¥˜: {e}")
        
        self.sessions.clear()
        logger.info("ChatbotService ì¢…ë£Œ ì™„ë£Œ")
    
    async def create_session(self, session_id: str) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ì±—ë´‡ ì„¸ì…˜ ìƒì„±"""
        if session_id in self.sessions:
            return {"error": f"ì„¸ì…˜ {session_id}ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤"}
        
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ê°ì§€ ë˜ëŠ” ì‚¬ìš©ì ì„¤ì • ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        from app.utils.ollama_manager import list_running_models
        try:
            running_models = await list_running_models()
            if running_models:
                # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš© (ëª¨ë¸ ë³€ê²½ ë°©ì§€)
                current_model = running_models[-1]
                logger.info(f"ì„¸ì…˜ {session_id} ìƒì„± ì‹œ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ìœ ì§€: {current_model}")
            else:
                # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì„¤ì • ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©í•˜ì§€ ì•Šê³  í˜„ì¬ ì‹¤í–‰ëœ ëª¨ë¸ ìœ ì§€
                # ëª¨ë¸ ë³€ê²½ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ í˜„ì¬ í´ë¼ì´ì–¸íŠ¸ì˜ ëª¨ë¸ì„ ìœ ì§€
                current_model = "gemma3-12b:latest"  # ìµœí›„ í´ë°±
                logger.info(f"ì„¸ì…˜ {session_id} ìƒì„± ì‹œ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ì–´ í´ë°± ëª¨ë¸ ì‚¬ìš©: {current_model}")
        except Exception as e:
            logger.warning(f"ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨, í´ë°± ëª¨ë¸ ì‚¬ìš©: {e}")
            current_model = "gemma3-12b:latest"
        
        config = Config(debug_mode=False, model=current_model)
        chatbot = DrugDevelopmentChatbot(config)
        
        # API ì—°ê²° í™•ì¸
        status = await chatbot.client.check_availability()
        if not status:
            return {"error": "Ollama APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        # ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì´ë¼ë©´ auto_select_model ê±´ë„ˆë›°ê¸°
        if running_models and current_model in running_models:
            logger.info(f"ëª¨ë¸ {current_model}ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë¯€ë¡œ auto_select_model ê±´ë„ˆë›°ê¸°")
            # ëª¨ë¸ëª…ë§Œ ì„¤ì •
            chatbot.client._model_name = current_model
            # ì„¤ì • ê¸°ë³¸ê°’ í™•ì¸
            if not hasattr(chatbot.client, 'model_name') or chatbot.client.model_name != current_model:
                chatbot.client.model_name = current_model
        else:
            # ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šë‹¤ë©´ í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸ì„ ì‹œì‘
            logger.info(f"ëª¨ë¸ {current_model}ì´ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŒ. ì‹œì‘ ì‹œë„ ì¤‘...")
            try:
                from app.utils.ollama_manager import start_model
                await start_model(current_model)
                chatbot.client._model_name = current_model
                if not hasattr(chatbot.client, 'model_name') or chatbot.client.model_name != current_model:
                    chatbot.client.model_name = current_model
                logger.info(f"âœ… ëª¨ë¸ {current_model} ì‹œì‘ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ {current_model} ì‹œì‘ ì‹¤íŒ¨: {e}")
                # auto_select_model ì‚¬ìš©ì„ ë¹„í™œì„±í™”í•˜ì—¬ ìë™ ëª¨ë¸ ë³€ê²½ ë°©ì§€
                # ëŒ€ì‹  í˜„ì¬ ëª¨ë¸ ì„¤ì •ì„ ìœ ì§€í•˜ê³  ê²½ê³ ë§Œ ë¡œê·¸
                logger.warning(f"ëª¨ë¸ {current_model} ì‹œì‘ ì‹¤íŒ¨, í•˜ì§€ë§Œ auto_select_model í˜¸ì¶œ ê±´ë„ˆë›°ê¸° (ìë™ ëª¨ë¸ ë³€ê²½ ë°©ì§€)")
                chatbot.client._model_name = current_model
                if not hasattr(chatbot.client, 'model_name') or chatbot.client.model_name != current_model:
                    chatbot.client.model_name = current_model
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¬ë¡œë“œ
        chatbot.current_prompt_type = "default"
        chatbot.system_prompt = self.prompt_manager.get_prompt("default")
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ í™•ì¸ ë¡œê·¸
        if chatbot.system_prompt:
            logger.info(f"ì„¸ì…˜ {session_id}: default í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ ({len(chatbot.system_prompt)}ì)")
        else:
            logger.warning(f"ì„¸ì…˜ {session_id}: default í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨, í´ë°± ì‚¬ìš©")
        
        self.sessions[session_id] = chatbot
        
        return {
            "session_id": session_id,
            "created_at": datetime.now().isoformat(),
            "model": chatbot.client.model_name,
            "mode": "normal",
            "prompt_type": "default",
            "mcp_enabled": False,
            "debug": False
        }
    
    async def delete_session(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ì‚­ì œ"""
        if session_id not in self.sessions:
            return {"error": f"ì„¸ì…˜ {session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        chatbot = self.sessions[session_id]
        
        # MCP ì„œë²„ ì¢…ë£Œ
        if chatbot.mcp_commands and chatbot.config.mcp_enabled:
            try:
                await chatbot.mcp_commands.stop_mcp()
            except Exception as e:
                logger.error(f"MCP ì¢…ë£Œ ì˜¤ë¥˜: {e}")
        
        del self.sessions[session_id]
        
        return {"message": f"ì„¸ì…˜ {session_id}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
    
    def get_session(self, session_id: str) -> Optional[DrugDevelopmentChatbot]:
        """ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        return self.sessions.get(session_id)
    
    async def generate_response(self, session_id: str, message: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
        """ì¼ë°˜ ì‘ë‹µ ìƒì„±"""
        chatbot = self.get_session(session_id)
        if not chatbot:
            # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
            logger.info(f"ì„¸ì…˜ {session_id}ê°€ ì—†ì–´ ìë™ ìƒì„±í•©ë‹ˆë‹¤.")
            session_result = await self.create_session(session_id)
            if "error" in session_result:
                return session_result
            chatbot = self.get_session(session_id)
        
        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì œê³µëœ ê²½ìš° ì±—ë´‡ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì—…ë°ì´íŠ¸
            if conversation_history:
                # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                chatbot.context = []
                chatbot.conversation_history = []
                
                # ìµœê·¼ ë©”ì‹œì§€ë§Œ ì²˜ë¦¬í•˜ë„ë¡ ì œí•œ (ì„±ëŠ¥ ë° ë©”ëª¨ë¦¬ ìµœì í™”)
                recent_history = conversation_history[-MAX_CONVERSATION_CONTEXT * 2:] if len(conversation_history) > MAX_CONVERSATION_CONTEXT * 2 else conversation_history
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì±—ë´‡ì— ë³µì›
                for i, msg in enumerate(recent_history):
                    if msg.get('role') == 'user':
                        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        chatbot.context.append(msg.get('content', ''))
                        # ì§ì„ ì´ë£¨ëŠ” assistant ì‘ë‹µì´ ìˆëŠ”ì§€ í™•ì¸
                        if i + 1 < len(recent_history) and recent_history[i + 1].get('role') == 'assistant':
                            assistant_msg = recent_history[i + 1]
                            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì§ˆë¬¸-ë‹µë³€ ìŒ ì¶”ê°€
                            chatbot.conversation_history.append({
                                "question": msg.get('content', ''),
                                "answer": assistant_msg.get('content', '')
                            })
                
                # ì»¨í…ìŠ¤íŠ¸ê°€ ì„¤ì •ëœ ìµœëŒ€ê°’ì„ ì´ˆê³¼í•˜ë©´ ìµœê·¼ ê²ƒë§Œ ìœ ì§€
                if len(chatbot.context) > MAX_CONVERSATION_CONTEXT:
                    chatbot.context = chatbot.context[-MAX_CONVERSATION_CONTEXT:]
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë„ ì„¤ì •ëœ ìµœëŒ€ê°’ìœ¼ë¡œ ì œí•œ
                if len(chatbot.conversation_history) > MAX_CONVERSATION_HISTORY:
                    chatbot.conversation_history = chatbot.conversation_history[-MAX_CONVERSATION_HISTORY:]
                
                logger.info(f"ì„¸ì…˜ {session_id}: {len(chatbot.context)}ê°œì˜ ì»¨í…ìŠ¤íŠ¸, {len(chatbot.conversation_history)}ê°œì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³µì›ë¨")
            
            response = await chatbot.generate_response(message, ask_to_save=False)
            
            # í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë””ë²„ê¹…ì— ë„ì›€
            current_prompt = self.prompt_manager.get_prompt(chatbot.current_prompt_type)
            
            return {
                "response": response,
                "mode": "deep_research" if chatbot.config.mcp_enabled else "normal",
                "model": chatbot.client.model_name,
                "prompt_type": chatbot.current_prompt_type,
                "debug_info": {
                    "system_prompt_length": len(current_prompt) if current_prompt else 0,
                    "system_prompt_preview": current_prompt[:200] + "..." if current_prompt and len(current_prompt) > 200 else current_prompt
                } if hasattr(chatbot.config, 'debug') and chatbot.config.debug else None
            }
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def generate_streaming_response(
        self, 
        message: str, 
        session_id: str,
        mode: str = "normal",
        mcp_enabled: bool = False,
        model: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        # ëª¨ë¸ ì „í™˜ ë¡œì§ ë¹„í™œì„±í™” - ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ì„ ìœ ì§€
        # ìë™ ëª¨ë¸ ì „í™˜ì„ í•˜ì§€ ì•Šê³ , í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # if model:
        #     from app.utils.ollama_manager import list_running_models
        #     running_models = await list_running_models()
        #     current_running_model = running_models[-1] if running_models else None
        #     
        #     if current_running_model != model:
        #         logger.info(f"ìš”ì²­ëœ ëª¨ë¸ {model}ì´ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ {current_running_model}ê³¼ ë‹¤ë¦„. ëª¨ë¸ ì „í™˜ ì‹œì‘...")
        #         switch_result = await self.switch_model_safely(model)
        #         if not switch_result.get("success"):
        #             yield f"ì˜¤ë¥˜: ëª¨ë¸ ì „í™˜ ì‹¤íŒ¨ - {switch_result.get('error', 'Unknown error')}"
        #             return
        
        # ëŒ€ì‹  í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ë¬´ì—‡ì¸ì§€ë§Œ ë¡œê¹…
        if model:
            from app.utils.ollama_manager import list_running_models
            try:
                running_models = await list_running_models()
                current_running_model = running_models[-1] if running_models else None
                logger.info(f"ğŸ¯ ìš”ì²­ëœ ëª¨ë¸: {model}, í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸: {current_running_model} (ìë™ ì „í™˜ ë¹„í™œì„±í™”ë¨)")
            except Exception as e:
                logger.warning(f"ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        chatbot = self.get_session(session_id)
        if not chatbot:
            # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
            logger.info(f"ì„¸ì…˜ {session_id}ê°€ ì—†ì–´ ìë™ ìƒì„±í•©ë‹ˆë‹¤.")
            session_result = await self.create_session(session_id)
            if "error" in session_result:
                yield f"ì˜¤ë¥˜: {session_result['error']}"
                return
            chatbot = self.get_session(session_id)
        
        # ì„¸ì…˜ì˜ ëª¨ë“œ ë° MCP ì„¤ì • ì—…ë°ì´íŠ¸
        if hasattr(chatbot, 'current_mode'):
            chatbot.current_mode = mode
        if hasattr(chatbot, 'mcp_enabled'):
            chatbot.mcp_enabled = mcp_enabled
        
        # í”„ë¡¬í”„íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ì¬ë¡œë“œ
        if not chatbot.system_prompt or len(chatbot.system_prompt) < 100:
            chatbot.system_prompt = self.prompt_manager.get_prompt(chatbot.current_prompt_type)
            logger.info(f"ì„¸ì…˜ {session_id}: í”„ë¡¬í”„íŠ¸ ì¬ë¡œë“œ ì™„ë£Œ ({len(chatbot.system_prompt)}ì)")
        
        logger.info(f"ì„¸ì…˜ {session_id} ëª¨ë“œ ì„¤ì •: {mode}, MCP: {mcp_enabled}, í”„ë¡¬í”„íŠ¸: {chatbot.current_prompt_type}")
        
        try:
            # Ollama ì—°ê²° ìƒíƒœ í™•ì¸
            connection_status = await chatbot.client.check_ollama_connection()
            if not connection_status["connected"]:
                error_msg = f"[ì—°ê²° ì˜¤ë¥˜] {connection_status['error']}"
                logger.error(error_msg)
                yield error_msg
                return
            
            # ëª¨ë¸ì´ ì‹¤í–‰ë˜ì§€ ì•Šì€ ê²½ìš° í˜„ì¬ ëª¨ë¸ ì‹œì‘ (ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½í•˜ì§€ ì•ŠìŒ)
            if not connection_status.get("current_model_running", False):
                logger.info(f"ëª¨ë¸ '{chatbot.client.model}'ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ. í˜„ì¬ ëª¨ë¸ ì‹œì‘ ì‹œë„ ì¤‘... (ìë™ ëª¨ë¸ ë³€ê²½ ë¹„í™œì„±í™”)")
                start_result = await chatbot.client.ensure_model_running()
                if not start_result["success"]:
                    error_msg = f"[ëª¨ë¸ ì‹œì‘ ì˜¤ë¥˜] {start_result['message']}"
                    logger.error(error_msg)
                    # ëª¨ë¸ ì‹œì‘ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ì¡´ ëª¨ë¸ ì„¤ì • ìœ ì§€í•˜ê³  ê³„ì† ì§„í–‰
                    logger.warning(f"ëª¨ë¸ ì‹œì‘ ì‹¤íŒ¨í–ˆì§€ë§Œ ê¸°ì¡´ ëª¨ë¸ ì„¤ì • ìœ ì§€í•˜ê³  ê³„ì† ì§„í–‰")
                    # yield error_msg
                    # return
                else:
                    logger.info(f"âœ… {start_result['message']}")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            async for chunk in chatbot.generate_streaming_response(message):
                yield chunk
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜¤ë¥˜: {e}")
            if "connection" in str(e).lower() or "ollama" in str(e).lower():
                yield f"[ì—°ê²° ì˜¤ë¥˜] Ollama ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}"
            else:
                yield f"ì˜¤ë¥˜: {str(e)}"
    
    async def process_command(self, command: str, session_id: str) -> Dict[str, Any]:
        """ëª…ë ¹ì–´ ì²˜ë¦¬"""
        chatbot = self.get_session(session_id)
        if not chatbot:
            # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
            logger.info(f"ì„¸ì…˜ {session_id}ê°€ ì—†ì–´ ìë™ ìƒì„±í•©ë‹ˆë‹¤.")
            session_result = await self.create_session(session_id)
            if "error" in session_result:
                return session_result
            chatbot = self.get_session(session_id)
        
        # ëª…ë ¹ì–´ ì •ê·œí™”
        if not command.startswith("/"):
            command = "/" + command
        
        try:
            # ëª…ë ¹ì–´ë³„ ì²˜ë¦¬
            if command == "/help":
                return {"type": "help", "content": self._get_help_text()}
            
            elif command.startswith("/mcp"):
                mcp_args = command[4:].strip()
                if not chatbot.mcp_commands:
                    return {"error": "MCP ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                result = await chatbot.mcp_commands.handle_mcp_command(mcp_args)
                # MCP í™œì„± ìƒíƒœ í™•ì¸ (ì•ˆì „í•œ ë°©ì‹)
                chatbot.config.mcp_enabled = getattr(chatbot.mcp_commands, 'is_mcp_active', lambda: chatbot.mcp_enabled)()
                
                return {
                    "type": "mcp",
                    "command": mcp_args,
                    "result": result,
                    "mcp_enabled": chatbot.config.mcp_enabled
                }
            
            elif command.startswith("/model"):
                parts = command.split(maxsplit=1)
                if len(parts) > 1:
                    result = await chatbot.change_model(parts[1])
                    return {
                        "type": "model",
                        "model": chatbot.client.model_name,
                        "result": result
                    }
                else:
                    return {"error": "ì‚¬ìš©ë²•: /model <ëª¨ë¸ëª…>"}
            
            elif command.startswith("/prompt"):
                parts = command.split(maxsplit=1)
                prompt_type = parts[1] if len(parts) > 1 else None
                result = await chatbot.change_prompt(prompt_type)
                
                return {
                    "type": "prompt",
                    "prompt_type": chatbot.current_prompt_type,
                    "result": result
                }
            
            elif command == "/debug":
                chatbot.config.debug_mode = not chatbot.config.debug_mode
                chatbot.client.set_debug_mode(chatbot.config.debug_mode)
                
                return {
                    "type": "debug",
                    "enabled": chatbot.config.debug_mode,
                    "message": f"ë””ë²„ê·¸ ëª¨ë“œê°€ {'ì¼œì§' if chatbot.config.debug_mode else 'êº¼ì§'}ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            
            elif command == "/normal":
                await chatbot.switch_to_normal_mode()
                return {
                    "type": "mode",
                    "mode": "normal",
                    "message": "ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            
            elif command == "/deep":
                # Deep Research ëª¨ë“œë¡œ ì „í™˜ (MCP ì‹œì‘)
                if not chatbot.mcp_commands:
                    return {"error": "MCP ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                
                result = await chatbot.mcp_commands.handle_mcp_command("start")
                chatbot.config.mcp_enabled = True
                
                return {
                    "type": "mode",
                    "mode": "deep_research",
                    "message": "Deep Research ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤. MCP í†µí•© ê²€ìƒ‰ì´ í™œì„±í™”ë©ë‹ˆë‹¤.",
                    "mcp_enabled": True
                }
            
            elif command == "/mcpshow":
                chatbot.toggle_mcp_output()
                return {
                    "type": "mcpshow",
                    "enabled": chatbot.config.show_mcp_output,
                    "message": f"MCP ì¶œë ¥ì´ {'í‘œì‹œ' if chatbot.config.show_mcp_output else 'ìˆ¨ê¹€'}ë©ë‹ˆë‹¤."
                }
            
            else:
                return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}"}
                
        except Exception as e:
            logger.error(f"ëª…ë ¹ì–´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def get_session_info(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        chatbot = self.get_session(session_id)
        if not chatbot:
            return {"error": f"ì„¸ì…˜ {session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        # í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì •ë³´
        prompt_template = self.prompt_manager.get_prompt_template(chatbot.current_prompt_type)
        prompt_desc = prompt_template.description if prompt_template else "ê¸°ë³¸"
        
        return {
            "session_id": session_id,
            "model": chatbot.client.model_name,
            "mode": "deep_research" if chatbot.config.mcp_enabled else "normal",
            "prompt_type": chatbot.current_prompt_type,
            "prompt_description": prompt_desc,
            "mcp_enabled": chatbot.config.mcp_enabled,
            "debug": chatbot.config.debug_mode,
            "mcp_output_visible": chatbot.config.show_mcp_output,
            "available_models": ["gemma3-12b:latest", "txgemma-chat:latest", "txgemma-predict:latest", "Gemma3:27b-it-q4_K_M"],
            "available_prompts": list(self.prompt_manager.templates.keys()) if hasattr(self.prompt_manager, 'templates') and self.prompt_manager.templates else []
        }
    
    def get_all_sessions(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        sessions = []
        for session_id in self.sessions:
            info = self.get_session_info(session_id)
            if "error" not in info:
                sessions.append(info)
        return sessions
    
    def _get_help_text(self) -> str:
        """ë„ì›€ë§ í…ìŠ¤íŠ¸"""
        return """
ğŸ“š GAIA-BT v2.0 API ë„ì›€ë§

ğŸ¯ ê¸°ë³¸ ëª…ë ¹ì–´:
  /help            - ì´ ë„ì›€ë§ í‘œì‹œ
  /debug           - ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€
  /mcpshow         - MCP ê²€ìƒ‰ ê³¼ì • í‘œì‹œ í† ê¸€
  /normal          - ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
  /model <ì´ë¦„>    - AI ëª¨ë¸ ë³€ê²½
  /prompt <ëª¨ë“œ>   - ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ë³€ê²½

ğŸ”¬ MCP ëª…ë ¹ì–´:
  /mcp start       - í†µí•© MCP ì‹œìŠ¤í…œ ì‹œì‘
  /mcp stop        - MCP ì„œë²„ ì¤‘ì§€
  /mcp status      - MCP ìƒíƒœ í™•ì¸

ğŸ“ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ:
  - default: ê¸°ë³¸ ì‹ ì•½ê°œë°œ AI
  - clinical: ì„ìƒì‹œí—˜ ì „ë¬¸
  - research: ì—°êµ¬ ë¶„ì„ ì „ë¬¸
  - chemistry: ì˜ì•½í™”í•™ ì „ë¬¸
  - regulatory: ê·œì œ ì „ë¬¸
"""
    
    @property
    def current_model(self) -> str:
        """í˜„ì¬ ëª¨ë¸"""
        if "default" in self.sessions:
            return self.sessions["default"].client.model_name
        return OLLAMA_MODEL
    
    @property
    def current_mode(self) -> str:
        """í˜„ì¬ ëª¨ë“œ"""
        if "default" in self.sessions:
            return "deep_research" if self.sessions["default"].config.mcp_enabled else "normal"
        return "normal"
    
    @property
    def mcp_enabled(self) -> bool:
        """MCP í™œì„±í™” ì—¬ë¶€"""
        if "default" in self.sessions:
            return self.sessions["default"].config.mcp_enabled
        return False
    
    @property
    def debug_mode(self) -> bool:
        """ë””ë²„ê·¸ ëª¨ë“œ"""
        if "default" in self.sessions:
            return self.sessions["default"].config.debug_mode
        return False
    
    def update_current_model(self, model_name: str) -> None:
        """í˜„ì¬ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if "default" in self.sessions:
            # OllamaClientì˜ _model_name ì†ì„±ì„ ì§ì ‘ ìˆ˜ì •
            self.sessions["default"].client._model_name = model_name
            logger.info(f"í˜„ì¬ ëª¨ë¸ì„ '{model_name}'ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
    
    async def switch_model_safely(self, new_model_name: str) -> Dict[str, Any]:
        """ì•ˆì „í•œ ëª¨ë¸ ì „í™˜ - ê¸°ì¡´ ëª¨ë¸ ì¤‘ì§€ í›„ ìƒˆ ëª¨ë¸ ì‹¤í–‰"""
        try:
            from app.utils.ollama_manager import stop_all_models, ensure_single_model_running
            
            logger.info(f"ğŸ”„ ì•ˆì „í•œ ëª¨ë¸ ì „í™˜ ì‹œì‘: {new_model_name}")
            
            # 1ë‹¨ê³„: ê¸°ì¡´ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ëª¨ë¸ ì¤‘ì§€
            logger.info("ğŸ›‘ ê¸°ì¡´ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ëª¨ë¸ ì¤‘ì§€...")
            try:
                await stop_all_models()
                logger.info("âœ… ëª¨ë“  ëª¨ë¸ ì¤‘ì§€ ì™„ë£Œ")
            except Exception as stop_error:
                logger.warning(f"ëª¨ë¸ ì¤‘ì§€ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ: {stop_error}")
            
            # 2ë‹¨ê³„: ìƒˆ ëª¨ë¸ ì‹œì‘ (ë‹¨ì¼ ëª¨ë¸ ë³´ì¥)
            logger.info(f"ğŸš€ ìƒˆ ëª¨ë¸ ì‹œì‘: {new_model_name}")
            try:
                await ensure_single_model_running(new_model_name)
                logger.info(f"âœ… ëª¨ë¸ ì „í™˜ ì™„ë£Œ: {new_model_name}")
                
                # 3ë‹¨ê³„: ì±—ë´‡ ì„¸ì…˜ì˜ ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
                self.update_current_model(new_model_name)
                
                return {
                    "success": True,
                    "message": f"ëª¨ë¸ì´ '{new_model_name}'ë¡œ ì„±ê³µì ìœ¼ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "current_model": new_model_name,
                    "previous_models_stopped": True
                }
            except Exception as start_error:
                logger.error(f"âŒ ìƒˆ ëª¨ë¸ ì‹œì‘ ì‹¤íŒ¨: {start_error}")
                return {
                    "success": False,
                    "error": f"ìƒˆ ëª¨ë¸ ì‹œì‘ ì‹¤íŒ¨: {str(start_error)}"
                }
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì „í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "error": f"ëª¨ë¸ ì „í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }