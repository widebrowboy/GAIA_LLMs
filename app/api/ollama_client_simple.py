#!/usr/bin/env python3
"""
ë‹¨ìˆœí™”ëœ Ollama í´ë¼ì´ì–¸íŠ¸ - ìŠ¤íŠ¸ë¦¬ë° ì „ìš©
"""

import json
import asyncio
from typing import Optional, AsyncGenerator
import httpx

class SimpleOllamaClient:
    """ë‹¨ìˆœí™”ëœ Ollama í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, model: str = "gemma3-12b:latest", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
        self._client = None
    
    async def _get_client(self):
        if self._client is None:
            self._client = httpx.AsyncClient(timeout=httpx.Timeout(600.0))
        return self._client
    
    async def close(self):
        if self._client:
            await self._client.aclose()
            self._client = None
    
    async def generate_stream(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        temperature: float = 0.7
    ) -> AsyncGenerator[str, None]:
        """
        ë‹¨ìˆœí™”ëœ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            
        Yields:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬
        """
        print(f"ğŸ”„ ì§ˆì˜ ì‹œì‘: {prompt[:50]}...")
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì: {prompt}\n\nì–´ì‹œìŠ¤í„´íŠ¸:"
        
        # í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": True,
            "options": {
                "temperature": temperature,
                "num_predict": 500,
                "keep_alive": "5m"
            }
        }
        
        try:
            client = await self._get_client()
            
            async with client.stream(
                "POST",
                f"{self.base_url}/api/generate",
                json=payload
            ) as response:
                response.raise_for_status()
                
                print("ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
                chunk_count = 0
                
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            chunk_data = json.loads(line)
                            
                            if "response" in chunk_data and chunk_data["response"]:
                                chunk_count += 1
                                if chunk_count % 20 == 0:  # 20ê°œ ì²­í¬ë§ˆë‹¤ ë¡œê·¸
                                    print(f"ğŸ“ ì²­í¬ {chunk_count} ìˆ˜ì‹ ")
                                yield chunk_data["response"]
                            
                            if chunk_data.get("done", False):
                                print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ (ì´ {chunk_count}ê°œ ì²­í¬)")
                                return
                                
                        except json.JSONDecodeError:
                            continue
                            
        except Exception as e:
            error_msg = f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
            print(f"âŒ {error_msg}")
            yield f"[ì˜¤ë¥˜: {error_msg}]"