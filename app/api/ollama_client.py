#!/usr/bin/env python3
"""
Ollama API í´ë¼ì´ì–¸íŠ¸

ì´ ëª¨ë“ˆì€ Ollama APIì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ë° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Ollama API í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤ëŠ” ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import os
from typing import Any, Dict, List, Optional

import httpx
from dotenv import load_dotenv
import aiohttp

# ì–´ëŒ‘í„° í´ë˜ìŠ¤ ì„í¬íŠ¸
from app.api.model_adapters import get_adapter_for_model

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class OllamaClient:
    """
    Ollama API í´ë¼ì´ì–¸íŠ¸

    Ollama LLM APIì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    í…ìŠ¤íŠ¸ ìƒì„±, ë³‘ë ¬ ìƒì„± ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(self,
                 model: str = "Gemma3:latest",
                 temperature: float = 0.7,
                 max_tokens: int = 4000,
                 min_response_length: int = 500,
                 ollama_url: Optional[str] = None,
                 debug_mode: bool = False):
        """
        Ollama API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: "Gemma3:latest")
            temperature: ìƒì„± ì˜¨ë„ (ê¸°ë³¸ê°’: 0.7)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 4000)
            min_response_length: ìµœì†Œ ì‘ë‹µ ê¸¸ì´ (ê¸°ë³¸ê°’: 500)
            ollama_url: Ollama API ì—”ë“œí¬ì¸íŠ¸ URL (ê¸°ë³¸ê°’: í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            debug_mode: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        """
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Ollama URL ë¡œë“œ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        self.ollama_url = ollama_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.min_response_length = min_response_length
        self.max_retries = 3

        # GPU ìµœì í™” íŒŒë¼ë¯¸í„° (windsurfrulesì— ë”°ë¦„)
        self.gpu_params = {
            "num_gpu": 99,       # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU í™œìš©
            "num_thread": 8,     # ë³‘ë ¬ ìŠ¤ë ˆë“œ í™œìš©
            "f16_kv": True,      # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            "mirostat": 2        # ê³ ê¸‰ ìƒ˜í”Œë§
        }

        # ëª¨ë¸ë³„ ì–´ëŒ‘í„° ì„¤ì •
        self._set_adapter(model)

        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜
        self._http_client = None

        # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
        self.debug_mode = debug_mode

        # ë””ë²„ê·¸ ëª¨ë“œ ì´ˆê¸°í™” ìƒíƒœ ì •ë³´ ì¶œë ¥
        print(f"[OllamaClient] ì´ˆê¸°í™” - ë””ë²„ê·¸ ëª¨ë“œ: {self.debug_mode}")
        if self.debug_mode:
            print(f"[OllamaClient] ì‚¬ìš©í•  ëª¨ë¸: {self.model}")
            print(f"[OllamaClient] Ollama URL: {self.ollama_url}")
            print(f"[OllamaClient] GPU íŒŒë¼ë¯¸í„°: {self.gpu_params}")


    def _set_adapter(self, model_name: str):
        """
        ëª¨ë¸ì— ë§ëŠ” ì–´ëŒ‘í„° ì„¤ì •

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
        """
        self.adapter = get_adapter_for_model(model_name)

    async def _get_http_client(self):
        """
        HTTP í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ë°˜í™˜ (í•„ìš”ì‹œ ìƒì„±)

        Returns:
            httpx.AsyncClient: ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        """
        if self._http_client is None:
            self._http_client = httpx.AsyncClient(timeout=120.0)
        return self._http_client

    async def close(self):
        """
        HTTP í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ
        """
        if self._http_client is not None:
            await self._http_client.aclose()
            self._http_client = None

    async def generate(self,
                       prompt: str,
                       system_prompt: Optional[str] = None,
                       temperature: Optional[float] = None,
                       max_retries: Optional[int] = None) -> str:
        """
        ì–´ëŒ‘í„° íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ëª¨ë¸ì— ë§ê²Œ í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            temperature: ìƒì„± ì˜¨ë„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        max_retries = max_retries or self.max_retries
        temp = temperature if temperature is not None else self.temperature

        # ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ëª¨ë¸ì— ë§ëŠ” ìš”ì²­ í˜•ì‹ ìƒì„±
        payload, endpoint_path = await self.adapter.format_request(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temp,
            max_tokens=self.max_tokens,
            gpu_params=self.gpu_params
        )

        # ëª¨ë¸ ì´ë¦„ ì¶”ê°€
        payload["model"] = self.model

        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€ (ë””ë²„ê·¸ ëª¨ë“œì¼ ë•Œë§Œ)
        if self.debug_mode:
            print(f"[ë””ë²„ê·¸] OllamaClient.generate í˜¸ì¶œ: ëª¨ë¸={self.model}, ì—”ë“œí¬ì¸íŠ¸={endpoint_path}")
            print(f"[ë””ë²„ê·¸] í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ì")
            print(f"[ë””ë²„ê·¸] í˜ì´ë¡œë“œ: {str(payload)[:300]}...")

        # ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        last_error = None
        for attempt in range(max_retries + 1):
            try:
                # HTTP í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
                client = await self._get_http_client()

                # API ìš”ì²­
                if self.debug_mode:
                    print(f"[ë””ë²„ê·¸] API ìš”ì²­ ì‹œì‘ (ì‹œë„ {attempt+1}/{max_retries+1})")
                response = await client.post(
                    f"{self.ollama_url}{endpoint_path}",  # ì–´ëŒ‘í„°ê°€ ì œê³µí•œ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )

                response.raise_for_status()  # HTTP ì˜¤ë¥˜ í™•ì¸

                # ì‘ë‹µ íŒŒì‹±
                result = response.json()

                # ë””ë²„ê·¸ ëª¨ë“œì¼ ë•Œë§Œ ë¡œê·¸ ì¶œë ¥
                if self.debug_mode:
                    print(f"[ë””ë²„ê·¸] API ì‘ë‹µ ìˆ˜ì‹ : ìƒíƒœ ì½”ë“œ={response.status_code}")
                    print(f"[ë””ë²„ê·¸] ì‘ë‹µ í‚¤: {list(result.keys())}")

                    # raw ì‘ë‹µ ë¡œê·¸
                    raw_snippet = str(result.get('response', ''))[:50]
                    print(f"[ë””ë²„ê·¸] ì›ì‹œ ì‘ë‹µ ì¼ë¶€: {raw_snippet}...")

                # ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë³„ ì‘ë‹µ íŒŒì‹±
                generated_text = self.adapter.parse_response(result)

                # ì‘ë‹µ í’ˆì§ˆ ê²€ì‚¬
                is_invalid_response = False
                reason = ""

                # 1. ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ê²€ì‚¬ (ì˜ˆ: "356")
                if generated_text.isdigit() or (
                    generated_text.replace('.', '', 1).isdigit() and generated_text.count('.') <= 1
                ):
                    is_invalid_response = True
                    reason = f"ìˆ«ìë§Œ ë°˜í™˜ë¨ ({generated_text})"

                # 2. ë„ˆë¬´ ì§§ì€ ì‘ë‹µ ê²€ì‚¬ (íŠ¹íˆ txgemma-predict ëª¨ë¸)
                elif len(generated_text.strip()) < 100 and self.model == "txgemma-predict:latest":
                    is_invalid_response = True
                    reason = f"ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ ({len(generated_text.strip())} ë¬¸ì)"

                # 3. txgemma-predict ëª¨ë¸ì— ëŒ€í•œ íŠ¹ë³„ ê²€ì‚¬
                elif self.model == "txgemma-predict:latest":
                    # ê·¼ìœ¡/ê±´ê°• ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬
                    keywords = ['ê·¼ìœ¡', 'ê±´ê°•', 'ë³´ì¶©ì œ', 'ì˜ì–‘', 'ë‹¨ë°±ì§ˆ', 'íš¨ê³¼', 'ì°¸ê³ ë¬¸í—Œ']
                    has_keywords = any(keyword in generated_text.lower() for keyword in keywords)

                    # ë©”íƒ€ë°ì´í„° ë³€ê²½ ì´í›„ ìœ íš¨í•œ ì¶œë ¥ì´ ì—†ì„ ë•Œ
                    if not has_keywords:
                        is_invalid_response = True
                        reason = "ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ"

                    # ì˜¤ë¥˜ ë©”ì‹œì§€ ê²€ì‚¬
                    error_terms = ['error', 'exception', 'ì˜¤ë¥˜', 'ì‹¤íŒ¨', '\\n\\n\\n']
                    if any(term in generated_text.lower() for term in error_terms):
                        is_invalid_response = True
                        reason = "ì˜¤ë¥˜ ë©”ì‹œì§€ í¬í•¨"

                # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ê²€ì¦ (íŠ¹íˆ txgemma ëª¨ë¸)
                if self.model.startswith("txgemma") and "#" not in generated_text and "\n\n" not in generated_text:
                    is_invalid_response = True
                    reason = "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì´ ì•„ë‹˜"

                if is_invalid_response and self.debug_mode:
                    print(f"[ë””ë²„ê·¸] ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µ: {reason}")
                    # ì‚¬ìš©ì ì¹œí™”ì ì¸ ëŒ€ì²´ ë©”ì‹œì§€ ë°˜í™˜
                    generated_text = f"""
# ê·¼ìœ¡ ê´€ë ¨ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì •ë³´

ì•ˆë…•í•˜ì„¸ìš”, ë„ì›€ì´ í•„ìš”í•˜ì‹  ë‚´ìš©ì„ ì •í™•íˆ ì´í•´í–ˆìŠµë‹ˆë‹¤.

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸({self.model})ì´ ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ í•œê³„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë‹¤ë¥¸ ëŒ€ì•ˆ

1. **ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©**:
   - `/model Gemma3` - ë” ì•ˆì •ì ì´ê³  ë‹¤ì–‘í•œ ì •ë³´ ì œê³µ ê°€ëŠ¥
   - `/model txgemma-chat` - ëŒ€í™”í˜• ëª¨ë¸ë¡œ ë‹¤ë¥¸ í˜•ì‹ì˜ ì‘ë‹µ ì œê³µ

2. **ì§ˆë¬¸ êµ¬ì²´í™”**:
   - ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš” (ex: "íŠ¹ì • ë³´ì¶©ì œì˜ íš¨ê³¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")

3. **í”¼ë“œë°± ëª¨ë“œ ì‚¬ìš©**:
   - `/feedback` ëª…ë ¹ì–´ë¡œ ë” ê¹Šì€ ì—°êµ¬ ìˆ˜í–‰ ê°€ëŠ¥

## ì°¸ê³  ë¬¸í—Œ

1. Journal of the International Society of Sports Nutrition (https://jissn.biomedcentral.com/)
2. American Journal of Clinical Nutrition (https://academic.oup.com/ajcn)
"""

                # ì‘ë‹µ í›„ì²˜ë¦¬
                generated_text = self.adapter.post_process(generated_text)

                # ìµœì†Œ ê¸¸ì´ í™•ì¸
                if not generated_text or generated_text.isspace():
                    print("[ê²½ê³ ] ë¹ˆ ì‘ë‹µì´ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤")
                    generated_text = "[ì‘ë‹µì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”. `/model Gemma3`]"

                if len(generated_text) < self.min_response_length:
                    print(f"âš ï¸ ê²½ê³ : ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(generated_text)} ì)")

                    # ë„ˆë¬´ ì§§ì€ ì‘ë‹µì— ëŒ€í•œ ì²˜ë¦¬ (ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°)
                    if len(generated_text) < 10 and (generated_text.isdigit() or generated_text.replace('.', '', 1).isdigit()):
                        generated_text = "[ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•˜ê±°ë‚˜ `/model Gemma3` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë³€ê²½í•´ë³´ì„¸ìš”.]"

                # ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ë¡œê·¸
                if self.debug_mode:
                    print(f"[ë””ë²„ê·¸] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(generated_text)} ì")
                    print(f"[ë””ë²„ê·¸] ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {generated_text[:200]}...")

                return generated_text

            except httpx.HTTPStatusError as e:
                last_error = f"HTTP ì˜¤ë¥˜: {e.response.status_code} - {e.response.text}"
                print(f"API ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries + 1}): {last_error}")

                # 429 Too Many Requestsì™€ ê°™ì€ ê²½ìš°ì—ë§Œ ì¬ì‹œë„
                if e.response.status_code == 429:
                    backoff_time = 2 ** attempt
                    print(f"â±ï¸ {backoff_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    await asyncio.sleep(backoff_time)
                    continue
                raise

            except (httpx.RequestError, json.JSONDecodeError, ValueError) as e:
                last_error = str(e)
                print(f"ì‹œë„ {attempt + 1}/{max_retries + 1} ì‹¤íŒ¨: {last_error}")
                if attempt < max_retries:
                    backoff_time = 1 + attempt * 2
                    print(f"â±ï¸ {backoff_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    await asyncio.sleep(backoff_time)
                else:
                    print(f"â›” ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {last_error}")

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        error_msg = f"ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}"
        print(f"âŒ {error_msg}")
        return f"[ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {error_msg}]"

    async def generate_parallel(self, prompts: List[Dict[str, Any]], max_concurrent: int = 2) -> List[str]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë³‘ë ¬ë¡œ í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompts: í”„ë¡¬í”„íŠ¸ ëª©ë¡ (ê°ê° 'prompt', 'system' í‚¤ í¬í•¨ ê°€ëŠ¥)
            max_concurrent: ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ (ê¸°ë³¸ê°’: 2)

        Returns:
            List[str]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ëª©ë¡
        """
        results = []

        # ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ìš”ì²­ ì œí•œ
        semaphore = asyncio.Semaphore(max_concurrent)

        async def _generate_with_limit(prompt_data):
            # ì„¸ë§ˆí¬ì–´ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ìš”ì²­ ì œí•œ
            async with semaphore:
                if isinstance(prompt_data, dict):
                    prompt = prompt_data.get('prompt', "")
                    system = prompt_data.get('system', None)
                    temp = prompt_data.get('temperature', None)
                else:
                    prompt = str(prompt_data)
                    system = None
                    temp = None

                return await self.generate(prompt, system_prompt=system, temperature=temp)

        # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰
        tasks = [_generate_with_limit(prompt_data) for prompt_data in prompts]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ì˜ˆì™¸ ì²˜ë¦¬
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                results[i] = f"[ì˜¤ë¥˜: {result!s}]"

        return results

    def update_model(self, model_name: str):
        """
        ëª¨ë¸ ë³€ê²½ ë° ì–´ëŒ‘í„° ì—…ë°ì´íŠ¸

        Args:
            model_name: ìƒˆë¡œìš´ ëª¨ë¸ ì´ë¦„
        """
        self.model = model_name
        self._set_adapter(model_name)

    def set_debug_mode(self, debug_mode: bool):
        """
        ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •

        Args:
            debug_mode: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        """
        self.debug_mode = debug_mode

    async def check_availability(self) -> dict:
        """Ollama API ì—°ê²° ë° ëª¨ë¸ ê°€ìš©ì„± í™•ì¸"""
        try:
            # API ì—°ê²° í™•ì¸
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_url}/api/tags") as response:
                    if response.status == 200:
                        print("âœ… Ollama API ì—°ê²° ì„±ê³µ")
                        
                        # ëª¨ë¸ í™•ì¸
                        data = await response.json()
                        models = [model["name"] for model in data.get("models", [])]
                        if self.model in models:
                            print(f"âœ… ëª¨ë¸ '{self.model}' í™•ì¸ë¨")
                            return {"available": True, "models": models}
                        else:
                            print(f"âŒ ëª¨ë¸ '{self.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            print(f"ğŸ”§ í•´ê²°ë°©ë²•: ollama pull {self.model}")
                            return {"available": False, "error": f"ëª¨ë¸ '{self.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "models": models}
                    else:
                        print("âŒ Ollama API ì—°ê²° ì‹¤íŒ¨")
                        return {"available": False, "error": "Ollama API ì—°ê²° ì‹¤íŒ¨"}
        except Exception as e:
            print(f"âŒ ëª¨ë¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {"available": False, "error": str(e)}

    async def list_models(self) -> List[Dict[str, Any]]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°

        Returns:
            List[Dict[str, Any]]: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
        """
        try:
            client = await self._get_http_client()
            response = await client.get(f"{self.ollama_url}/api/tags")
            response.raise_for_status()

            # API ì‘ë‹µì—ì„œ ëª¨ë¸ ëª©ë¡ ì¶”ì¶œ
            return response.json().get("models", [])
        except Exception as e:
            print(f"ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e!s}")
            return []

    async def check_model_availability(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """
        íŠ¹ì • ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸

        Args:
            model_name: í™•ì¸í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: í˜„ì¬ ëª¨ë¸)

        Returns:
            Dict[str, Any]: ëª¨ë¸ ê°€ìš©ì„± ì •ë³´
        """
        model = model_name or self.model

        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            models_list = await self.list_models()

            # ëª¨ë¸ëª…ë§Œ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ë¬´ê´€)
            available_models = []
            for m in models_list:
                name = m.get("name", "")
                if name:  # ë¹ˆ ê²½ìš° ì²˜ë¦¬
                    available_models.append(name)

            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë¹„êµ
            is_available = False
            matched_model = None

            for avail_model in available_models:
                # ì •í™•íˆ ë™ì¼í•œ ê²½ìš°
                if model == avail_model:
                    is_available = True
                    matched_model = avail_model
                    break
                # ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê³  ë¹„êµ
                elif model.lower() == avail_model.lower():
                    is_available = True
                    matched_model = avail_model
                    break

            if is_available:
                # ì í•©í•œ ì–´ëŒ‘í„° ì¶”ê°€ í™•ì¸
                adapter_class = get_adapter_for_model(model).__class__.__name__

                return {
                    "available": True,
                    "model": matched_model,  # ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ëª… ë°˜í™˜
                    "adapter": adapter_class
                }
            else:
                return {
                    "available": False,
                    "model": model,
                    "message": f"ëª¨ë¸ '{model}'ì´(ê°€) ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.",
                    "available_models": available_models
                }

        except Exception as e:
            return {
                "available": False,
                "model": model,
                "error": str(e),
                "message": f"ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e!s}"
            }


# ëª¨ë“ˆ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    async def test():
        client = OllamaClient()

        # API ê°€ìš©ì„± í™•ì¸
        status = await client.check_availability()
        print(f"Ollama API ìƒíƒœ: {status}")

        if status:
            # ë‹¨ì¼ ìƒì„± í…ŒìŠ¤íŠ¸
            response = await client.generate(
                prompt="ê·¼ìœ¡ ë°œë‹¬ì— ê°€ì¥ ì¤‘ìš”í•œ ì˜ì–‘ì†Œ 3ê°€ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                system_prompt="ë‹¹ì‹ ì€ ê·¼ìœ¡ ê´€ë ¨ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            )
            print(f"\nìƒì„±ëœ ì‘ë‹µ:\n{response}")

    asyncio.run(test())
