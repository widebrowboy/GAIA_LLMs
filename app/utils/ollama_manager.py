"""Utility functions to manage Ollama model processes.

This module centralises low-level interaction with the `ollama` CLI so that
API routers / services can reliably guarantee that the requested model is
running before returning control to the frontend.

All functions are **non-blocking** at FastAPI level (they use standard
`subprocess` calls executed in a thread via `asyncio.to_thread`).  Each helper
contains detailed logging so that operational problems can be diagnosed from
server logs.

Assumptions
-----------
1. Ollama daemon is already up and listening on http://localhost:11434.
2. The CLI is accessible via the `ollama` command in the PATH of the uvicorn
   process.
3. Only **one** model should run at a time for GPU memory reasons.  When a new
   model is requested we stop all previously running models first.

Note: The `--json` flag was removed from `ollama ps` recently, so we need to
parse the plain-text table output.
"""

from __future__ import annotations

import asyncio
import logging
import re
import shlex
import subprocess
from pathlib import Path
from typing import List

LOG = logging.getLogger(__name__)

OLLAMA_BIN = "ollama"
OLLAMA_HOST = "http://localhost:11434"

_TABLE_ROW_PATTERN = re.compile(r"^(?P<name>\S+)\s+(?P<status>\S+)")


async def _run(cmd: str, timeout: int = 30) -> str:
    """Run *cmd* in a background thread and return stdout.

    Args
    ----
    cmd: Command line string to execute.
    timeout: Seconds before killing the subprocess.
    """

    def _blocking() -> str:
        result = subprocess.run(
            shlex.split(cmd), capture_output=True, text=True, timeout=timeout
        )
        if result.returncode != 0:
            raise RuntimeError(result.stderr.strip() or result.stdout)
        return result.stdout

    return await asyncio.to_thread(_blocking)


async def list_running_models() -> List[str]:
    """Return a list of model names currently shown by ``ollama ps``."""
    try:
        output = await _run(f"{OLLAMA_BIN} ps")
        LOG.debug(f"ollama ps raw output: {repr(output)}")
    except Exception as exc:
        LOG.warning("Could not list ollama models: %s", exc)
        return []  # ë¹ˆ ëª©ë¡ ë°˜í™˜

    models: List[str] = []
    lines = output.strip().splitlines()
    LOG.debug(f"ollama ps lines: {lines}")
    
    for line_num, line in enumerate(lines):
        LOG.debug(f"Processing line {line_num}: {repr(line)}")
        
        # Skip header or empty lines - "NAME"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í—¤ë” ìŠ¤í‚µ
        if not line or line.strip().upper().startswith("NAME") or "ID" in line:
            LOG.debug(f"Skipping header/empty line: {repr(line)}")
            continue
            
        match = _TABLE_ROW_PATTERN.match(line)
        if match:
            name = match.group("name").strip()
            LOG.debug(f"Found model name: {repr(name)}")
            if name and name != "NAME":  # "NAME" í—¤ë” ì œì™¸
                models.append(name)
        else:
            LOG.debug(f"No match for line: {repr(line)}")
    
    LOG.info(f"Final running models list: {models}")
    # ë¹ˆ ê²°ê³¼ëŠ” ë¹ˆ ëª©ë¡ìœ¼ë¡œ ë°˜í™˜ (ê¸°ë³¸ê°’ ë°˜í™˜í•˜ì§€ ì•ŠìŒ)
    return models


def _is_embedding_model(model_name: str) -> bool:
    """Check if a model is an embedding model based on naming convention."""
    model_lower = model_name.lower()
    embedding_keywords = ['embed', 'embedding', 'mxbai']
    return any(keyword in model_lower for keyword in embedding_keywords)

async def stop_all_models(except_model: str | None = None) -> None:
    """Stop every running model except *except_model* and embedding models."""
    running = await list_running_models()
    tasks = []
    for model in running:
        if model == except_model:
            continue
        # ì„ë² ë”© ëª¨ë¸ì€ ì¤‘ì§€í•˜ì§€ ì•ŠìŒ (ë‹¤ì¤‘ ì‹¤í–‰ í—ˆìš©)
        if _is_embedding_model(model):
            LOG.info("Keeping embedding model '%s' running", model)
            continue
        LOG.info("Stopping Ollama model '%s'", model)
        tasks.append(_run(f"{OLLAMA_BIN} stop {shlex.quote(model)}"))
    if tasks:
        await asyncio.gather(*tasks, return_exceptions=True)


async def start_model(model: str) -> None:
    """Start *model* in detached mode if not already running."""
    running = await list_running_models()
    if model in running:
        LOG.info("Model '%s' already running", model)
        return

    LOG.info("Starting Ollama model '%s'â€¦", model)
    
    # ì„ë² ë”© ëª¨ë¸ê³¼ ì¼ë°˜ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ ì‹œì‘ ë°©ë²• ì‚¬ìš©
    if _is_embedding_model(model):
        LOG.info("Detected embedding model '%s', using API embeddings endpoint", model)
        # ì„ë² ë”© ëª¨ë¸ì€ APIë¥¼ í†µí•´ ì‹œì‘
        try:
            import httpx
            async with httpx.AsyncClient(timeout=30.0) as client:
                LOG.info("Attempting to load embedding model via API: %s", model)
                response = await client.post(
                    "http://localhost:11434/api/embeddings",
                    json={
                        "model": model,
                        "prompt": "test embedding load"
                    }
                )
                if response.status_code == 200:
                    LOG.info("Embedding model '%s' loaded successfully via API", model)
                    return
                else:
                    LOG.warning("API embedding load failed with status %d: %s", 
                              response.status_code, response.text)
        except Exception as e:
            LOG.warning("API embedding load failed: %s", e)
        
        # í´ë°±: pull ëª…ë ¹ì–´ë§Œ ì‹œë„
        start_attempts = [
            (f"{OLLAMA_BIN} pull {shlex.quote(model)}", 120),
        ]
    else:
        LOG.info("Detected chat model '%s', using generate endpoint", model)
        start_attempts = [
            # ì¼ë°˜ ëª¨ë¸: run with prompt (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
            (f"{OLLAMA_BIN} run {shlex.quote(model)} \"Hello\"", 30),
            # í´ë°±: pull ëª…ë ¹ì–´
            (f"{OLLAMA_BIN} pull {shlex.quote(model)}", 120),
        ]
    
    for cmd, timeout in start_attempts:
        try:
            LOG.info("Attempting to start model with command: %s", cmd)
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ë„ë¡ subprocess ì§ì ‘ ì‚¬ìš©
            process = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # ì§§ì€ ì‹œê°„ë§Œ ëŒ€ê¸° (ëª¨ë¸ ë¡œë”©ì´ ì‹œì‘ë˜ë©´ ì¶©ë¶„)
            try:
                await asyncio.wait_for(process.wait(), timeout=5)
            except asyncio.TimeoutError:
                # íƒ€ì„ì•„ì›ƒì€ ì •ìƒ - ëª¨ë¸ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘
                LOG.info("Model loading started in background")
            
        except Exception as e:
            LOG.warning("Failed with method: %s, error: %s", cmd, e)
            continue

    # Wait until model appears in ps (í™•ì¥ëœ readiness check ~90s max)
    LOG.info("Waiting for model '%s' to appear in running list...", model)
    for i in range(45):  # 45íšŒ * 2ì´ˆ = 90ì´ˆ
        await asyncio.sleep(2)
        current_running = await list_running_models()
        
        # ë””ë²„ê·¸: í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
        if i % 5 == 0:  # 10ì´ˆë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
            LOG.info("Current running models (attempt %d/45): %s", i + 1, current_running)
        
        if model in current_running:
            LOG.info("Model '%s' is now running (attempt %d/45)", model, i + 1)
            
            # ì¶”ê°€ ì•ˆì •ì„± í™•ì¸ - ëª‡ ì´ˆ ë” ëŒ€ê¸°í•˜ì—¬ ëª¨ë¸ì´ ì™„ì „íˆ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            LOG.info("Verifying model '%s' stability...", model)
            await asyncio.sleep(3)
            
            final_check = await list_running_models()
            if model in final_check:
                LOG.info("Model '%s' is stable and ready", model)
                return
            else:
                LOG.warning("Model '%s' disappeared during stability check, retrying...", model)
        else:
            LOG.debug("Model '%s' not yet running (attempt %d/45)", model, i + 1)
    
    # ìµœì¢… ì‹¤íŒ¨
    final_running = await list_running_models()
    raise RuntimeError(f"Timed-out waiting for model '{model}' to start. Current running models: {final_running}")


async def ensure_models_running(required: list[str]) -> None:
    """Ensure *required* models are running, stop all others."""
    running = await list_running_models()
    # Stop models not required
    await stop_all_models(except_model=None)
    for model in running:
        if model in required:
            continue
    # After stop_all_models all stopped; start required ones
    for model in required:
        await start_model(model)

async def ensure_single_model_running(model: str) -> None:
    """Guarantee that **only** *model* is running (excluding embedding models).

    1. Stop all other non-embedding models.
    2. Start *model* if necessary.
    3. Verify model is actually running.
    
    Note: Embedding models (mxbai-embed-large, etc.) are preserved and can run alongside.
    """
    LOG.info(f"ğŸ¯ ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰ ë³´ì¥ ì‹œì‘: {model}")
    
    # 1ë‹¨ê³„: ëª¨ë“  ë¹„ì„ë² ë”© ëª¨ë¸ ì¤‘ì§€ (ìš”ì²­ëœ ëª¨ë¸ ë° ì„ë² ë”© ëª¨ë¸ ì œì™¸)
    await stop_all_models(except_model=model)
    
    # 2ë‹¨ê³„: ìš”ì²­ëœ ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    running_models = await list_running_models()
    LOG.info(f"ğŸ“‹ ì¤‘ì§€ í›„ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {running_models}")
    
    # 3ë‹¨ê³„: ìš”ì²­ëœ ëª¨ë¸ ì‹œì‘ (ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹Œ ê²½ìš°)
    if model not in running_models:
        LOG.info(f"ğŸš€ ëª¨ë¸ {model} ì‹œì‘ ì¤‘...")
        await start_model(model)
    else:
        LOG.info(f"âœ… ëª¨ë¸ {model} ì´ë¯¸ ì‹¤í–‰ ì¤‘")
    
    # 4ë‹¨ê³„: ìµœì¢… í™•ì¸ - ìš”ì²­ëœ ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆëŠ”ì§€ ê²€ì¦
    final_running = await list_running_models()
    LOG.info(f"ğŸ“‹ ìµœì¢… ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ë“¤: {final_running}")
    
    if model not in final_running:
        raise RuntimeError(f"ëª¨ë¸ {model} ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì‹¤í–‰ ì¤‘: {final_running}")
    
    # ì„ë² ë”©ì´ ì•„ë‹Œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì´ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ê²½ê³  (ì„ë² ë”© ëª¨ë¸ì€ í—ˆìš©)
    other_running = [m for m in final_running if m != model and not _is_embedding_model(m)]
    if other_running:
        LOG.warning(f"âš ï¸ ë‹¤ë¥¸ ë¹„ì„ë² ë”© ëª¨ë¸ë“¤ì´ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘: {other_running}")
    
    # ì„ë² ë”© ëª¨ë¸ ëª©ë¡ ì¶œë ¥ (ì •ë³´ìš©)
    embedding_running = [m for m in final_running if _is_embedding_model(m)]
    if embedding_running:
        LOG.info(f"ğŸ”§ ì‹¤í–‰ ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ë“¤ (ìœ ì§€ë¨): {embedding_running}")
    
    LOG.info(f"âœ… ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰ ë³´ì¥ ì™„ë£Œ: {model}")
