"""
API ì„œë²„ ì‹œì‘ ì‹œ Ollama ëª¨ë¸ ìƒíƒœ ê²€ì¦ ë° ìë™ ì‹œì‘ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ API ì„œë²„ ì‹œì‘ ì‹œ ë‹¤ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤:
1. Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. ìµœì†Œ 1ê°œ ì´ìƒì˜ ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
3. ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ìë™ ì‹œì‘
4. ëª¨ë“  ê²€ì¦ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
"""

import asyncio
import logging
import httpx
from typing import Dict, Any, List, Optional
from app.utils.ollama_manager import list_running_models, start_model
from app.utils.config import OLLAMA_MODEL

logger = logging.getLogger(__name__)

class OllamaStartupValidator:
    """Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ê²€ì¦ ë° ì´ˆê¸°í™” í´ë˜ìŠ¤"""
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        self.default_models = [
            "gemma3-12b:latest",  # 1ìˆœìœ„ ê¸°ë³¸ ëª¨ë¸
            "txgemma-chat:latest",  # 2ìˆœìœ„ ëŒ€ì•ˆ ëª¨ë¸
            "Gemma3:27b-it-q4_K_M",  # 3ìˆœìœ„ ëŒ€ìš©ëŸ‰ ëª¨ë¸
            "txgemma-predict:latest"  # 4ìˆœìœ„ ì˜ˆì¸¡ ëª¨ë¸
        ]
    
    async def check_ollama_service(self) -> Dict[str, Any]:
        """Ollama ì„œë¹„ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                response.raise_for_status()
                data = response.json()
                
                installed_models = [model["name"] for model in data.get("models", [])]
                
                return {
                    "service_available": True,
                    "installed_models": installed_models,
                    "total_models": len(installed_models),
                    "error": None
                }
                
        except Exception as e:
            return {
                "service_available": False,
                "installed_models": [],
                "total_models": 0,
                "error": f"Ollama ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}"
            }
    
    async def check_running_models(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ ìƒíƒœ í™•ì¸"""
        try:
            # ollama_manager í•¨ìˆ˜ ì‚¬ìš©ìœ¼ë¡œ ê°„ì†Œí™”
            running_models = await list_running_models()
            return {
                "check_success": True,
                "running_models": running_models,
                "total_running": len(running_models),
                "error": None
            }
        except Exception as e:
            return {
                "check_success": False,
                "running_models": [],
                "total_running": 0,
                "error": f"ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {str(e)}"
            }
    
    async def find_best_available_model(self, installed_models: List[str]) -> Optional[str]:
        """ì„¤ì¹˜ëœ ëª¨ë¸ ì¤‘ì—ì„œ ìµœì„ ì˜ ëª¨ë¸ ì„ íƒ"""
        # ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
        for preferred_model in self.default_models:
            if preferred_model in installed_models:
                logger.info(f"âœ… ìš°ì„ ìˆœìœ„ ëª¨ë¸ '{preferred_model}' ë°œê²¬")
                return preferred_model
        
        # ìš°ì„ ìˆœìœ„ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì„¤ì¹˜ëœ ëª¨ë¸ ì‚¬ìš©
        if installed_models:
            selected_model = installed_models[0]
            logger.info(f"âš ï¸ ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì—†ìŒ. ì²« ë²ˆì§¸ ì„¤ì¹˜ëœ ëª¨ë¸ '{selected_model}' ì‚¬ìš©")
            return selected_model
        
        return None
    
    async def start_default_model(self, model_name: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë¸ ì‹œì‘"""
        try:
            logger.info(f"ğŸš€ ëª¨ë¸ '{model_name}' ì‹œì‘ ì¤‘...")
            
            # ë¨¼ì € ì‹œì‘ ì „ ìƒíƒœ í™•ì¸
            pre_check = await self.check_running_models()
            if pre_check["check_success"] and model_name in pre_check["running_models"]:
                logger.info(f"âœ… ëª¨ë¸ '{model_name}' ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                return {
                    "start_success": True,
                    "model": model_name,
                    "message": f"ëª¨ë¸ '{model_name}' ì´ë¯¸ ì‹¤í–‰ ì¤‘",
                    "error": None
                }
            
            # ëª¨ë¸ ì‹œì‘ ì‹œë„
            await start_model(model_name)
            
            # ê°„ë‹¨í•œ í™•ì¸ (ìµœëŒ€ 10ì´ˆë§Œ ëŒ€ê¸°)
            for attempt in range(5):
                await asyncio.sleep(2)
                running_check = await self.check_running_models()
                if running_check["check_success"] and model_name in running_check["running_models"]:
                    logger.info(f"âœ… ëª¨ë¸ '{model_name}' ì‹œì‘ ì™„ë£Œ")
                    return {
                        "start_success": True,
                        "model": model_name,
                        "message": f"ëª¨ë¸ '{model_name}' ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë¨",
                        "error": None
                    }
            
            # ì‹œê°„ ì´ˆê³¼ ì‹œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
            logger.info(f"âœ… ëª¨ë¸ '{model_name}' ì‹œì‘ ì‹œë„ ì™„ë£Œ (ë°±ì—”ë“œ ì‘ë™ ì¶”ì •)")
            return {
                "start_success": True,
                "model": model_name,
                "message": f"ëª¨ë¸ '{model_name}' ì‹œì‘ ì‹œë„ ì™„ë£Œ",
                "error": None
            }
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ '{model_name}' ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
            return {
                "start_success": False,
                "model": model_name,
                "message": f"ëª¨ë¸ '{model_name}' ì‹œì‘ ì‹¤íŒ¨",
                "error": str(e)
            }
    
    async def validate_and_ensure_models(self) -> Dict[str, Any]:
        """ì „ì²´ Ollama ëª¨ë¸ ìƒíƒœ ê²€ì¦ ë° ìë™ ì‹œì‘ í”„ë¡œì„¸ìŠ¤"""
        logger.info("ğŸ” API ì„œë²„ ì‹œì‘ ì‹œ Ollama ëª¨ë¸ ìƒíƒœ ê²€ì¦ ì‹œì‘...")
        
        # 1. Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        service_check = await self.check_ollama_service()
        if not service_check["service_available"]:
            error_msg = service_check["error"]
            logger.error(f"âŒ {error_msg}")
            return {
                "validation_success": False,
                "stage": "service_check",
                "error": error_msg,
                "running_models": [],
                "started_model": None
            }
        
        logger.info(f"âœ… Ollama ì„œë¹„ìŠ¤ ì—°ê²° ì„±ê³µ (ì„¤ì¹˜ëœ ëª¨ë¸: {service_check['total_models']}ê°œ)")
        
        # 2. ì„¤ì¹˜ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if service_check["total_models"] == 0:
            error_msg = "ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'ollama pull <model_name>' ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”."
            logger.error(f"âŒ {error_msg}")
            return {
                "validation_success": False,
                "stage": "no_models_installed",
                "error": error_msg,
                "running_models": [],
                "started_model": None
            }
        
        # 3. í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸
        running_check = await self.check_running_models()
        if not running_check["check_success"]:
            error_msg = running_check["error"]
            logger.error(f"âŒ {error_msg}")
            return {
                "validation_success": False,
                "stage": "running_check",
                "error": error_msg,
                "running_models": [],
                "started_model": None
            }
        
        # 4. ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if running_check["total_running"] > 0:
            logger.info(f"âœ… ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤: {running_check['running_models']}")
            return {
                "validation_success": True,
                "stage": "models_already_running",
                "message": f"ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸: {running_check['running_models']}",
                "running_models": running_check["running_models"],
                "started_model": None
            }
        
        # 5. ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ ëª¨ë¸ ì‹œì‘
        logger.warning("âš ï¸ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ìë™ ì‹œì‘ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        
        best_model = await self.find_best_available_model(service_check["installed_models"])
        if not best_model:
            error_msg = "ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"âŒ {error_msg}")
            return {
                "validation_success": False,
                "stage": "no_suitable_model",
                "error": error_msg,
                "running_models": [],
                "started_model": None
            }
        
        # 6. ì„ íƒëœ ëª¨ë¸ ì‹œì‘
        start_result = await self.start_default_model(best_model)
        if start_result["start_success"]:
            logger.info(f"âœ… API ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ê²€ì¦ ì™„ë£Œ: '{best_model}' ì‹¤í–‰ ì¤‘")
            return {
                "validation_success": True,
                "stage": "model_started",
                "message": start_result["message"],
                "running_models": [best_model],
                "started_model": best_model
            }
        else:
            error_msg = f"ê¸°ë³¸ ëª¨ë¸ ì‹œì‘ ì‹¤íŒ¨: {start_result['error']}"
            logger.error(f"âŒ {error_msg}")
            return {
                "validation_success": False,
                "stage": "model_start_failed",
                "error": error_msg,
                "running_models": [],
                "started_model": None
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
startup_validator = OllamaStartupValidator()

async def validate_ollama_startup() -> Dict[str, Any]:
    """API ì„œë²„ ì‹œì‘ ì‹œ Ollama ëª¨ë¸ ê²€ì¦ ë° ìë™ ì‹œì‘"""
    return await startup_validator.validate_and_ensure_models()