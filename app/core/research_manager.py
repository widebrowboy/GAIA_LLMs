#!/usr/bin/env python3
"""
ì—°êµ¬ ê´€ë¦¬ ëª¨ë“ˆ
ì‹ ì•½ê°œë°œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ì´ê´„ ê´€ë¦¬
"""

import asyncio
import os
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

from app.api.ollama_client import OllamaClient
from app.core.answer_evaluator import AnswerEvaluator
from app.core.file_storage import FileStorage
from app.core.answer_generator import AnswerGenerator
from app.core.question_handler import QuestionHandler
from app.core.research_parallel import ResearchParallel


class ResearchManager:
    """
    ì‹ ì•½ê°œë°œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤
    ì§ˆë¬¸ ì²˜ë¦¬ë¶€í„° ë‹µë³€ ìƒì„±, í”¼ë“œë°±, ì €ì¥ê¹Œì§€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
    """

    def __init__(self,
                ollama_client: Optional[OllamaClient] = None,
                feedback_depth: int = 2,
                feedback_width: int = 2,
                concurrent_research: int = 2,
                output_dir: Optional[str] = None):
        """
        ì—°êµ¬ ê´€ë¦¬ì ì´ˆê¸°í™”

        Args:
            ollama_client: OllamaClient ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
            feedback_depth: í”¼ë“œë°± ë£¨í”„ ê¹Šì´ (ê¸°ë³¸ê°’: 2)
            feedback_width: í”¼ë“œë°± ë£¨í”„ ë„ˆë¹„ (ê¸°ë³¸ê°’: 2)
            concurrent_research: ë™ì‹œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: 2)
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: environment OUTPUT_DIR)
        """
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.client = ollama_client or OllamaClient()
        self.question_handler = QuestionHandler(self.client)
        self.answer_generator = AnswerGenerator(self.client)
        self.evaluator = AnswerEvaluator(self.client)
        self.parallel_executor = ResearchParallel(self.client, concurrent_research)

        # í™˜ê²½ ì„¤ì •
        self.output_dir = output_dir or os.getenv('OUTPUT_DIR', './research_outputs')
        self.file_storage = FileStorage(self.output_dir)

        # í”¼ë“œë°± ì„¤ì •
        self.feedback_depth = max(1, min(feedback_depth, 10))  # 1~10 ë²”ìœ„ í™•ì¸
        self.feedback_width = max(1, min(feedback_width, 10))  # 1~10 ë²”ìœ„ í™•ì¸
        self.concurrent_research = concurrent_research

        # ì„¸ì…˜ ì‹ë³„ì ì„¤ì • (íƒ€ì„ìŠ¤íƒ¬í”„)
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ì‹¤í–‰ í†µê³„
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_questions": 0,
            "completed_questions": 0,
            "failed_questions": 0,
            "total_feedback_loops": 0
        }

    async def load_questions(self, questions_file: Optional[str] = None, questions: Optional[List[str]] = None) -> List[str]:
        """
        ì—°êµ¬ ì§ˆë¬¸ ë¡œë“œ

        Args:
            questions_file: ì§ˆë¬¸ ëª©ë¡ íŒŒì¼ ê²½ë¡œ (JSON ë˜ëŠ” í…ìŠ¤íŠ¸)
            questions: ì§ì ‘ ì œê³µëœ ì§ˆë¬¸ ëª©ë¡

        Returns:
            List[str]: ë¡œë“œëœ ì§ˆë¬¸ ëª©ë¡
        """
        if questions:
            return list(questions)

        if questions_file:
            return await self.question_handler.load_questions_from_file(questions_file)

        # ê¸°ë³¸ ì˜ˆì‹œ ì§ˆë¬¸
        return [
            "ì‹ ì•½ê°œë°œì—ì„œ ë¶„ì íƒ€ê²Ÿ ë°œêµ´ì˜ ì£¼ìš” ì ‘ê·¼ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì „ì„ìƒ ë…ì„±ì‹œí—˜ì˜ ì£¼ìš” ë‹¨ê³„ì™€ í‰ê°€ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í•­ì•”ì œ ê°œë°œì—ì„œ ë°”ì´ì˜¤ë§ˆì»¤ì˜ ì—­í• ê³¼ ì¤‘ìš”ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]

    async def run_research(self, questions: Optional[List[str]] = None, questions_file: Optional[str] = None) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ ëª©ë¡ì— ëŒ€í•œ ì—°êµ¬ ì§„í–‰

        Args:
            questions: ì§ì ‘ ì œê³µëœ ì§ˆë¬¸ ëª©ë¡
            questions_file: ì§ˆë¬¸ ëª©ë¡ íŒŒì¼ ê²½ë¡œ

        Returns:
            Dict[str, Any]: ì—°êµ¬ ê²°ê³¼ ìš”ì•½
        """
        # íƒ€ì´ë¨¸ ì‹œì‘
        self.stats["start_time"] = time.time()

        # ì§ˆë¬¸ ë¡œë“œ
        research_questions = await self.load_questions(questions_file, questions)
        self.stats["total_questions"] = len(research_questions)

        print(f"===== ì‹ ì•½ê°œë°œ ì—°êµ¬ ì‹œì‘ ({len(research_questions)}ê°œ ì§ˆë¬¸) =====")
        print(f"- í”¼ë“œë°± ë£¨í”„: ê¹Šì´={self.feedback_depth}, ë„ˆë¹„={self.feedback_width}")
        print(f"- ë™ì‹œ ì—°êµ¬ ì§„í–‰: {self.concurrent_research}ê°œ")

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
        research_dir = await self.file_storage.create_session_directory(self.session_id)
        print(f"- ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {research_dir}")

        # API ê°€ìš©ì„± í™•ì¸
        status = await self.client.check_availability()
        if status["status"] != "available":
            print(f"âŒ Ollama APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {status.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return {"error": "API ì‚¬ìš© ë¶ˆê°€", "stats": self.stats}

        print(f"\nğŸš€ {status['current_model']} ëª¨ë¸ ì‚¬ìš© ì¤‘...\n")

        # ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì—°êµ¬ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(self.concurrent_research)

        # ê° ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        async def research_question(idx: int, question: str):
            async with semaphore:
                qid = f"Q{idx+1:02d}"
                try:
                    print(f"\n[{qid}] ì—°êµ¬ ì‹œì‘: '{question[:50]}...'")

                    # ì—°êµ¬ ì§ˆë¬¸ì— ëŒ€í•œ ì²˜ë¦¬ ìˆ˜í–‰
                    result = await self.research_question(question, question_id=qid)

                    if 'error' in result:
                        print(f"[{qid}] âŒ ì—°êµ¬ ì‹¤íŒ¨: {result['error']}")
                        self.stats["failed_questions"] += 1
                        return {"question": question, "error": result['error'], "question_id": qid, "status": "failed"}

                    self.stats["total_feedback_loops"] += result.get("feedback_loops", 0)
                    self.stats["completed_questions"] += 1

                    print(f"[{qid}] âœ… ì—°êµ¬ ì™„ë£Œ (ì ìˆ˜: {result.get('score', 'N/A')}, ë£¨í”„: {result.get('feedback_loops', 0)}íšŒ)")

                    return result

                except Exception as e:
                    print(f"[{qid}] âŒ ì˜¤ë¥˜ ë°œìƒ: {e!s}")
                    self.stats["failed_questions"] += 1
                    return {"question": question, "question_id": qid, "error": str(e), "status": "failed"}

        # ëª¨ë“  ì§ˆë¬¸ ë™ì‹œ ì—°êµ¬ ìˆ˜í–‰
        time.time()
        tasks = [research_question(i, q) for i, q in enumerate(research_questions)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
        self.stats["end_time"] = time.time()
        elapsed = self.stats["end_time"] - self.stats["start_time"]

        # ìš”ì•½ ì •ë³´
        summary = {
            "session_id": self.session_id,
            "output_directory": research_dir,
            "total_questions": self.stats["total_questions"],
            "completed_questions": self.stats["completed_questions"],
            "failed_questions": self.stats["failed_questions"],
            "total_feedback_loops": self.stats["total_feedback_loops"],
            "elapsed_seconds": elapsed,
            "model": status["current_model"],
            "feedback_settings": {
                "depth": self.feedback_depth,
                "width": self.feedback_width
            },
            "results": results
        }

        # ìš”ì•½ ì •ë³´ ì €ì¥
        await self.file_storage.save_summary(summary, self.session_id)

        # ì™„ë£Œ ì •ë³´ ì¶œë ¥
        print("\n===== ì—°êµ¬ ì™„ë£Œ =====")
        print(f"- ì´ ì§ˆë¬¸: {self.stats['total_questions']}ê°œ")
        print(f"- ì™„ë£Œ: {self.stats['completed_questions']}ê°œ")
        print(f"- ì‹¤íŒ¨: {self.stats['failed_questions']}ê°œ")
        print(f"- ì´ í”¼ë“œë°± ë£¨í”„: {self.stats['total_feedback_loops']}íšŒ")
        print(f"- ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"- ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {research_dir}")

        return summary

    async def research_question(self, question: str, question_id: Optional[str] = None) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ ìˆ˜í–‰

        Args:
            question: ì—°êµ¬ ì§ˆë¬¸
            question_id: ì§ˆë¬¸ ì‹ë³„ì (ê¸°ë³¸ê°’: ìë™ ìƒì„±)

        Returns:
            Dict[str, Any]: ì—°êµ¬ ê²°ê³¼
        """
        # ì§ˆë¬¸ IDê°€ ì—†ìœ¼ë©´ ìƒì„±
        qid = question_id or f"Q{int(time.time())%1000:03d}"

        try:
            # ëŒ€ì²´ ë‹µë³€ ë³‘ë ¬ ìƒì„± (widthë§Œí¼)
            print(f"[{qid}] {self.feedback_width}ê°œì˜ ëŒ€ì²´ ë‹µë³€ ë³‘ë ¬ ìƒì„± ì¤‘...")
            template = "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì‹ ì•½ê°œë°œ ì „ë¬¸ê°€ë¡œì„œ ê³¼í•™ì  ê·¼ê±°ì™€ ì°¸ê³ ë¬¸í—Œì„ í¬í•¨í•˜ì—¬ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n{question}"
            system_prompt = """ë‹¹ì‹ ì€ ì‹ ì•½ê°œë°œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
            1. ë¬¸ì œ ì •ì˜
            2. í•µì‹¬ ë‚´ìš© (ì´ë¡ , ê°œë…, ì›ë¦¬)
            3. ê³¼í•™ì  ê·¼ê±° (ì—°êµ¬ ê²°ê³¼, ë°ì´í„°)
            4. ì„ìƒì‹œí—˜ ë° ê°œë°œ ë‹¨ê³„
            5. ê²°ë¡  ë° ìš”ì•½
            6. ì°¸ê³  ë¬¸í—Œ (ìµœì†Œ 2ê°œ)

            ë‹µë³€ì€ ìµœì†Œ 1000ì ì´ìƒì´ì–´ì•¼ í•˜ë©°, í•­ìƒ ê³¼í•™ì  ê·¼ê±°ì™€ ì°¸ê³ ë¬¸í—Œì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."""

            alternative_answers = await self.parallel_executor.generate_alternative_answers(
                question=question,
                prompt_template=template,
                system_prompt=system_prompt,
                width=self.feedback_width
            )

            # ê° ë‹µë³€ í‰ê°€ (ë³‘ë ¬)
            print(f"[{qid}] ëŒ€ì²´ ë‹µë³€ í‰ê°€ ì¤‘...")
            evaluation_tasks = []
            for answer in alternative_answers:
                evaluation_tasks.append(
                    self.evaluator.evaluate_answer(question=question, answer=answer)
                )

            evaluations = await asyncio.gather(*evaluation_tasks)

            # ìµœê³  ì ìˆ˜ì˜ ë‹µë³€ ì„ íƒ
            scores = [evaluation.get("score", 0) for evaluation in evaluations]
            best_idx = scores.index(max(scores))

            # í˜„ì¬ ë‹µë³€ ì„¤ì • (ìµœê³  ì ìˆ˜)
            current_answer = alternative_answers[best_idx]
            current_score = scores[best_idx]
            current_feedback = evaluations[best_idx].get("feedback", "")

            print(f"[{qid}] ìµœê³  ì ìˆ˜ ë‹µë³€ ì„ íƒ: {current_score}/10")

            # ë‹µë³€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            answer_history = [
                {
                    "iteration": 0,
                    "answer": current_answer,
                    "feedback": current_feedback,
                    "score": current_score,
                    "timestamp": datetime.now().isoformat()
                }
            ]

            # í”¼ë“œë°± ë£¨í”„ ì‹¤í–‰
            for i in range(1, self.feedback_depth + 1):
                # ì¢…ë£Œ ì¡°ê±´: ì¶©ë¶„íˆ ë†’ì€ ì ìˆ˜
                if current_score >= 9.0:  # 9ì  ì´ìƒì´ë©´ ì¶©ë¶„íˆ ì¢‹ì€ ë‹µë³€
                    print(f"[{qid}] âœ¨ ì¶©ë¶„íˆ ë†’ì€ ì ìˆ˜ ë‹¬ì„± ({current_score}/10), í”¼ë“œë°± ë£¨í”„ ì¢…ë£Œ")
                    break

                # ë§ˆì§€ë§‰ ë£¨í”„ê°€ ì•„ë‹Œ ê²½ìš° ê°œì„ ëœ ë‹µë³€ ìƒì„±
                print(f"[{qid}] í”¼ë“œë°± ë£¨í”„ {i}/{self.feedback_depth}: ë‹µë³€ ê°œì„  ì¤‘...")
                improved_answer = await self.answer_generator.improve_answer(
                    question=question,
                    previous_answer=current_answer,
                    feedback=current_feedback
                )

                # ê°œì„ ëœ ë‹µë³€ í‰ê°€
                evaluation = await self.evaluator.evaluate_answer(
                    question=question,
                    answer=improved_answer
                )

                # ì ìˆ˜ ë° í”¼ë“œë°± ì¶”ì¶œ
                score = evaluation.get("score", 0)
                feedback = evaluation.get("feedback", "")

                # ì ìˆ˜ê°€ ë” ë†’ì•„ì¡Œìœ¼ë©´ ì—…ë°ì´íŠ¸
                if score > current_score:
                    current_answer = improved_answer
                    current_score = score
                    current_feedback = feedback
                    print(f"[{qid}] ì ìˆ˜ í–¥ìƒ: {score}/10 â¬†ï¸")
                else:
                    print(f"[{qid}] ì ìˆ˜ ìœ ì§€/ê°ì†Œ: {score}/10 (ì´ì „ ë‹µë³€ ìœ ì§€)")

                # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                answer_history.append({
                    "iteration": i,
                    "answer": improved_answer,
                    "feedback": feedback,
                    "score": score,
                    "timestamp": datetime.now().isoformat()
                })

            # ê²°ê³¼ ì €ì¥
            result = {
                "question_id": qid,
                "question": question,
                "final_answer": current_answer,
                "score": current_score,
                "feedback_loops": len(answer_history) - 1,  # ì´ˆê¸° ë‹µë³€ ì œì™¸
                "answer_history": answer_history,
                "timestamp": datetime.now().isoformat()
            }

            # íŒŒì¼ë¡œ ì €ì¥
            await self.file_storage.save_research_result(result, self.session_id, qid)

            return result

        except Exception as e:
            # ì˜¤ë¥˜ ì²˜ë¦¬
            error_msg = f"ì—°êµ¬ ì˜¤ë¥˜: {e!s}"
            return {
                "question_id": qid,
                "question": question,
                "error": error_msg,
                "timestamp": datetime.now().isoformat()
            }

    async def conduct_research(self, questions: List[str], concurrent_limit: Optional[int] = None) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ ëª©ë¡ì— ëŒ€í•œ ì‹¬ì¸µ ì—°êµ¬ ìˆ˜í–‰ - GPU ê°€ì† ë° ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

        Args:
            questions: ì—°êµ¬í•  ì§ˆë¬¸ ëª©ë¡
            concurrent_limit: ë™ì‹œ ì—°êµ¬ ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’: self.concurrent_research)

        Returns:
            Dict[str, Any]: ì—°êµ¬ ê²°ê³¼ ìš”ì•½
        """
        start_time = time.time()

        # ì—°êµ¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì¤€ë¹„
        research_dir = os.path.join(self.output_dir, self.session_id)
        os.makedirs(research_dir, exist_ok=True)

        # ê° ì§ˆë¬¸ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        os.makedirs(os.path.join(research_dir, "questions"), exist_ok=True)

        # ì´ ì§ˆë¬¸ ìˆ˜ ê¸°ë¡
        stats = {
            "total_questions": len(questions),
            "completed": 0,
            "failed": 0,
            "total_loops": 0
        }

        # ê° ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ì •ì˜
        async def process_question(idx: int, q: str):
            qid = f"Q{idx+1:02d}"
            print(f"\n[{qid}] ì—°êµ¬ ì‹œì‘: '{q[:50]}...'")

            result = await self.research_question(q, qid)

            if 'error' in result:
                stats["failed"] += 1
                print(f"[{qid}] âŒ ì—°êµ¬ ì‹¤íŒ¨: {result['error']}")
            else:
                stats["completed"] += 1
                stats["total_loops"] += result.get("feedback_loops", 0)
                print(f"[{qid}] âœ… ì—°êµ¬ ì™„ë£Œ (ì ìˆ˜: {result.get('score', 'N/A')}/10)")

            return result

        # ResearchParallelì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        limit = concurrent_limit or self.concurrent_research
        results = await self.parallel_executor.process_questions_parallel(
            questions, process_question, limit
        )

        # ìš”ì•½ ì •ë³´
        elapsed = time.time() - start_time
        summary = {
            "session_id": self.session_id,
            "output_directory": research_dir,
            "total_questions": stats["total_questions"],
            "completed_questions": stats["completed"],
            "failed_questions": stats["failed"],
            "total_feedback_loops": stats["total_loops"],
            "elapsed_seconds": elapsed,
            "model": self.client.model,
            "feedback_settings": {
                "depth": self.feedback_depth,
                "width": self.feedback_width
            },
            "results": results
        }

        # ìš”ì•½ ì •ë³´ ì €ì¥
        await self.file_storage.save_summary(summary, self.session_id)

        # ì™„ë£Œ ì •ë³´ ì¶œë ¥
        print("\n===== ì‹¬ì¸µ ì—°êµ¬ ì™„ë£Œ =====")
        print(f"- ì´ ì§ˆë¬¸: {stats['total_questions']}ê°œ")
        print(f"- ì™„ë£Œ: {stats['completed']}ê°œ")
        print(f"- ì‹¤íŒ¨: {stats['failed']}ê°œ")
        print(f"- ì´ í”¼ë“œë°± ë£¨í”„: {stats['total_loops']}íšŒ")
        print(f"- ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print(f"- ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {research_dir}")

        return summary


# ëª¨ë“ˆ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ì‹ ì•½ê°œë°œ ì—°êµ¬ ì‹œìŠ¤í…œ')
    parser.add_argument('--questions', '-q', type=str, default=None,
                       help='ì§ˆë¬¸ ëª©ë¡ íŒŒì¼ ê²½ë¡œ (JSON ë˜ëŠ” í…ìŠ¤íŠ¸)')
    parser.add_argument('--depth', '-d', type=int, default=2,
                       help='í”¼ë“œë°± ë£¨í”„ ê¹Šì´ (ê¸°ë³¸ê°’: 2, ë²”ìœ„: 1-10)')
    parser.add_argument('--width', '-w', type=int, default=2,
                       help='í”¼ë“œë°± ë£¨í”„ ë„ˆë¹„ (ê¸°ë³¸ê°’: 2, ë²”ìœ„: 1-10)')
    parser.add_argument('--concurrent', '-c', type=int, default=2,
                       help='ë™ì‹œ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: 2)')

    args = parser.parse_args()

    async def main():
        # ì—°êµ¬ ê´€ë¦¬ì ìƒì„±
        manager = ResearchManager(
            feedback_depth=args.depth,
            feedback_width=args.width,
            concurrent_research=args.concurrent
        )

        # ì—°êµ¬ ì‹¤í–‰
        await manager.run_research(questions_file=args.questions)

    asyncio.run(main())
