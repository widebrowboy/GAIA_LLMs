"""
í”¼ë“œë°± ë²¡í„° ì €ìž¥ì†Œ
ì‚¬ìš©ìžì˜ ì¸ì—…/ì¸ë‹¤ìš´ í”¼ë“œë°±ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ìž¥í•˜ê³  ê´€ë¦¬
"""

import uuid
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from app.rag.embeddings import EmbeddingService

logger = logging.getLogger(__name__)


@dataclass
class FeedbackData:
    """í”¼ë“œë°± ë°ì´í„° êµ¬ì¡°"""
    id: str
    question_text: str
    answer_text: str
    feedback_type: str  # "positive" or "negative"
    question_embedding: List[float]
    answer_embedding: List[float]
    timestamp: str
    session_id: str
    user_id: str
    context_sources: List[str]
    model_version: str
    response_time: float
    confidence_score: float


@dataclass
class QAPairData:
    """ì§ˆë¬¸-ì‘ë‹µ ìŒ ë°ì´í„° êµ¬ì¡°"""
    id: str
    question_text: str
    answer_text: str
    question_embedding: List[float]
    answer_embedding: List[float]
    positive_feedback_count: int
    negative_feedback_count: int
    quality_score: float
    is_training_candidate: bool
    domain_tags: List[str]
    created_at: str
    updated_at: str


class FeedbackVectorStore:
    """í”¼ë“œë°± ë²¡í„° ì €ìž¥ì†Œ í´ëž˜ìŠ¤"""
    
    def __init__(self, 
                 use_server: bool = False,
                 host: str = "localhost",
                 port: int = 19530,
                 db_file: str = "./feedback_milvus.db"):
        self.use_server = use_server
        self.host = host
        self.port = port
        self.db_file = db_file
        self.embedding_service = EmbeddingService()
        self.embedding_dim = 1024  # mxbai-embed-large ì°¨ì› (1024ë¡œ í†µì¼)
        
        # ì»¬ë ‰ì…˜ ì´ë¦„
        self.feedback_collection_name = "feedback_collection"
        self.qa_pairs_collection_name = "qa_pairs_collection"
        
        self._connect()
        self._create_collections()
    
    def _connect(self):
        """Milvus ì—°ê²°"""
        try:
            if self.use_server:
                # Milvus ì„œë²„ ì—°ê²°
                connections.connect(
                    alias="default",
                    host=self.host,
                    port=self.port
                )
                logger.info(f"Connected to Milvus Server: {self.host}:{self.port}")
                logger.info("ðŸŒ Milvus ì›¹ UI: http://localhost:9091/webui")
            else:
                # Milvus Lite ì—°ê²° (ë¡œì»¬ íŒŒì¼ ê¸°ë°˜)
                connections.connect(
                    alias="default",
                    uri=self.db_file
                )
                logger.info(f"Connected to Milvus Lite: {self.db_file}")
                logger.info("ðŸ’¡ Milvus ì›¹ UIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì„œë²„ ëª¨ë“œë¡œ ì „í™˜í•˜ì„¸ìš”.")
        except Exception as e:
            logger.error(f"Failed to connect to Milvus: {e}")
            if self.use_server:
                logger.error("Milvus ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
                logger.error("./scripts/milvus_manager.sh start ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œìž‘í•˜ì„¸ìš”.")
            raise
    
    def _create_collections(self):
        """í”¼ë“œë°± ë° QA ìŒ ì»¬ë ‰ì…˜ ìƒì„±"""
        try:
            self._create_feedback_collection()
            self._create_qa_pairs_collection()
        except Exception as e:
            logger.error(f"Failed to create collections: {e}")
            raise
    
    def _create_feedback_collection(self):
        """í”¼ë“œë°± ì»¬ë ‰ì…˜ ìƒì„±"""
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=64, is_primary=True),
            FieldSchema(name="question_text", dtype=DataType.VARCHAR, max_length=4096),
            FieldSchema(name="answer_text", dtype=DataType.VARCHAR, max_length=8192),
            FieldSchema(name="feedback_type", dtype=DataType.VARCHAR, max_length=16),
            FieldSchema(name="question_embedding", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="answer_embedding", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="timestamp", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="session_id", dtype=DataType.VARCHAR, max_length=64),
            FieldSchema(name="user_id", dtype=DataType.VARCHAR, max_length=64),
            FieldSchema(name="context_sources", dtype=DataType.VARCHAR, max_length=2048),
            FieldSchema(name="model_version", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="response_time", dtype=DataType.FLOAT),
            FieldSchema(name="confidence_score", dtype=DataType.FLOAT),
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="ì‚¬ìš©ìž í”¼ë“œë°± ì €ìž¥ì†Œ"
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± (ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        if not utility.has_collection(self.feedback_collection_name):
            self.feedback_collection = Collection(
                name=self.feedback_collection_name,
                schema=schema
            )
            
            # ì¸ë±ìŠ¤ ìƒì„±
            self.feedback_collection.create_index(
                field_name="question_embedding",
                index_params={
                    "metric_type": "IP",  # Inner Product (Cosine similarity)
                    "index_type": "FLAT"
                }
            )
            
            self.feedback_collection.create_index(
                field_name="answer_embedding",
                index_params={
                    "metric_type": "IP",
                    "index_type": "FLAT"
                }
            )
            
            logger.info(f"Created feedback collection: {self.feedback_collection_name}")
        else:
            self.feedback_collection = Collection(self.feedback_collection_name)
            logger.info(f"Loaded existing feedback collection: {self.feedback_collection_name}")
    
    def _create_qa_pairs_collection(self):
        """QA ìŒ ì»¬ë ‰ì…˜ ìƒì„±"""
        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=64, is_primary=True),
            FieldSchema(name="question_text", dtype=DataType.VARCHAR, max_length=4096),
            FieldSchema(name="answer_text", dtype=DataType.VARCHAR, max_length=8192),
            FieldSchema(name="question_embedding", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="answer_embedding", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="positive_feedback_count", dtype=DataType.INT64),
            FieldSchema(name="negative_feedback_count", dtype=DataType.INT64),
            FieldSchema(name="quality_score", dtype=DataType.FLOAT),
            FieldSchema(name="is_training_candidate", dtype=DataType.BOOL),
            FieldSchema(name="domain_tags", dtype=DataType.VARCHAR, max_length=512),
            FieldSchema(name="created_at", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="updated_at", dtype=DataType.VARCHAR, max_length=32),
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="ì§ˆë¬¸-ì‘ë‹µ ìŒ ë° í’ˆì§ˆ ë°ì´í„°"
        )
        
        if not utility.has_collection(self.qa_pairs_collection_name):
            self.qa_pairs_collection = Collection(
                name=self.qa_pairs_collection_name,
                schema=schema
            )
            
            # ì¸ë±ìŠ¤ ìƒì„±
            self.qa_pairs_collection.create_index(
                field_name="question_embedding",
                index_params={
                    "metric_type": "IP",
                    "index_type": "FLAT"
                }
            )
            
            self.qa_pairs_collection.create_index(
                field_name="answer_embedding",
                index_params={
                    "metric_type": "IP",
                    "index_type": "FLAT"
                }
            )
            
            logger.info(f"Created QA pairs collection: {self.qa_pairs_collection_name}")
        else:
            self.qa_pairs_collection = Collection(self.qa_pairs_collection_name)
            logger.info(f"Loaded existing QA pairs collection: {self.qa_pairs_collection_name}")
    
    def check_duplicate_feedback(self, 
                                question: str, 
                                answer: str, 
                                similarity_threshold: float = 0.95) -> Optional[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ í”¼ë“œë°±ì´ ì´ë¯¸ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
        
        Args:
            question: ì‚¬ìš©ìž ì§ˆë¬¸
            answer: AI ì‘ë‹µ
            similarity_threshold: ìœ ì‚¬ë„ ìž„ê³„ê°’ (0.95 ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨)
            
        Returns:
            ì¤‘ë³µëœ í”¼ë“œë°± ì •ë³´ ë˜ëŠ” None
        """
        try:
            # ì§ˆë¬¸ ìž„ë² ë”© ìƒì„±
            question_embedding = self.embedding_service.embed_text(question)
            answer_embedding = self.embedding_service.embed_text(answer)
            
            # í”¼ë“œë°± ì»¬ë ‰ì…˜ ë¡œë“œ
            self.feedback_collection.load()
            
            search_params = {
                "metric_type": "IP",
                "params": {"nprobe": 10}
            }
            
            # ì§ˆë¬¸ ê¸°ì¤€ ìœ ì‚¬ í”¼ë“œë°± ê²€ìƒ‰
            question_results = self.feedback_collection.search(
                data=[question_embedding],
                anns_field="question_embedding",
                param=search_params,
                limit=5,
                expr=None,
                output_fields=["id", "question_text", "answer_text", "feedback_type", "timestamp"]
            )
            
            # ë‹µë³€ ê¸°ì¤€ ìœ ì‚¬ í”¼ë“œë°± ê²€ìƒ‰
            answer_results = self.feedback_collection.search(
                data=[answer_embedding],
                anns_field="answer_embedding",
                param=search_params,
                limit=5,
                expr=None,
                output_fields=["id", "question_text", "answer_text", "feedback_type", "timestamp"]
            )
            
            # ìœ ì‚¬ë„ ìž„ê³„ê°’ ì´ìƒì¸ í•­ëª© í™•ì¸
            for results, search_type in [(question_results, "ì§ˆë¬¸"), (answer_results, "ë‹µë³€")]:
                if results and len(results[0]) > 0:
                    for hit in results[0]:
                        if hit.distance >= similarity_threshold:
                            logger.info(f"ì¤‘ë³µ í”¼ë“œë°± ë°œê²¬ ({search_type} ê¸°ì¤€): ìœ ì‚¬ë„ {hit.distance:.3f}")
                            return {
                                "duplicate_id": hit.id,
                                "similarity": hit.distance,
                                "search_type": search_type,
                                "existing_question": hit.entity.get("question_text"),
                                "existing_answer": hit.entity.get("answer_text"),
                                "existing_feedback": hit.entity.get("feedback_type"),
                                "existing_timestamp": hit.entity.get("timestamp")
                            }
            
            return None
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return None
    
    def store_feedback(self, 
                      question: str,
                      answer: str,
                      feedback_type: str,
                      session_id: str = None,
                      user_id: str = None,
                      context_sources: List[str] = None,
                      model_version: str = "gemma3-12b",
                      response_time: float = 0.0,
                      confidence_score: float = 0.0,
                      check_duplicates: bool = True,
                      similarity_threshold: float = 0.95) -> Dict[str, Any]:
        """
        í”¼ë“œë°± ë°ì´í„° ì €ìž¥
        
        Args:
            question: ì‚¬ìš©ìž ì§ˆë¬¸
            answer: AI ì‘ë‹µ
            feedback_type: "positive" or "negative"
            session_id: ì„¸ì…˜ ID
            user_id: ì‚¬ìš©ìž ID
            context_sources: RAG ì»¨í…ìŠ¤íŠ¸ ì†ŒìŠ¤ë“¤
            model_version: ëª¨ë¸ ë²„ì „
            response_time: ì‘ë‹µ ìƒì„± ì‹œê°„
            confidence_score: ëª¨ë¸ ìžì‹ ê° ì ìˆ˜
            check_duplicates: ì¤‘ë³µ ê²€ì‚¬ ì—¬ë¶€
            similarity_threshold: ìœ ì‚¬ë„ ìž„ê³„ê°’
            
        Returns:
            ì €ìž¥ ê²°ê³¼ ì •ë³´ (Dict)
        """
        try:
            # ì¤‘ë³µ ê²€ì‚¬ ìˆ˜í–‰
            if check_duplicates:
                duplicate_info = self.check_duplicate_feedback(
                    question, answer, similarity_threshold
                )
                if duplicate_info:
                    logger.info(f"ì¤‘ë³µ í”¼ë“œë°±ìœ¼ë¡œ ì¸í•´ ì €ìž¥ ê±´ë„ˆëœ€: {duplicate_info['duplicate_id']}")
                    return {
                        "status": "duplicate",
                        "message": f"ìœ ì‚¬í•œ í”¼ë“œë°±ì´ ì´ë¯¸ ì¡´ìž¬í•©ë‹ˆë‹¤ (ìœ ì‚¬ë„: {duplicate_info['similarity']:.3f})",
                        "duplicate_info": duplicate_info,
                        "feedback_id": None
                    }
            
            # ìž„ë² ë”© ìƒì„±
            question_embedding = self.embedding_service.embed_text(question)
            answer_embedding = self.embedding_service.embed_text(answer)
            
            # í”¼ë“œë°± ë°ì´í„° ìƒì„±
            feedback_id = str(uuid.uuid4())
            feedback_data = FeedbackData(
                id=feedback_id,
                question_text=question,
                answer_text=answer,
                feedback_type=feedback_type,
                question_embedding=question_embedding,
                answer_embedding=answer_embedding,
                timestamp=datetime.now().isoformat(),
                session_id=session_id or str(uuid.uuid4()),
                user_id=user_id or "anonymous",
                context_sources=context_sources or [],
                model_version=model_version,
                response_time=response_time,
                confidence_score=confidence_score
            )
            
            # Milvus í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            entities = [
                [feedback_data.id],
                [feedback_data.question_text],
                [feedback_data.answer_text],
                [feedback_data.feedback_type],
                [feedback_data.question_embedding],
                [feedback_data.answer_embedding],
                [feedback_data.timestamp],
                [feedback_data.session_id],
                [feedback_data.user_id],
                [str(feedback_data.context_sources)],  # JSON ë¬¸ìžì—´ë¡œ ì €ìž¥
                [feedback_data.model_version],
                [feedback_data.response_time],
                [feedback_data.confidence_score]
            ]
            
            # ë°ì´í„° ì‚½ìž…
            self.feedback_collection.insert(entities)
            self.feedback_collection.flush()
            
            # QA ìŒ ë°ì´í„° ì—…ë°ì´íŠ¸
            self._update_qa_pair(question, answer, feedback_type, question_embedding, answer_embedding)
            
            logger.info(f"Stored feedback: {feedback_id} ({feedback_type})")
            return {
                "status": "success",
                "message": "í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "feedback_id": feedback_id,
                "duplicate_info": None
            }
            
        except Exception as e:
            logger.error(f"Failed to store feedback: {e}")
            return {
                "status": "error",
                "message": f"í”¼ë“œë°± ì €ìž¥ ì‹¤íŒ¨: {str(e)}",
                "feedback_id": None,
                "duplicate_info": None
            }
    
    def _update_qa_pair(self, 
                       question: str, 
                       answer: str, 
                       feedback_type: str,
                       question_embedding: List[float],
                       answer_embedding: List[float]):
        """QA ìŒ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ QA ìŒ ê²€ìƒ‰ (ì§ˆë¬¸ ê¸°ì¤€ ìœ ì‚¬ì„±)
            self.qa_pairs_collection.load()
            
            search_params = {
                "metric_type": "IP",
                "params": {"nprobe": 10}
            }
            
            results = self.qa_pairs_collection.search(
                data=[question_embedding],
                anns_field="question_embedding",
                param=search_params,
                limit=1,
                expr=None,
                output_fields=["id", "positive_feedback_count", "negative_feedback_count", "quality_score"]
            )
            
            if results and len(results[0]) > 0 and results[0][0].distance > 0.95:  # ë†’ì€ ìœ ì‚¬ì„±
                # ê¸°ì¡´ QA ìŒ ì—…ë°ì´íŠ¸
                existing_qa = results[0][0]
                qa_id = existing_qa.entity.get("id")
                
                positive_count = existing_qa.entity.get("positive_feedback_count", 0)
                negative_count = existing_qa.entity.get("negative_feedback_count", 0)
                
                if feedback_type == "positive":
                    positive_count += 1
                else:
                    negative_count += 1
                
                # í’ˆì§ˆ ì ìˆ˜ ìž¬ê³„ì‚°
                total_feedback = positive_count + negative_count
                positive_ratio = positive_count / total_feedback if total_feedback > 0 else 0
                quality_score = self._calculate_quality_score(positive_ratio)
                
                # TODO: ì‹¤ì œë¡œëŠ” UPDATE ì¿¼ë¦¬ í•„ìš” (í˜„ìž¬ Milvus LiteëŠ” ì œí•œì )
                logger.info(f"Would update QA pair {qa_id}: +{positive_count}, -{negative_count}, quality: {quality_score}")
                
            else:
                # ìƒˆë¡œìš´ QA ìŒ ìƒì„±
                qa_id = str(uuid.uuid4())
                positive_count = 1 if feedback_type == "positive" else 0
                negative_count = 1 if feedback_type == "negative" else 0
                quality_score = self._calculate_quality_score(positive_count / (positive_count + negative_count))
                
                qa_data = QAPairData(
                    id=qa_id,
                    question_text=question,
                    answer_text=answer,
                    question_embedding=question_embedding,
                    answer_embedding=answer_embedding,
                    positive_feedback_count=positive_count,
                    negative_feedback_count=negative_count,
                    quality_score=quality_score,
                    is_training_candidate=quality_score >= 0.8,
                    domain_tags=self._extract_domain_tags(question + " " + answer),
                    created_at=datetime.now().isoformat(),
                    updated_at=datetime.now().isoformat()
                )
                
                # QA ìŒ ì‚½ìž…
                entities = [
                    [qa_data.id],
                    [qa_data.question_text],
                    [qa_data.answer_text],
                    [qa_data.question_embedding],
                    [qa_data.answer_embedding],
                    [qa_data.positive_feedback_count],
                    [qa_data.negative_feedback_count],
                    [qa_data.quality_score],
                    [qa_data.is_training_candidate],
                    [str(qa_data.domain_tags)],
                    [qa_data.created_at],
                    [qa_data.updated_at]
                ]
                
                self.qa_pairs_collection.insert(entities)
                self.qa_pairs_collection.flush()
                
                logger.info(f"Created new QA pair: {qa_id} (quality: {quality_score})")
                
        except Exception as e:
            logger.error(f"Failed to update QA pair: {e}")
    
    def _calculate_quality_score(self, positive_ratio: float) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ë²„ì „: ê¸ì • í”¼ë“œë°± ë¹„ìœ¨ ê¸°ë°˜
        # ì¶”í›„ ë” ë³µìž¡í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í™•ìž¥ ê°€ëŠ¥
        return positive_ratio
    
    def _extract_domain_tags(self, text: str) -> List[str]:
        """ë„ë©”ì¸ íƒœê·¸ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        domain_keywords = {
            "oncology": ["cancer", "tumor", "oncology", "EGFR", "chemotherapy", "radiation"],
            "cardiology": ["heart", "cardiac", "cardiovascular", "hypertension"],
            "neurology": ["brain", "neurological", "alzheimer", "parkinson"],
            "clinical_trial": ["trial", "study", "phase", "clinical", "randomized"],
            "drug_development": ["drug", "compound", "molecule", "pharmaceutical", "therapy"]
        }
        
        tags = []
        text_lower = text.lower()
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                tags.append(domain)
        
        return tags[:5]  # ìµœëŒ€ 5ê°œ íƒœê·¸
    
    def search_similar_feedback(self, 
                               question: str, 
                               feedback_type: str = None,
                               limit: int = 10) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ í”¼ë“œë°± ê²€ìƒ‰"""
        try:
            question_embedding = self.embedding_service.embed_text(question)
            
            self.feedback_collection.load()
            
            search_params = {
                "metric_type": "IP",
                "params": {"nprobe": 10}
            }
            
            # í”¼ë“œë°± íƒ€ìž… í•„í„°
            expr = f'feedback_type == "{feedback_type}"' if feedback_type else None
            
            results = self.feedback_collection.search(
                data=[question_embedding],
                anns_field="question_embedding",
                param=search_params,
                limit=limit,
                expr=expr,
                output_fields=["question_text", "answer_text", "feedback_type", "timestamp", "quality_score"]
            )
            
            similar_feedback = []
            if results and len(results[0]) > 0:
                for hit in results[0]:
                    similar_feedback.append({
                        "question": hit.entity.get("question_text"),
                        "answer": hit.entity.get("answer_text"),
                        "feedback_type": hit.entity.get("feedback_type"),
                        "timestamp": hit.entity.get("timestamp"),
                        "similarity": hit.distance,
                        "id": hit.id
                    })
            
            return similar_feedback
            
        except Exception as e:
            logger.error(f"Failed to search similar feedback: {e}")
            return []
    
    def get_training_candidates(self, 
                               min_quality_score: float = 0.8,
                               limit: int = 1000) -> List[Dict[str, Any]]:
        """íŒŒì¸íŠœë‹ í›„ë³´ ë°ì´í„° ì¡°íšŒ"""
        try:
            self.qa_pairs_collection.load()
            
            # ë†’ì€ í’ˆì§ˆ ì ìˆ˜ì˜ QA ìŒ ê²€ìƒ‰
            expr = f"quality_score >= {min_quality_score} && is_training_candidate == true"
            
            results = self.qa_pairs_collection.query(
                expr=expr,
                output_fields=["question_text", "answer_text", "quality_score", "positive_feedback_count", "domain_tags"],
                limit=limit
            )
            
            training_data = []
            for result in results:
                training_data.append({
                    "instruction": "ì‹ ì•½ê°œë°œ ì „ë¬¸ AIë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.",
                    "input": result["question_text"],
                    "output": result["answer_text"],
                    "quality_score": result["quality_score"],
                    "feedback_count": result["positive_feedback_count"],
                    "domain": result["domain_tags"]
                })
            
            return training_data
            
        except Exception as e:
            logger.error(f"Failed to get training candidates: {e}")
            return []
    
    def get_feedback_stats(self) -> Dict[str, Any]:
        """í”¼ë“œë°± í†µê³„ ì¡°íšŒ"""
        try:
            self.feedback_collection.load()
            self.qa_pairs_collection.load()
            
            # ì „ì²´ í”¼ë“œë°± ìˆ˜
            total_feedback = self.feedback_collection.num_entities
            
            # ê¸ì •/ë¶€ì • í”¼ë“œë°± ìˆ˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì¿¼ë¦¬ í•„ìš”)
            positive_results = self.feedback_collection.query(
                expr='feedback_type == "positive"',
                output_fields=["id"],
                limit=10000
            )
            
            negative_results = self.feedback_collection.query(
                expr='feedback_type == "negative"',
                output_fields=["id"],
                limit=10000
            )
            
            positive_count = len(positive_results)
            negative_count = len(negative_results)
            
            # QA ìŒ ìˆ˜
            total_qa_pairs = self.qa_pairs_collection.num_entities
            
            # íŒŒì¸íŠœë‹ í›„ë³´ ìˆ˜
            training_candidates = self.qa_pairs_collection.query(
                expr="is_training_candidate == true",
                output_fields=["id"],
                limit=10000
            )
            
            return {
                "total_feedback": total_feedback,
                "positive_feedback": positive_count,
                "negative_feedback": negative_count,
                "positive_ratio": positive_count / total_feedback if total_feedback > 0 else 0,
                "total_qa_pairs": total_qa_pairs,
                "training_candidates": len(training_candidates),
                "collections": {
                    "feedback_collection": self.feedback_collection_name,
                    "qa_pairs_collection": self.qa_pairs_collection_name
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get feedback stats: {e}")
            return {"error": str(e)}