"""
title: GAIA-BT MCP Integration Pipeline
author: GAIA-BT Team
date: 2024-06-18
version: 1.0
license: MIT
description: Deep Research Mode with MCP integration for drug development
requirements: requests, asyncio, subprocess
"""

import os
import sys
import json
import asyncio
import subprocess
from typing import List, Optional, Iterator, Dict, Any
from pydantic import BaseModel, Field

# Add GAIA-BT to Python path
GAIA_BT_PATHS = [
    os.environ.get("GAIA_BT_PATH", "/gaia-bt"),
    "/home/gaia-bt/workspace/GAIA_LLMs",
    "/app/gaia-bt",
    "/gaia-bt"
]

for path in GAIA_BT_PATHS:
    if path and os.path.exists(path) and path not in sys.path:
        sys.path.insert(0, path)

try:
    from app.cli.chatbot import DrugDevelopmentChatbot
    from app.core.research_manager import ResearchManager
    from app.cli.mcp_commands import MCPCommands
    from app.utils.prompt_manager import get_prompt_manager, get_system_prompt
    from app.utils.config import Config
    GAIA_BT_AVAILABLE = True
    print("âœ… GAIA-BT modules imported successfully")
except ImportError as e:
    print(f"âš ï¸ GAIA-BT modules not available: {e}")
    print(f"   Python path: {sys.path[:3]}...")
    GAIA_BT_AVAILABLE = False


class Pipeline:
    def __init__(self):
        self.name = "GAIA-BT MCP Pipeline"
        self.description = "ì‹ ì•½ê°œë°œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ with Deep Research Mode"
        
        # Initialize valves (configuration)
        self.valves = self.Valves(
            **{
                "GAIA_BT_MODE": "normal",
                "MCP_OUTPUT_ENABLED": False,
                "SELECTED_PROMPT_MODE": "default",
                "DEFAULT_MODEL": "gemma3:27b-it-q4_K_M",
                "ENABLE_DEEP_SEARCH": True,
                "DEBUG_MODE": False
            }
        )
        
        # Initialize GAIA-BT components if available
        if GAIA_BT_AVAILABLE:
            try:
                # Initialize configuration
                self.config = Config()
                
                # Initialize chatbot with proper async setup
                self.chatbot = DrugDevelopmentChatbot()
                
                # Initialize research manager
                self.research_manager = ResearchManager()
                
                # Initialize MCP commands
                self.mcp_commands = MCPCommands()
                
                # Initialize prompt manager
                self.prompt_manager = get_prompt_manager()
                
                print("âœ… GAIA-BT components initialized successfully")
                print(f"   Config loaded: {self.config.OLLAMA_MODEL}")
                print(f"   Prompt manager: {len(self.prompt_manager.prompts)} prompts available")
                
            except Exception as e:
                print(f"âš ï¸ Failed to initialize GAIA-BT components: {e}")
                import traceback
                print(f"   Traceback: {traceback.format_exc()}")
                self.chatbot = None
                self.research_manager = None
                self.mcp_commands = None
                self.prompt_manager = None
        else:
            self.chatbot = None
            self.research_manager = None
            self.mcp_commands = None
            self.prompt_manager = None

    class Valves(BaseModel):
        GAIA_BT_MODE: str = Field(
            default="normal", 
            description="GAIA-BT Operation Mode",
            enum=["normal", "deep_research"]
        )
        MCP_OUTPUT_ENABLED: bool = Field(
            default=False, 
            description="Show MCP search process output"
        )
        SELECTED_PROMPT_MODE: str = Field(
            default="default", 
            description="Prompt specialization mode",
            enum=["default", "clinical", "research", "chemistry", "regulatory"]
        )
        DEFAULT_MODEL: str = Field(
            default="gemma3:27b-it-q4_K_M",
            description="Default model for GAIA-BT"
        )
        ENABLE_DEEP_SEARCH: bool = Field(
            default=True,
            description="Enable MCP Deep Search functionality"
        )
        DEBUG_MODE: bool = Field(
            default=False,
            description="Enable debug logging"
        )

    async def on_startup(self):
        """Pipeline startup initialization"""
        print(f"ğŸ§¬ GAIA-BT MCP Pipeline v1.0 initialized")
        print(f"   Mode: {self.valves.GAIA_BT_MODE}")
        print(f"   Prompt Mode: {self.valves.SELECTED_PROMPT_MODE}")
        print(f"   Default Model: {self.valves.DEFAULT_MODEL}")
        print(f"   Deep Search: {'Enabled' if self.valves.ENABLE_DEEP_SEARCH else 'Disabled'}")
        print(f"   GAIA-BT Available: {'Yes' if GAIA_BT_AVAILABLE else 'No'}")
        
        # Test GAIA-BT connectivity
        if GAIA_BT_AVAILABLE and self.chatbot:
            try:
                # Test basic functionality
                result = await self._test_gaia_bt_connection()
                if result:
                    print("âœ… GAIA-BT connection test successful")
                else:
                    print("âš ï¸ GAIA-BT connection test failed")
            except Exception as e:
                print(f"âš ï¸ GAIA-BT connection error: {e}")

    async def on_shutdown(self):
        """Pipeline shutdown cleanup"""
        print("ğŸ§¬ GAIA-BT MCP Pipeline shutdown")

    def pipe(
        self, 
        prompt: str = None, 
        **kwargs
    ) -> Iterator[str]:
        """Main pipeline processing function"""
        
        if self.valves.DEBUG_MODE:
            yield f"ğŸ”§ [DEBUG] Processing prompt in {self.valves.GAIA_BT_MODE} mode\n"
            yield f"ğŸ”§ [DEBUG] Prompt: {prompt[:100]}...\n"
        
        # Check if GAIA-BT is available
        if not GAIA_BT_AVAILABLE:
            yield "âš ï¸ GAIA-BT ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Mock ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.\n\n"
            yield from self._generate_mock_response(prompt)
            return
        
        # GAIA-BT ë°°ë„ˆ ì¶œë ¥
        yield from self._generate_gaia_banner()
        
        # Process based on mode
        if self.valves.GAIA_BT_MODE == "deep_research":
            yield from self._process_deep_research_mode(prompt, **kwargs)
        else:
            yield from self._process_normal_mode(prompt, **kwargs)

    def _generate_gaia_banner(self) -> Iterator[str]:
        """Generate GAIA-BT branded banner"""
        mode_emoji = "ğŸ”¬" if self.valves.GAIA_BT_MODE == "deep_research" else "ğŸ’¬"
        mode_name = "Deep Research Mode" if self.valves.GAIA_BT_MODE == "deep_research" else "Normal Mode"
        
        banner = f"""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸ§¬ GAIA-BT v2.0 Alpha ì‹ ì•½ê°œë°œ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸      â”‚
â”‚ {mode_emoji} Mode: {mode_name:<35} â”‚
â”‚ ğŸ“‹ Prompt: {self.valves.SELECTED_PROMPT_MODE:<33} â”‚
â”‚ ğŸ¤– Model: {self.valves.DEFAULT_MODEL:<34} â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

"""
        yield banner

    def _process_normal_mode(self, prompt: str, **kwargs) -> Iterator[str]:
        """Process in normal mode"""
        yield "ğŸ’¬ **ì¼ë°˜ ëª¨ë“œ**ë¡œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...\n\n"
        
        try:
            if self.chatbot and hasattr(self.chatbot, 'process_user_input'):
                # Use GAIA-BT chatbot's process_user_input method
                yield "ğŸ§¬ GAIA-BT ì‹ ì•½ê°œë°œ AI ë¶„ì„ ì¤‘...\n\n"
                
                # Get system prompt based on selected mode
                if self.prompt_manager:
                    system_prompt = get_system_prompt(self.valves.SELECTED_PROMPT_MODE)
                    if system_prompt:
                        yield f"ğŸ“‹ **í”„ë¡¬í”„íŠ¸ ëª¨ë“œ**: {self.valves.SELECTED_PROMPT_MODE}\n\n"
                
                # Process through GAIA-BT chatbot
                # Note: This would need async handling in real implementation
                try:
                    # For now, use a simplified approach
                    response = self._call_gaia_bt_sync(prompt)
                    yield response
                except Exception as chatbot_error:
                    yield f"âš ï¸ GAIA-BT ì²˜ë¦¬ ì‹¤íŒ¨: {str(chatbot_error)}\n"
                    yield from self._generate_specialized_mock_response(prompt)
            else:
                yield from self._generate_specialized_mock_response(prompt)
        except Exception as e:
            yield f"âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n"
            yield from self._generate_specialized_mock_response(prompt)

    def _process_deep_research_mode(self, prompt: str, **kwargs) -> Iterator[str]:
        """Process in deep research mode with MCP integration"""
        yield "ğŸ”¬ **Deep Research Mode**ë¡œ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...\n\n"
        
        if not self.valves.ENABLE_DEEP_SEARCH:
            yield "âš ï¸ Deep Search ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            yield from self._process_normal_mode(prompt, **kwargs)
            return
        
        try:
            # Show MCP search process if enabled
            if self.valves.MCP_OUTPUT_ENABLED:
                yield "ğŸ” **MCP í†µí•© ê²€ìƒ‰ ì‹œì‘**\n"
                yield "```\n"
                yield "ğŸ”— BiomCP: PubMed/ì„ìƒì‹œí—˜ ê²€ìƒ‰ ì¤‘...\n"
                yield "ğŸ”— ChEMBL: í™”í•™êµ¬ì¡°/ì•½ë¬¼ìƒí˜¸ì‘ìš© ë¶„ì„ ì¤‘...\n"
                yield "ğŸ”— Sequential Thinking: ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘...\n"
                yield "```\n\n"
            
            if self.mcp_commands and self.research_manager:
                # Perform integrated MCP search
                research_results = self._perform_integrated_search(prompt)
                
                # Generate comprehensive analysis
                yield "ğŸ“Š **í†µí•© ë¶„ì„ ê²°ê³¼**\n\n"
                yield research_results
                
                # Generate final recommendation
                yield "\nğŸ¯ **ì—°êµ¬ ê¶Œì¥ì‚¬í•­**\n\n"
                recommendations = self._generate_recommendations(prompt, research_results)
                yield recommendations
            else:
                yield from self._generate_mock_deep_research(prompt)
                
        except Exception as e:
            yield f"âš ï¸ Deep Research ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\n"
            yield from self._generate_mock_deep_research(prompt)

    def _call_gaia_bt_sync(self, prompt: str) -> str:
        """Call GAIA-BT chatbot synchronously"""
        try:
            if self.chatbot and hasattr(self.chatbot, 'ollama_client'):
                # Get system prompt
                system_prompt = get_system_prompt(self.valves.SELECTED_PROMPT_MODE)
                
                # Create full prompt with system context
                full_prompt = f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:"
                
                # Call Ollama client directly for synchronous response
                import requests
                import json
                
                # Use local Ollama API
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": self.valves.DEFAULT_MODEL,
                        "prompt": full_prompt,
                        "stream": False
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get('response', 'No response received')
                else:
                    return f"API ì˜¤ë¥˜: {response.status_code}"
                    
            else:
                return "GAIA-BT chatbot not available"
                
        except Exception as e:
            return f"ë™ê¸° í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}"

    def _generate_mock_response(self, prompt: str) -> Iterator[str]:
        """Generate mock response for testing"""
        yield f"ğŸ§¬ **GAIA-BT ì‹ ì•½ê°œë°œ ì‘ë‹µ** (Mock Mode)\n\n"
        yield f"ì§ˆë¬¸: {prompt}\n\n"
        
        if "cancer" in prompt.lower() or "ì•”" in prompt:
            yield "ğŸ¯ **ì•” ì¹˜ë£Œì œ ê°œë°œ ê´€ì ì—ì„œì˜ ë¶„ì„**\n\n"
            yield "â€¢ íƒ€ê²Ÿ ê²€ì¦: ì¢…ì–‘ íŠ¹ì´ì  ë°”ì´ì˜¤ë§ˆì»¤ í™•ì¸ í•„ìš”\n"
            yield "â€¢ í™”í•©ë¬¼ ìµœì í™”: ADMET í”„ë¡œíŒŒì¼ ê°œì„  ë°©í–¥\n"
            yield "â€¢ ì„ìƒì‹œí—˜ ì „ëµ: Phase I/II ë””ìì¸ ê³ ë ¤ì‚¬í•­\n\n"
        else:
            yield "ì´ ì§ˆë¬¸ì€ ì‹ ì•½ê°œë°œ ê´€ì ì—ì„œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
            yield "ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
        
        yield f"---\n*GAIA-BT v2.0 Alpha - {self.valves.SELECTED_PROMPT_MODE} mode*"

    def _generate_specialized_mock_response(self, prompt: str) -> Iterator[str]:
        """Generate specialized mock response based on prompt mode"""
        mode = self.valves.SELECTED_PROMPT_MODE
        
        yield f"ğŸ§¬ **GAIA-BT {mode.title()} Mode** ì „ë¬¸ ë¶„ì„\n\n"
        
        if mode == "clinical":
            yield "ğŸ¥ **ì„ìƒì‹œí—˜ ê´€ì  ë¶„ì„**\n"
            yield "â€¢ í™˜ì ì•ˆì „ì„± ìµœìš°ì„  ê³ ë ¤\n"
            yield "â€¢ ê·œì œ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜ í•„ìš”\n"
            yield "â€¢ ë°”ì´ì˜¤ë§ˆì»¤ ê°œë°œ ê²€í† \n\n"
        elif mode == "research":
            yield "ğŸ”¬ **ì—°êµ¬ ë¶„ì„ ê´€ì **\n"
            yield "â€¢ ìµœì‹  ë¬¸í—Œ ê²€í†  í•„ìš”\n"
            yield "â€¢ ì‹¤í—˜ ì„¤ê³„ ìµœì í™”\n"
            yield "â€¢ í†µê³„ì  ìœ ì˜ì„± í™•ë³´\n\n"
        elif mode == "chemistry":
            yield "âš—ï¸ **ì˜ì•½í™”í•™ ê´€ì  ë¶„ì„**\n"
            yield "â€¢ ë¶„ì êµ¬ì¡°-í™œì„± ê´€ê³„(SAR) ë¶„ì„\n"
            yield "â€¢ ADMET íŠ¹ì„± ìµœì í™”\n"
            yield "â€¢ í•©ì„± ê²½ë¡œ ê²€í† \n\n"
        elif mode == "regulatory":
            yield "ğŸ“‹ **ê·œì œ ê´€ì  ë¶„ì„**\n"
            yield "â€¢ FDA/EMA ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜\n"
            yield "â€¢ í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„ êµ¬ì¶•\n"
            yield "â€¢ ì œì¶œ ìë£Œ ì¤€ë¹„ ê³„íš\n\n"
        else:
            yield "ğŸ’Š **ì‹ ì•½ê°œë°œ ì¢…í•© ë¶„ì„**\n"
            yield "â€¢ íƒ€ê²Ÿ ê²€ì¦ë¶€í„° ì‹œì¥ ì¶œì‹œê¹Œì§€\n"
            yield "â€¢ ê³¼í•™ì  ê·¼ê±° ê¸°ë°˜ ì ‘ê·¼\n"
            yield "â€¢ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì „ëµ\n\n"
        
        yield f"---\n*GAIA-BT v2.0 Alpha - {mode} ì „ë¬¸ ëª¨ë“œë¡œ ë¶„ì„*"

    def _generate_mock_deep_research(self, prompt: str) -> Iterator[str]:
        """Generate mock deep research response"""
        yield "ğŸ“š **ë¬¸í—Œ ê²€ìƒ‰ ê²°ê³¼** (Mock)\n"
        yield "â€¢ PubMed: ê´€ë ¨ ë…¼ë¬¸ 15í¸ ë°œê²¬\n"
        yield "â€¢ ClinicalTrials.gov: ì§„í–‰ ì¤‘ì¸ ì„ìƒì‹œí—˜ 3ê±´\n"
        yield "â€¢ ChEMBL: ìœ ì‚¬ í™”í•©ë¬¼ 127ê°œ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ì¹­\n\n"
        
        yield "ğŸ”¬ **ì‹¤í—˜ ì„¤ê³„ ì œì•ˆ** (Mock)\n"
        yield "â€¢ In vitro ì„¸í¬ ë…ì„± ì‹¤í—˜\n"
        yield "â€¢ ë™ë¬¼ ëª¨ë¸ì„ ì´ìš©í•œ efficacy í‰ê°€\n"
        yield "â€¢ ADMET í”„ë¡œíŒŒì¼ ë¶„ì„\n\n"
        
        yield "ğŸ“ˆ **ì‹œì¥ ë¶„ì„** (Mock)\n"
        yield "â€¢ ê¸€ë¡œë²Œ ì‹œì¥ ê·œëª¨: $50B (ì˜ˆìƒ)\n"
        yield "â€¢ ì£¼ìš” ê²½ìŸì‚¬: 3ê°œ íšŒì‚¬ ê°œë°œ ì¤‘\n"
        yield "â€¢ ê·œì œ í˜„í™©: FDA ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ í•„ìš”\n\n"

    def _perform_integrated_search(self, query: str) -> str:
        """Perform integrated MCP search"""
        try:
            # Use GAIA-BT MCP integration
            if self.mcp_commands:
                search_results = self.mcp_commands.handle_deep_search(query)
                return search_results
            else:
                return self._mock_integrated_search(query)
        except Exception as e:
            if self.valves.DEBUG_MODE:
                return f"Mock search results for: {query}\n\nError: {str(e)}"
            return self._mock_integrated_search(query)

    def _mock_integrated_search(self, query: str) -> str:
        """Mock integrated search results"""
        return f"""
**í†µí•© ê²€ìƒ‰ ê²°ê³¼ (Mock)**

ğŸ“– **ë¬¸í—Œ ë¶„ì„**
- ê´€ë ¨ ë…¼ë¬¸: 25í¸ (ìµœê·¼ 5ë…„)
- í•µì‹¬ ì—°êµ¬ì: Dr. Smith (í•˜ë²„ë“œ), Dr. Kim (ì„œìš¸ëŒ€)
- ì£¼ìš” ë°œê²¬: ìƒˆë¡œìš´ íƒ€ê²Ÿ ë©”ì»¤ë‹ˆì¦˜ í™•ì¸

ğŸ§ª **í™”í•™ ë¶„ì„**
- ìœ ì‚¬ í™”í•©ë¬¼: 45ê°œ (ChEMBL ë°ì´í„°ë² ì´ìŠ¤)
- ì•½ë¬¼ ìƒí˜¸ì‘ìš©: 3ê°œ ì£¼ìš” ê²½ë¡œ í™•ì¸
- ADMET ì˜ˆì¸¡: ì¤‘ë“±ë„ íˆ¬ê³¼ì„±, ë‚®ì€ ë…ì„±

ğŸ¥ **ì„ìƒ í˜„í™©**
- ì§„í–‰ ì¤‘ì¸ ì‹œí—˜: 7ê±´ (Phase I-III)
- ìŠ¹ì¸ëœ ì¹˜ë£Œì œ: 2ê°œ (2019, 2021)
- ê·œì œ ê°€ì´ë“œë¼ì¸: ìµœì‹  FDA/EMA ìš”êµ¬ì‚¬í•­

Query processed: {query}
"""

    def _generate_recommendations(self, query: str, results: str) -> str:
        """Generate research recommendations based on results"""
        try:
            if self.research_manager:
                recommendations = self.research_manager.generate_recommendations(query, results)
                return recommendations
            else:
                return self._mock_recommendations(query)
        except Exception as e:
            return self._mock_recommendations(query)

    def _mock_recommendations(self, query: str) -> str:
        """Mock research recommendations"""
        return f"""
**ì—°êµ¬ ê¶Œì¥ì‚¬í•­ (Mock)**

ğŸ¯ **ë‹¨ê¸° ëª©í‘œ (6ê°œì›”)**
1. íƒ€ê²Ÿ ê²€ì¦ ì‹¤í—˜ ì™„ë£Œ
2. ë¦¬ë“œ í™”í•©ë¬¼ 3-5ê°œ ì„ ì •
3. ì´ˆê¸° ADMET ìŠ¤í¬ë¦¬ë‹

ğŸš€ **ì¤‘ê¸° ëª©í‘œ (1-2ë…„)**
1. í™”í•©ë¬¼ ìµœì í™” ì™„ë£Œ
2. ì „ì„ìƒ ì•ˆì „ì„± í‰ê°€
3. IND ì‹ ì²­ ì¤€ë¹„

ğŸ“Š **ì¥ê¸° ëª©í‘œ (3-5ë…„)**
1. Phase I ì„ìƒì‹œí—˜ ì‹œì‘
2. ë°”ì´ì˜¤ë§ˆì»¤ ê°œë°œ
3. íŒŒíŠ¸ë„ˆë§ ê¸°íšŒ íƒìƒ‰

ğŸ’¡ **í•µì‹¬ ê³ ë ¤ì‚¬í•­**
- ì§€ì ì¬ì‚°ê¶Œ í™•ë³´ ì „ëµ
- ê·œì œ ì „ëµ ìˆ˜ë¦½
- ìê¸ˆ ì¡°ë‹¬ ê³„íš

Query: {query}
"""

    async def _test_gaia_bt_connection(self) -> bool:
        """Test GAIA-BT system connectivity"""
        try:
            if self.chatbot:
                # Simple connectivity test
                test_response = self.chatbot.generate_specialized_response("test", mode="default")
                return len(test_response) > 0
            return False
        except Exception:
            return False

    def get_provider_models(self) -> List[Dict[str, Any]]:
        """Provide GAIA-BT specific models"""
        return [
            {
                "id": "gaia-bt-normal",
                "name": "GAIA-BT Normal Mode",
                "model": {
                    "id": "gaia-bt-normal",
                    "name": "GAIA-BT Normal Mode",
                    "meta": {
                        "description": "ì‹ ì•½ê°œë°œ ì¼ë°˜ ëª¨ë“œ - ë¹ ë¥¸ ì‘ë‹µ",
                        "capabilities": ["text"],
                        "vision": False
                    },
                    "ollama": {
                        "name": self.valves.DEFAULT_MODEL,
                        "model": self.valves.DEFAULT_MODEL
                    }
                }
            },
            {
                "id": "gaia-bt-deep-research",
                "name": "GAIA-BT Deep Research Mode",
                "model": {
                    "id": "gaia-bt-deep-research", 
                    "name": "GAIA-BT Deep Research Mode",
                    "meta": {
                        "description": "ì‹ ì•½ê°œë°œ Deep Research ëª¨ë“œ - MCP í†µí•© ê²€ìƒ‰",
                        "capabilities": ["text", "research", "mcp"],
                        "vision": False
                    },
                    "ollama": {
                        "name": self.valves.DEFAULT_MODEL,
                        "model": self.valves.DEFAULT_MODEL
                    }
                }
            }
        ]