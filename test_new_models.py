#!/usr/bin/env python3
"""
ìƒˆë¡œ ì¶”ê°€ëœ ëª¨ë¸ë“¤ì„ í¬í•¨í•œ ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
"""

import asyncio
import httpx
import time

# ìƒ‰ìƒ ì½”ë“œ
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
CYAN = "\033[96m"
RESET = "\033[0m"

class ModelTester:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.models = [
            "gemma3-12b:latest",           # ìƒˆë¡œ ì¶”ê°€ëœ ëª¨ë¸
            "txgemma-chat:latest",         # ê¸°ì¡´ ëª¨ë¸
            "txgemma-predict:latest",      # ê¸°ì¡´ ëª¨ë¸
            "Gemma3:27b-it-q4_K_M"        # ê¸°ì¡´ ëŒ€ìš©ëŸ‰ ëª¨ë¸
        ]
    
    async def test_model_direct(self, model_name):
        """Ollama API ì§ì ‘ í…ŒìŠ¤íŠ¸"""
        print(f"\n{CYAN}=== {model_name} ì§ì ‘ API í…ŒìŠ¤íŠ¸ ==={RESET}")
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                start_time = time.time()
                
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": "Hello! Please introduce yourself as GAIA-BT, a drug development AI assistant, in 2-3 sentences.",
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 200
                        }
                    }
                )
                
                end_time = time.time()
                elapsed = end_time - start_time
                
                if response.status_code == 200:
                    result = response.json()
                    response_text = result.get('response', '')
                    
                    print(f"{GREEN}âœ“ ì„±ê³µ{RESET}")
                    print(f"â±ï¸  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
                    print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")
                    print(f"ğŸ’¬ ì‘ë‹µ: {response_text[:150]}...")
                    
                    return {
                        'success': True,
                        'time': elapsed,
                        'response_length': len(response_text),
                        'response': response_text
                    }
                else:
                    print(f"{RED}âœ— HTTP ì˜¤ë¥˜: {response.status_code}{RESET}")
                    return {'success': False, 'error': f"HTTP {response.status_code}"}
                    
        except Exception as e:
            print(f"{RED}âœ— ì˜¤ë¥˜: {e}{RESET}")
            return {'success': False, 'error': str(e)}
    
    async def test_model_gaia_api(self, model_name):
        """GAIA-BT API ì„œë²„ë¥¼ í†µí•œ í…ŒìŠ¤íŠ¸"""
        print(f"\n{CYAN}=== {model_name} GAIA-BT API í…ŒìŠ¤íŠ¸ ==={RESET}")
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                # ì„¸ì…˜ ìƒì„±
                session_response = await client.post(
                    "http://localhost:8000/api/session/create",
                    json={"session_id": f"test_{model_name.replace(':', '_')}"}
                )
                
                if session_response.status_code != 200:
                    print(f"{RED}âœ— ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨{RESET}")
                    return {'success': False, 'error': 'Session creation failed'}
                
                session_data = session_response.json()
                session_id = session_data['session_id']
                
                # ëª¨ë¸ ë³€ê²½
                model_response = await client.post(
                    "http://localhost:8000/api/system/model",
                    json={"model": model_name, "session_id": session_id}
                )
                
                if model_response.status_code != 200:
                    print(f"{RED}âœ— ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨{RESET}")
                    return {'success': False, 'error': 'Model change failed'}
                
                # ë©”ì‹œì§€ ì „ì†¡
                start_time = time.time()
                
                message_response = await client.post(
                    "http://localhost:8000/api/chat/message",
                    json={
                        "message": "ì‹ ì•½ê°œë°œ ê³¼ì •ì˜ ì£¼ìš” ë‹¨ê³„ 3ê°€ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                        "session_id": session_id
                    }
                )
                
                end_time = time.time()
                elapsed = end_time - start_time
                
                if message_response.status_code == 200:
                    result = message_response.json()
                    response_text = result.get('response', '')
                    
                    print(f"{GREEN}âœ“ ì„±ê³µ{RESET}")
                    print(f"â±ï¸  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
                    print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")
                    print(f"ğŸ’¬ ì‘ë‹µ: {response_text[:150]}...")
                    
                    return {
                        'success': True,
                        'time': elapsed,
                        'response_length': len(response_text),
                        'response': response_text
                    }
                else:
                    print(f"{RED}âœ— HTTP ì˜¤ë¥˜: {message_response.status_code}{RESET}")
                    return {'success': False, 'error': f"HTTP {message_response.status_code}"}
                    
        except Exception as e:
            print(f"{RED}âœ— ì˜¤ë¥˜: {e}{RESET}")
            return {'success': False, 'error': str(e)}
    
    async def test_model_streaming(self, model_name):
        """ìŠ¤íŠ¸ë¦¬ë° API í…ŒìŠ¤íŠ¸"""
        print(f"\n{CYAN}=== {model_name} ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ==={RESET}")
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                start_time = time.time()
                
                async with client.stream(
                    "POST",
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": "Explain drug discovery briefly.",
                        "stream": True,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 100
                        }
                    }
                ) as response:
                    if response.status_code != 200:
                        print(f"{RED}âœ— HTTP ì˜¤ë¥˜: {response.status_code}{RESET}")
                        return {'success': False, 'error': f"HTTP {response.status_code}"}
                    
                    chunks = []
                    async for line in response.aiter_lines():
                        if line.strip():
                            try:
                                import json
                                chunk = json.loads(line)
                                if chunk.get("response"):
                                    chunks.append(chunk["response"])
                                if chunk.get("done", False):
                                    break
                            except json.JSONDecodeError:
                                continue
                
                end_time = time.time()
                elapsed = end_time - start_time
                
                full_response = ''.join(chunks)
                
                print(f"{GREEN}âœ“ ìŠ¤íŠ¸ë¦¬ë° ì„±ê³µ{RESET}")
                print(f"â±ï¸  ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
                print(f"ğŸ“¦ ì²­í¬ ìˆ˜: {len(chunks)}")
                print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(full_response)} ë¬¸ì")
                print(f"ğŸ’¬ ì‘ë‹µ: {full_response[:150]}...")
                
                return {
                    'success': True,
                    'time': elapsed,
                    'chunks': len(chunks),
                    'response_length': len(full_response),
                    'response': full_response
                }
                
        except Exception as e:
            print(f"{RED}âœ— ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}{RESET}")
            return {'success': False, 'error': str(e)}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"{BLUE}{'='*70}{RESET}")
    print(f"{BLUE}ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ìƒˆ ëª¨ë¸ í¬í•¨){RESET}")
    print(f"{BLUE}{'='*70}{RESET}")
    
    tester = ModelTester()
    all_results = {}
    
    # ê° ëª¨ë¸ì— ëŒ€í•´ 3ê°€ì§€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for model_name in tester.models:
        print(f"\n{YELLOW}ğŸ§ª {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘{RESET}")
        
        model_results = {}
        
        # 1. ì§ì ‘ API í…ŒìŠ¤íŠ¸
        direct_result = await tester.test_model_direct(model_name)
        model_results['direct'] = direct_result
        
        # 2. GAIA-BT API í…ŒìŠ¤íŠ¸
        gaia_result = await tester.test_model_gaia_api(model_name)
        model_results['gaia_api'] = gaia_result
        
        # 3. ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
        streaming_result = await tester.test_model_streaming(model_name)
        model_results['streaming'] = streaming_result
        
        all_results[model_name] = model_results
        
        # ëª¨ë¸ë³„ ìš”ì•½
        success_count = sum(1 for r in model_results.values() if r.get('success', False))
        print(f"\n{CYAN}ğŸ“Š {model_name} ìš”ì•½: {success_count}/3 í…ŒìŠ¤íŠ¸ ì„±ê³µ{RESET}")
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\n{BLUE}{'='*70}{RESET}")
    print(f"{BLUE}ğŸ“ˆ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½{RESET}")
    print(f"{BLUE}{'='*70}{RESET}")
    
    successful_models = []
    
    for model_name, results in all_results.items():
        direct_success = results['direct'].get('success', False)
        gaia_success = results['gaia_api'].get('success', False)
        streaming_success = results['streaming'].get('success', False)
        
        success_rate = sum([direct_success, gaia_success, streaming_success]) / 3
        
        if direct_success:
            avg_time = (
                results['direct'].get('time', 0) + 
                results['gaia_api'].get('time', 0) + 
                results['streaming'].get('time', 0)
            ) / 3
            successful_models.append((model_name, avg_time, success_rate))
        
        status = f"{GREEN}âœ“{RESET}" if success_rate >= 0.67 else f"{RED}âœ—{RESET}"
        print(f"{status} {model_name}: {success_rate*100:.0f}% ì„±ê³µë¥ ")
        
        if direct_success:
            print(f"    â±ï¸  í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
        
        if not direct_success:
            print(f"    âŒ ì˜¤ë¥˜: {results['direct'].get('error', 'Unknown error')}")
    
    # ì„±ëŠ¥ ìˆœìœ„
    if successful_models:
        successful_models.sort(key=lambda x: x[1])  # í‰ê·  ì‹œê°„ìˆœ ì •ë ¬
        print(f"\n{CYAN}ğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (í‰ê·  ì‘ë‹µ ì‹œê°„ ê¸°ì¤€):{RESET}")
        for i, (model_name, avg_time, success_rate) in enumerate(successful_models, 1):
            print(f"  {i}. {model_name} ({avg_time:.2f}ì´ˆ, {success_rate*100:.0f}% ì„±ê³µë¥ )")
    
    # ê¶Œì¥ì‚¬í•­
    print(f"\n{BLUE}ğŸ¯ ê¶Œì¥ì‚¬í•­{RESET}")
    if successful_models:
        fastest_model = successful_models[0][0]
        print(f"ğŸ¥‡ ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {GREEN}{fastest_model}{RESET}")
        print(f"ğŸ“‹ ì„¤ì • ë³€ê²½ ì œì•ˆ:")
        print(f"   export OLLAMA_MODEL={fastest_model}")
        print(f"   OLLAMA_MODEL = '{fastest_model}' # config.pyì—ì„œ")
        
        # ìƒˆ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
        gemma3_12b_result = all_results.get('gemma3-12b:latest')
        if gemma3_12b_result and gemma3_12b_result['direct'].get('success'):
            gemma3_time = gemma3_12b_result['direct']['time']
            print(f"\nğŸ†• ìƒˆ ëª¨ë¸ ë¶„ì„:")
            print(f"   gemma3-12b:latest: {gemma3_time:.2f}ì´ˆ")
            
            # ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ
            txgemma_chat_result = all_results.get('txgemma-chat:latest')
            if txgemma_chat_result and txgemma_chat_result['direct'].get('success'):
                txgemma_time = txgemma_chat_result['direct']['time']
                if gemma3_time < txgemma_time:
                    print(f"   âœ… gemma3-12bê°€ txgemma-chatë³´ë‹¤ {txgemma_time - gemma3_time:.2f}ì´ˆ ë¹ ë¦„")
                else:
                    print(f"   âš ï¸  txgemma-chatì´ gemma3-12bë³´ë‹¤ {gemma3_time - txgemma_time:.2f}ì´ˆ ë¹ ë¦„")
    
    print(f"\n{GREEN}ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!{RESET}")

if __name__ == "__main__":
    asyncio.run(main())