import streamlit as st
import time
import os
import requests
from datetime import datetime
from typing import List, Dict, Any
import json

# Ollama API ì„¤ì •
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

def get_available_models():
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model["name"] for model in data.get("models", [])]
        else:
            return ["llama3.2:latest", "gemma2:latest", "qwen2.5:latest"]  # ê¸°ë³¸ê°’
    except Exception as e:
        st.error(f"GAIA GPT ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return ["llama3.2:latest", "gemma2:latest", "qwen2.5:latest"]  # ê¸°ë³¸ê°’

def generate_ollama_response(prompt: str, model: str, mode: str, expert_mode: str = "default"):
    """Ollama APIë¥¼ í†µí•´ ì‘ë‹µ ìƒì„± (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° íƒ€ì„ì•„ì›ƒ ê´€ë¦¬)"""
    try:
        # ì „ë¬¸ê°€ ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        expert_prompts = {
            "default": "ë‹¹ì‹ ì€ ì‹ ì•½ê°œë°œ ì „ë°˜ì— ëŒ€í•œ ê· í˜•ì¡íŒ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
            "clinical": "ë‹¹ì‹ ì€ ì„ìƒì‹œí—˜ ë° í™˜ì ì¤‘ì‹¬ ì•½ë¬¼ ê°œë°œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì„ìƒì‹œí—˜ ì„¤ê³„, í™˜ì ì•ˆì „ì„±, íš¨ëŠ¥ í‰ê°€, ê·œì œ ìš”êµ¬ì‚¬í•­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            "research": "ë‹¹ì‹ ì€ ë¬¸í—Œ ë¶„ì„ ë° ê³¼í•™ì  ì¦ê±° ì¢…í•© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ì—°êµ¬ ë™í–¥, ë…¼ë¬¸ ë¶„ì„, ê³¼í•™ì  ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
            "chemistry": "ë‹¹ì‹ ì€ ì˜ì•½í™”í•™ ë° ë¶„ì ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í™”í•™ êµ¬ì¡°, ë¶„ì ë©”ì»¤ë‹ˆì¦˜, SAR(êµ¬ì¡°-í™œì„± ê´€ê³„), ì•½ë¬¼ ìµœì í™”ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            "regulatory": "ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ì˜ì•½í’ˆ ê·œì œ ë° ìŠ¹ì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. FDA, EMA, MFDS ë“± ê·œì œ ê¸°ê´€ì˜ ìš”êµ¬ì‚¬í•­, ìŠ¹ì¸ ê³¼ì •, ê·œì œ ì „ëµì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }
        
        # ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (2ê°œ ëª¨ë“œ)
        mode_prompts = {
            "Normal": expert_prompts.get(expert_mode, expert_prompts["default"]),
            "Deep Research": f"{expert_prompts.get(expert_mode, expert_prompts['default'])} ê³¼í•™ì  ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ì œê³µí•˜ê³ , ì—°êµ¬ ë°°ê²½, í•µì‹¬ ë°œê²¬ì‚¬í•­, ì‹¤ìš©ì  ì œì•ˆì„ í¬í•¨í•˜ì—¬ í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }
        
        system_prompt = mode_prompts.get(mode, mode_prompts["Normal"])
        
        # í† í° ìˆ˜ ì œí•œ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        max_tokens = min(st.session_state.get("max_tokens", 1000), 1500)
        
        payload = {
            "model": model,
            "prompt": f"{system_prompt}\n\nì§ˆë¬¸: {prompt}\n\në‹µë³€:",
            "stream": False,
            "options": {
                "temperature": st.session_state.get("temperature", 0.7),
                "num_predict": max_tokens,
                "top_k": 40,
                "top_p": 0.9,
                "repeat_penalty": 1.1
            }
        }
        
        # ë‹¨ê³„ì  íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate", 
            json=payload, 
            timeout=300  # 5ë¶„ìœ¼ë¡œ ì¦ê°€
        )
        
        if response.status_code == 200:
            result = response.json()
            answer = result.get("response", "").strip()
            
            if not answer:
                return "ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            return answer
        else:
            return f"ì„œë²„ ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code}). ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
    except requests.exceptions.Timeout:
        return """â° **ì‘ë‹µ ì‹œê°„ ì´ˆê³¼**
        
í˜„ì¬ ì§ˆë¬¸ì´ ì²˜ë¦¬í•˜ê¸°ì— ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ì„œë²„ê°€ ë°”ìœ ìƒíƒœì…ë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. ë” ê°„ë‹¨í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
2. ì‚¬ì´ë“œë°”ì—ì„œ ìµœëŒ€ í† í° ìˆ˜ë¥¼ 1000 ì´í•˜ë¡œ ì¤„ì´ê¸°
3. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ê¸°

**ì¶”ì²œ ì§ˆë¬¸ í˜•íƒœ**:
- "ì‹ ì•½ê°œë°œ ê³¼ì •ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"
- "ì„ìƒì‹œí—˜ 1ìƒì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        """
    except requests.exceptions.ConnectionError:
        return """ğŸ”Œ **ì„œë²„ ì—°ê²° ì‹¤íŒ¨**
        
GAIA GPT ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. ì‚¬ì´ë“œë°”ì˜ "ì„œë²„ ìƒíƒœ í™•ì¸" ë²„íŠ¼ í´ë¦­
2. GAIA GPT ì„œë²„ ì¬ì‹œì‘: `ollama serve`
3. í„°ë¯¸ë„ì—ì„œ ì„œë²„ ìƒíƒœ í™•ì¸: `curl http://localhost:11434/api/tags`
        """
    except Exception as e:
        error_msg = str(e)
        if "timeout" in error_msg.lower():
            return """â° **ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼**
            
ì‘ë‹µ ìƒì„±ì— ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.

**ì¦‰ì‹œ í•´ê²° ë°©ë²•**:
1. ì‚¬ì´ë“œë°”ì—ì„œ ìµœëŒ€ í† í° ìˆ˜ë¥¼ 500-1000ìœ¼ë¡œ ì¤„ì´ê¸°
2. ë” ì§§ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€ê²½
3. ë‹¤ë¥¸ ëª¨ë¸ ì„ íƒí•´ë³´ê¸° (txgemma-chat ì¶”ì²œ)
            """
        else:
            return f"""âŒ **ì˜¤ë¥˜ ë°œìƒ**
            
ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}

**í•´ê²° ë°©ë²•**:
1. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ë‹¤ì‹œ ì‹œë„
2. ì‚¬ì´ë“œë°”ì—ì„œ "ì„œë²„ ìƒíƒœ í™•ì¸" ë²„íŠ¼ í´ë¦­
3. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ ì„ íƒ
            """

def check_ollama_status():
    """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=3)
        return response.status_code == 200
    except:
        return False

def check_mcp_status():
    """MCP ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        # GAIA-BT ì‹œìŠ¤í…œì˜ MCP ì„œë²„ í™•ì¸ (í¬íŠ¸ 8080)
        response = requests.get("http://localhost:8080/health", timeout=3)
        return response.status_code == 200
    except:
        try:
            # ëŒ€ì•ˆ: MCP í”„ë¡œì„¸ìŠ¤ í™•ì¸
            import subprocess
            result = subprocess.run(['pgrep', '-f', 'mcp'], capture_output=True, text=True)
            return len(result.stdout.strip()) > 0
        except:
            return False

def start_mcp_servers():
    """MCP ì„œë²„ ì‹œì‘"""
    try:
        import subprocess
        # GAIA-BT MCP ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        script_path = "/home/gaia-bt/workspace/GAIA_LLMs/scripts/run_mcp_servers.sh"
        result = subprocess.run([script_path], capture_output=True, text=True, timeout=30)
        return result.returncode == 0
    except Exception as e:
        st.error(f"MCP ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        return False

def stop_mcp_servers():
    """MCP ì„œë²„ ì¤‘ì§€"""
    try:
        import subprocess
        # GAIA-BT MCP ì„œë²„ ì¤‘ì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        script_path = "/home/gaia-bt/workspace/GAIA_LLMs/scripts/stop_mcp_servers.sh"
        result = subprocess.run([script_path], capture_output=True, text=True, timeout=30)
        return result.returncode == 0
    except Exception as e:
        st.error(f"MCP ì„œë²„ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
        return False

# GAIA-BT GPT ì±—ë´‡ ì„¤ì •
st.set_page_config(
    page_title="GAIA-BT GPT",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for GAIA-BT theme
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 50%, #06b6d4 100%);
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
        color: white;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .chat-container {
        background: #f8fafc;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        border-left: 4px solid #3b82f6;
    }
    
    .user-message {
        background: linear-gradient(135deg, #3b82f6, #1e40af);
        color: white;
        padding: 1rem;
        border-radius: 15px 15px 5px 15px;
        margin: 0.5rem 0;
        max-width: 80%;
        margin-left: auto;
    }
    
    .bot-message {
        background: linear-gradient(135deg, #f1f5f9, #e2e8f0);
        color: #1e293b;
        padding: 1rem;
        border-radius: 15px 15px 15px 5px;
        margin: 0.5rem 0;
        max-width: 80%;
        border-left: 4px solid #06b6d4;
    }
    
    .status-indicator {
        display: inline-block;
        width: 10px;
        height: 10px;
        border-radius: 50%;
        margin-right: 8px;
        animation: pulse 2s infinite;
    }
    
    .status-online { background-color: #10b981; }
    .status-processing { background-color: #f59e0b; }
    .status-offline { background-color: #ef4444; }
    
    @keyframes pulse {
        0% { opacity: 1; }
        50% { opacity: 0.5; }
        100% { opacity: 1; }
    }
    
    .mode-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 9999px;
        font-size: 0.75rem;
        font-weight: 600;
        margin: 0.25rem;
    }
    
    .mode-normal {
        background-color: #e5e7eb;
        color: #374151;
    }
    
    .mode-research {
        background-color: #dcfce7;
        color: #166534;
    }
    
    .expert-default {
        background-color: #e5e7eb;
        color: #374151;
    }
    
    .expert-clinical {
        background-color: #fef3c7;
        color: #92400e;
    }
    
    .expert-research {
        background-color: #dbeafe;
        color: #1e40af;
    }
    
    .expert-chemistry {
        background-color: #fecaca;
        color: #dc2626;
    }
    
    .expert-regulatory {
        background-color: #e9d5ff;
        color: #7c3aed;
    }
    
    .sidebar-section {
        background: #f8fafc;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
        border: 1px solid #e2e8f0;
    }
    
    .stButton > button[disabled] {
        background-color: #f1f5f9 !important;
        color: #94a3b8 !important;
        border-color: #e2e8f0 !important;
        cursor: not-allowed !important;
    }
    
    .stButton > button[disabled]:hover {
        background-color: #f1f5f9 !important;
        color: #94a3b8 !important;
        border-color: #e2e8f0 !important;
    }
</style>
""", unsafe_allow_html=True)

# ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "mode" not in st.session_state:
    st.session_state.mode = "Normal"

if "available_models" not in st.session_state:
    st.session_state.available_models = get_available_models()

if "model" not in st.session_state:
    # ê¸°ë³¸ ëª¨ë¸ì„ Gemma3:27b-it-q4_K_Më¡œ ì„¤ì •
    default_model = "Gemma3:27b-it-q4_K_M"
    if st.session_state.available_models and default_model in st.session_state.available_models:
        st.session_state.model = default_model
    elif st.session_state.available_models:
        st.session_state.model = st.session_state.available_models[0]
    else:
        st.session_state.model = "Gemma3:27b-it-q4_K_M"

if "session_id" not in st.session_state:
    st.session_state.session_id = f"session_{int(time.time())}"

if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7

if "max_tokens" not in st.session_state:
    st.session_state.max_tokens = 800  # ê¸°ë³¸ê°’ì„ ë‚®ì¶¤

if "ollama_status" not in st.session_state:
    st.session_state.ollama_status = check_ollama_status()

if "mcp_status" not in st.session_state:
    # ì²˜ìŒ ì‹œì‘í•  ë•ŒëŠ” MCP ì„œë²„ì— ì—°ê²°í•˜ì§€ ì•ŠìŒ
    st.session_state.mcp_status = False

# ë©”ì¸ í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ§¬ GAIA-BT GPT</h1>
    <p>ì‹ ì•½ê°œë°œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ | ì°¨ì„¸ëŒ€ ì˜ì•½í’ˆ ì—°êµ¬ ì§€ì›</p>
</div>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.markdown("### ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
    
    # ì‹œìŠ¤í…œ ìƒíƒœ
    ollama_status = "ì˜¨ë¼ì¸" if st.session_state.ollama_status else "ì˜¤í”„ë¼ì¸"
    ollama_class = "status-online" if st.session_state.ollama_status else "status-offline"
    
    mcp_status = "ì—°ê²°ë¨" if st.session_state.mcp_status else "ì—°ê²° ì•ˆë¨"
    mcp_class = "status-online" if st.session_state.mcp_status else "status-offline"
    
    st.markdown(f"""
    <div class="sidebar-section">
        <h4>ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ</h4>
        <p><span class="status-indicator {ollama_class}"></span>GAIA GPT ì„œë²„: {ollama_status}</p>
        <p><span class="status-indicator status-online"></span>ëª¨ë¸: í™œì„±í™”</p>
        <p><span class="status-indicator {mcp_class}"></span>MCP ì„œë²„: {mcp_status}</p>
        <p><span class="status-indicator status-processing"></span>ì„¸ì…˜: {st.session_state.session_id}</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì„œë²„ ìƒíƒœ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
    if st.button("ğŸ”„ ì„œë²„ ìƒíƒœ í™•ì¸"):
        st.session_state.ollama_status = check_ollama_status()
        st.session_state.mcp_status = check_mcp_status()
        st.session_state.available_models = get_available_models()
        st.rerun()
    
    # MCP ìˆ˜ë™ ì œì–´ ë²„íŠ¼
    st.markdown("#### ğŸ”¬ MCP ì„œë²„ ì œì–´")
    col1, col2 = st.columns(2)
    
    with col1:
        # MCP ì„œë²„ê°€ ì¤‘ì§€ëœ ìƒíƒœì¼ ë•Œë§Œ ì‹œì‘ ë²„íŠ¼ í™œì„±í™”
        start_disabled = st.session_state.mcp_status
        start_help = "MCP ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤" if start_disabled else "MCP ì„œë²„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘"
        
        if st.button("ğŸ”Œ MCP ì‹œì‘", 
                    disabled=start_disabled, 
                    help=start_help):
            with st.spinner("MCP ì„œë²„ ì‹œì‘ ì¤‘..."):
                if start_mcp_servers():
                    st.session_state.mcp_status = True
                    st.success("âœ… MCP ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                else:
                    st.error("âŒ MCP ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
                    st.session_state.mcp_status = check_mcp_status()
            st.rerun()
    
    with col2:
        # MCP ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¼ ë•Œë§Œ ì¤‘ì§€ ë²„íŠ¼ í™œì„±í™”
        stop_disabled = not st.session_state.mcp_status
        stop_help = "MCP ì„œë²„ê°€ ì´ë¯¸ ì¤‘ì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤" if stop_disabled else "MCP ì„œë²„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¤‘ì§€"
        
        if st.button("ğŸ”Œ MCP ì¤‘ì§€", 
                    disabled=stop_disabled, 
                    help=stop_help):
            with st.spinner("MCP ì„œë²„ ì¤‘ì§€ ì¤‘..."):
                if stop_mcp_servers():
                    st.session_state.mcp_status = False
                    st.success("âœ… MCP ì„œë²„ ì¤‘ì§€ ì™„ë£Œ")
                else:
                    st.error("âŒ MCP ì„œë²„ ì¤‘ì§€ ì‹¤íŒ¨")
                    st.session_state.mcp_status = check_mcp_status()
            st.rerun()
    
    # ëª¨ë“œ ì„ íƒ
    st.markdown("#### ğŸ¯ ì‘ì—… ëª¨ë“œ")
    modes = ["Normal", "Deep Research"]
    current_mode_index = 0
    if st.session_state.mode in modes:
        current_mode_index = modes.index(st.session_state.mode)
    elif st.session_state.mode in ["Clinical Analysis", "Molecular Design"]:
        # ê¸°ì¡´ 4ê°œ ëª¨ë“œ ì‚¬ìš©ìë¥¼ Deep Researchë¡œ ë§¤í•‘
        current_mode_index = 1
        st.session_state.mode = "Deep Research"
    
    mode = st.selectbox(
        "ëª¨ë“œ ì„ íƒ:",
        modes,
        index=current_mode_index,
        help="Normal: ì¼ë°˜ ì‹ ì•½ê°œë°œ ì§ˆë¬¸ | Deep Research: MCP ì—°ë™ ì‹¬ì¸µ ë¶„ì„"
    )
    
    # ëª¨ë“œ ë³€ê²½ ì‹œ MCP ì„œë²„ ë° ëª¨ë¸ ìë™ ì œì–´
    if mode != st.session_state.mode:
        st.session_state.mode = mode
        
        if mode == "Normal":
            # Normal ëª¨ë“œ: MCP ì„œë²„ ì¤‘ì§€ + Gemma3 ëª¨ë¸ë¡œ ë³€ê²½
            with st.spinner("Normal ëª¨ë“œë¡œ ì „í™˜ ì¤‘..."):
                # MCP ì„œë²„ ì¤‘ì§€
                if st.session_state.mcp_status:
                    if stop_mcp_servers():
                        st.session_state.mcp_status = False
                    else:
                        st.session_state.mcp_status = check_mcp_status()
                
                # ëª¨ë¸ì„ Gemma3:27b-it-q4_K_Më¡œ ë³€ê²½
                target_model = "Gemma3:27b-it-q4_K_M"
                if target_model in st.session_state.available_models:
                    st.session_state.model = target_model
                    st.success("âœ… Normal ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ - Gemma3 ëª¨ë¸ í™œì„±í™”")
                else:
                    st.success("âœ… Normal ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ")
                    st.warning("âš ï¸ Gemma3 ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ëª¨ë¸ì„ ìœ ì§€í•©ë‹ˆë‹¤")
        
        elif mode == "Deep Research":
            # Deep Research ëª¨ë“œ: MCP ì„œë²„ ì‹œì‘ + txgemma-chat ëª¨ë¸ë¡œ ë³€ê²½
            with st.spinner("Deep Research ëª¨ë“œë¡œ ì „í™˜ ì¤‘..."):
                # MCP ì„œë²„ ì‹œì‘
                if not st.session_state.mcp_status:
                    if start_mcp_servers():
                        st.session_state.mcp_status = True
                    else:
                        st.session_state.mcp_status = check_mcp_status()
                
                # ëª¨ë¸ì„ txgemma-chat:latestë¡œ ë³€ê²½
                target_model = "txgemma-chat:latest"
                if target_model in st.session_state.available_models:
                    st.session_state.model = target_model
                    st.success("âœ… Deep Research ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ - txgemma-chat ëª¨ë¸ í™œì„±í™”")
                else:
                    st.success("âœ… Deep Research ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ")
                    st.warning("âš ï¸ txgemma-chat ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ëª¨ë¸ì„ ìœ ì§€í•©ë‹ˆë‹¤")
        
        st.rerun()
    else:
        st.session_state.mode = mode
    
    # ëª¨ë“œë³„ ì„¤ëª…
    if mode == "Normal":
        st.info("ğŸ’¬ ì¼ë°˜ ëª¨ë“œ: ì‹ ì•½ê°œë°œ ê¸°ë³¸ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ AI ì‘ë‹µ")
    else:
        mcp_info = "ğŸ”¬ Deep Research ëª¨ë“œ: "
        if st.session_state.mcp_status:
            mcp_info += "MCP ì„œë²„ ì—°ë™ìœ¼ë¡œ ë…¼ë¬¸, ì„ìƒì‹œí—˜ ë°ì´í„° ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„"
        else:
            mcp_info += "MCP ì„œë²„ ë¯¸ì—°ê²° ìƒíƒœ - AI ë¶„ì„ ì¤‘ì‹¬ ì‘ë‹µ"
        st.info(mcp_info)
    
    # ëª¨ë¸ ì„ íƒ
    st.markdown("#### ğŸ¤– GAIA GPT ëª¨ë¸")
    if st.session_state.available_models:
        current_index = 0
        if st.session_state.model in st.session_state.available_models:
            current_index = st.session_state.available_models.index(st.session_state.model)
        
        model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ:",
            st.session_state.available_models,
            index=current_index,
            help="GAIA GPT ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"
        )
        st.session_state.model = model
        
        # ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        if "txgemma-chat" in model:
            st.info("ğŸ’¬ ì‹ ì•½ê°œë°œ íŠ¹í™”í˜• ëª¨ë¸ (Deep Research ìµœì í™”)")
        elif "txgemma-predict" in model:
            st.info("ğŸ“Š ë¶„ì„ íŠ¹í™” ëª¨ë¸ - ì˜ˆì¸¡/ë¶„ì„")
        elif "Gemma3" in model:
            st.info("ğŸ§  ìµœì‹  ëª¨ë¸ - ê³ í’ˆì§ˆ ì‘ë‹µ (Normal ëª¨ë“œ ìµœì í™”)")
    else:
        st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. GAIA GPT ì„œë²„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.session_state.model = "llama3.2:latest"
    
    # ì „ë¬¸ê°€ ëª¨ë“œ ì„¤ì •
    st.markdown("#### ğŸ‘¨â€ğŸ”¬ ì „ë¬¸ê°€ ëª¨ë“œ")
    expert_modes = {
        "default": "ì‹ ì•½ê°œë°œ ì „ë°˜ì— ëŒ€í•œ ê· í˜•ì¡íŒ AI ì–´ì‹œìŠ¤í„´íŠ¸",
        "clinical": "ì„ìƒì‹œí—˜ ë° í™˜ì ì¤‘ì‹¬ ì•½ë¬¼ ê°œë°œ ì „ë¬¸ê°€", 
        "research": "ë¬¸í—Œ ë¶„ì„ ë° ê³¼í•™ì  ì¦ê±° ì¢…í•© ì „ë¬¸ê°€",
        "chemistry": "ì˜ì•½í™”í•™ ë° ë¶„ì ì„¤ê³„ ì „ë¬¸ê°€",
        "regulatory": "ê¸€ë¡œë²Œ ì˜ì•½í’ˆ ê·œì œ ë° ìŠ¹ì¸ ì „ë¬¸ê°€"
    }
    
    if "expert_mode" not in st.session_state:
        st.session_state.expert_mode = "default"
    
    expert_mode = st.selectbox(
        "ì „ë¬¸ê°€ ëª¨ë“œ ì„ íƒ:",
        list(expert_modes.keys()),
        format_func=lambda x: expert_modes[x],
        index=list(expert_modes.keys()).index(st.session_state.expert_mode),
        help="AIì˜ ì „ë¬¸ì„± ë°©í–¥ì„ ì„¤ì •í•©ë‹ˆë‹¤"
    )
    st.session_state.expert_mode = expert_mode
    
    # ì„ íƒëœ ì „ë¬¸ê°€ ëª¨ë“œ ì •ë³´ í‘œì‹œ
    st.info(f"ğŸ¯ {expert_modes[expert_mode]}")
    
    # ê´€ì‹¬ ë¶„ì•¼ (ê¸°ì¡´ ìœ ì§€)
    st.markdown("#### ğŸ”¬ ê´€ì‹¬ ë¶„ì•¼")
    specialty = st.multiselect(
        "ì„¸ë¶€ ê´€ì‹¬ ë¶„ì•¼:",
        ["ì‹ ì•½ê°œë°œ", "ì„ìƒì‹œí—˜", "ì˜ì•½í™”í•™", "ìƒë¬¼ì •ë³´í•™", "ê·œì œê³¼í•™", "ì•½ë¬¼ë™íƒœí•™"],
        default=["ì‹ ì•½ê°œë°œ", "ì„ìƒì‹œí—˜"]
    )
    
    # ê³ ê¸‰ ì„¤ì •
    with st.expander("âš™ï¸ ê³ ê¸‰ ì„¤ì •"):
        st.session_state.temperature = st.slider(
            "ì°½ì˜ì„± (Temperature)", 
            0.0, 1.0, 
            st.session_state.temperature, 
            0.1,
            help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ì‘ë‹µ"
        )
        st.session_state.max_tokens = st.slider(
            "ìµœëŒ€ í† í° ìˆ˜", 
            100, 4000, 
            st.session_state.max_tokens, 
            100,
            help="ì‘ë‹µì˜ ìµœëŒ€ ê¸¸ì´"
        )
        show_sources = st.checkbox("ì¶œì²˜ í‘œì‹œ", value=True)
        realtime_mode = st.checkbox("ì‹¤ì‹œê°„ ëª¨ë“œ", value=False)
        
        # ë¹ ë¥¸ ì„¤ì • í”„ë¦¬ì…‹
        st.markdown("##### ğŸ›ï¸ ë¹ ë¥¸ ì„¤ì •")
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("âš¡ ë¹ ë¥¸ ì‘ë‹µ", help="ë¹ ë¥¸ ì‘ë‹µìš© ì„¤ì •"):
                st.session_state.temperature = 0.5
                st.session_state.max_tokens = 500
                st.rerun()
        
        with col2:
            if st.button("ğŸ”¬ ìƒì„¸ ë¶„ì„", help="ìƒì„¸ ë¶„ì„ìš© ì„¤ì •"):
                st.session_state.temperature = 0.7
                st.session_state.max_tokens = 1200
                st.rerun()
    
    # ë¹ ë¥¸ ì•¡ì…˜
    st.markdown("#### âš¡ ë¹ ë¥¸ ì•¡ì…˜")
    if st.button("ğŸ”„ ìƒˆ ì„¸ì…˜ ì‹œì‘"):
        st.session_state.messages = []
        st.session_state.session_id = f"session_{int(time.time())}"
        st.rerun()
    
    if st.button("ğŸ’¾ ëŒ€í™” ì €ì¥"):
        chat_data = {
            "session_id": st.session_state.session_id,
            "timestamp": datetime.now().isoformat(),
            "mode": st.session_state.mode,
            "model": st.session_state.model,
            "messages": st.session_state.messages
        }
        st.download_button(
            label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
            data=json.dumps(chat_data, ensure_ascii=False, indent=2),
            file_name=f"gaia-bt-chat-{st.session_state.session_id}.json",
            mime="application/json"
        )
    
    # í†µê³„
    st.markdown(f"""
    <div class="sidebar-section">
        <h4>ğŸ“ˆ ì„¸ì…˜ í†µê³„</h4>
        <p>ë©”ì‹œì§€ ìˆ˜: {len(st.session_state.messages)}</p>
        <p>í™œì„± ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}</p>
        <p>í˜„ì¬ ëª¨ë“œ: <span class="mode-badge mode-{'research' if 'Research' in st.session_state.mode else 'normal'}">{st.session_state.mode}</span></p>
        <p>ì „ë¬¸ê°€ ëª¨ë“œ: <span class="mode-badge expert-{st.session_state.get('expert_mode', 'default')}">
        {'ì‹ ì•½ê°œë°œ ì „ë°˜ì— ëŒ€í•œ ê· í˜•ì¡íŒ AI ì–´ì‹œìŠ¤í„´íŠ¸' if st.session_state.get('expert_mode', 'default') == 'default' 
         else 'ì„ìƒì‹œí—˜ ë° í™˜ì ì¤‘ì‹¬ ì•½ë¬¼ ê°œë°œ ì „ë¬¸ê°€' if st.session_state.get('expert_mode', 'default') == 'clinical'
         else 'ë¬¸í—Œ ë¶„ì„ ë° ê³¼í•™ì  ì¦ê±° ì¢…í•© ì „ë¬¸ê°€' if st.session_state.get('expert_mode', 'default') == 'research'
         else 'ì˜ì•½í™”í•™ ë° ë¶„ì ì„¤ê³„ ì „ë¬¸ê°€' if st.session_state.get('expert_mode', 'default') == 'chemistry'
         else 'ê¸€ë¡œë²Œ ì˜ì•½í’ˆ ê·œì œ ë° ìŠ¹ì¸ ì „ë¬¸ê°€' if st.session_state.get('expert_mode', 'default') == 'regulatory'
         else 'ì‹ ì•½ê°œë°œ ì „ë°˜ì— ëŒ€í•œ ê· í˜•ì¡íŒ AI ì–´ì‹œìŠ¤í„´íŠ¸'}
        </span></p>
    </div>
    """, unsafe_allow_html=True)

# ë©”ì¸ ì±„íŒ… ì˜ì—­
st.markdown("### ğŸ’¬ ëŒ€í™”")

# ëª¨ë“œë³„ ì•ˆë‚´ ë©”ì‹œì§€
if st.session_state.mode == "Normal":
    st.info("**Normal ëª¨ë“œ**: ì‹ ì•½ê°œë°œ ì „ë°˜ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ AI ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
else:  # Deep Research
    if st.session_state.mcp_status:
        st.info("**Deep Research ëª¨ë“œ**: MCP ì„œë²„ ì—°ë™ìœ¼ë¡œ ë…¼ë¬¸, ì„ìƒì‹œí—˜ ë°ì´í„°ë¥¼ í™œìš©í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
    else:
        st.warning("**Deep Research ëª¨ë“œ**: MCP ì„œë²„ê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ AI ì¤‘ì‹¬ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ì™„ì „í•œ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” MCP ì„œë²„ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")

# ì¶”ì²œ ì§ˆë¬¸ (ì²˜ìŒ ì‹œì‘í•  ë•Œë§Œ í‘œì‹œ)
if len(st.session_state.messages) == 0:
    st.markdown("#### ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸")
    
    # ì „ë¬¸ê°€ ëª¨ë“œë³„ ì¶”ì²œ ì§ˆë¬¸
    expert_questions = {
        "default": {
            "Normal": [
                ("ğŸ§ª ì‹ ì•½ê°œë°œ ê¸°ì´ˆ", "ì‹ ì•½ê°œë°œì˜ ê¸°ë³¸ ê³¼ì •ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
                ("ğŸ’Š ì„ìƒì‹œí—˜ì´ë€?", "ì„ìƒì‹œí—˜ì´ ì™œ í•„ìš”í•˜ê³  ì–´ë–»ê²Œ ì§„í–‰ë˜ë‚˜ìš”?"),
                ("âš—ï¸ ì•½ë¬¼ ì‘ìš©ì›ë¦¬", "ì•½ë¬¼ì´ ìš°ë¦¬ ëª¸ì—ì„œ ì–´ë–»ê²Œ ì‘ìš©í•˜ë‚˜ìš”?"),
                ("ğŸ¥ FDA ìŠ¹ì¸ê³¼ì •", "ì‹ ì•½ì´ ìŠ¹ì¸ë°›ëŠ” ê³¼ì •ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
            ],
            "Deep Research": [
                ("ğŸ”¬ mRNA ë°±ì‹  ì—°êµ¬", "mRNA ë°±ì‹  ê¸°ìˆ ì˜ ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ ì‹ ì•½ê°œë°œ ì ìš© ê°€ëŠ¥ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ§¬ í•­ì•”ì œ ì €í•­ì„±", "í•­ì•”ì œ ì €í•­ì„± ê·¹ë³µì„ ìœ„í•œ ìµœì‹  ì—°êµ¬ í˜„í™©ê³¼ í•´ê²° ë°©ì•ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ¯ ì •ë°€ì˜í•™ ë™í–¥", "ì •ë°€ì˜í•™ ê¸°ë°˜ ì‹ ì•½ê°œë°œì˜ í˜„í™©ê³¼ ë¯¸ë˜ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ”¬ AI ì‹ ì•½ê°œë°œ", "AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ì‹ ì•½ê°œë°œì˜ ìµœì‹  ë™í–¥ê³¼ ì„±ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
            ]
        },
        "clinical": {
            "Normal": [
                ("ğŸ¥ ì„ìƒì‹œí—˜ ì„¤ê³„", "ì„ìƒì‹œí—˜ì€ ì–´ë–»ê²Œ ì„¤ê³„í•˜ê³  ê³„íší•˜ë‚˜ìš”?"),
                ("ğŸ‘¥ í™˜ì ëª¨ì§‘", "ì„ìƒì‹œí—˜ì—ì„œ ì ì ˆí•œ í™˜ìë¥¼ ëª¨ì§‘í•˜ëŠ” ë°©ë²•ì€?"),
                ("ğŸ“Š ì„ìƒ ë°ì´í„°", "ì„ìƒì‹œí—˜ ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ë¶„ì„í•˜ë‚˜ìš”?"),
                ("âš–ï¸ ìœ¤ë¦¬ì  ê³ ë ¤", "ì„ìƒì‹œí—˜ì—ì„œ í™˜ì ì•ˆì „ê³¼ ìœ¤ë¦¬ëŠ” ì–´ë–»ê²Œ ë³´ì¥í•˜ë‚˜ìš”?")
            ],
            "Deep Research": [
                ("ğŸ”¬ ë°”ì´ì˜¤ë§ˆì»¤ ê°œë°œ", "ì„ìƒì‹œí—˜ì—ì„œ ë°”ì´ì˜¤ë§ˆì»¤ ê°œë°œê³¼ ê²€ì¦ ì „ëµì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ“ˆ ì ì‘ì  ì„¤ê³„", "ì ì‘ì  ì„ìƒì‹œí—˜ ì„¤ê³„ì˜ ìµœì‹  ë™í–¥ê³¼ ì¥ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸŒ ê¸€ë¡œë²Œ ì„ìƒ", "ë‹¤êµ­ê°€ ì„ìƒì‹œí—˜ì˜ ë„ì „ê³¼ì œì™€ í•´ê²° ë°©ì•ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ’¡ í˜ì‹ ì  ì‹œí—˜ë²•", "í˜ì‹ ì  ì„ìƒì‹œí—˜ ë°©ë²•ë¡ ì˜ í˜„í™©ê³¼ ë¯¸ë˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
            ]
        },
        "research": {
            "Normal": [
                ("ğŸ“š ë¬¸í—Œ ê²€ìƒ‰", "ì‹ ì•½ê°œë°œ ì—°êµ¬ë¥¼ ìœ„í•œ íš¨ê³¼ì ì¸ ë¬¸í—Œ ê²€ìƒ‰ ë°©ë²•ì€?"),
                ("ğŸ“Š ì—°êµ¬ ë°©ë²•ë¡ ", "ì‹ ì•½ê°œë°œ ì—°êµ¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë°©ë²•ë¡ ì€?"),
                ("ğŸ”— ì—°êµ¬ ë™í–¥", "ìµœê·¼ ì‹ ì•½ê°œë°œ ì—°êµ¬ì˜ ì£¼ìš” íŠ¸ë Œë“œëŠ”?"),
                ("ğŸ“‹ ì—°êµ¬ ì„¤ê³„", "íš¨ê³¼ì ì¸ ì‹ ì•½ê°œë°œ ì—°êµ¬ëŠ” ì–´ë–»ê²Œ ì„¤ê³„í•˜ë‚˜ìš”?")
            ],
            "Deep Research": [
                ("ğŸ§¬ ì˜¤ë¯¹ìŠ¤ ì—°êµ¬", "ì˜¤ë¯¹ìŠ¤ ê¸°ìˆ ì„ í™œìš©í•œ ì‹ ì•½ê°œë°œ ì—°êµ¬ì˜ ìµœì‹  ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ¤– AI ì—°êµ¬ ë™í–¥", "AIë¥¼ í™œìš©í•œ ì‹ ì•½ê°œë°œ ì—°êµ¬ì˜ í˜„í™©ê³¼ ì„±ê³¼ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ”¬ íŠ¸ëœìŠ¤ë ˆì´ì…”ë„", "íŠ¸ëœìŠ¤ë ˆì´ì…”ë„ ì—°êµ¬ì˜ ì‹ ì•½ê°œë°œì—ì„œì˜ ì—­í• ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸŒ êµ­ì œ í˜‘ë ¥", "êµ­ì œ ì‹ ì•½ê°œë°œ ì—°êµ¬ í˜‘ë ¥ì˜ í˜„í™©ê³¼ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")
            ]
        },
        "chemistry": {
            "Normal": [
                ("âš—ï¸ ë¶„ì ì„¤ê³„", "ì‹ ì•½ì˜ ë¶„ì êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ì„¤ê³„í•˜ë‚˜ìš”?"),
                ("ğŸ§ª í™”í•™ í•©ì„±", "ì‹ ì•½ í›„ë³´ë¬¼ì§ˆì€ ì–´ë–»ê²Œ í•©ì„±í•˜ë‚˜ìš”?"),
                ("ğŸ“ˆ SAR ë¶„ì„", "êµ¬ì¡°-í™œì„± ê´€ê³„(SAR) ë¶„ì„ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"),
                ("ğŸ¯ íƒ€ê²Ÿ ê²°í•©", "ì•½ë¬¼ì´ íƒ€ê²Ÿ ë‹¨ë°±ì§ˆì— ê²°í•©í•˜ëŠ” ì›ë¦¬ëŠ”?")
            ],
            "Deep Research": [
                ("ğŸ’Š ì•½ë¬¼ ìµœì í™”", "lead í™”í•©ë¬¼ì˜ ì•½ë¬¼ì„± ìµœì í™” ì „ëµì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ”¬ ì»´í“¨í„° ì„¤ê³„", "ì»´í“¨í„° ê¸°ë°˜ ì•½ë¬¼ ì„¤ê³„(CADD)ì˜ ìµœì‹  ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("âš¡ í”„ë¡œí… ë¶„í•´", "PROTACê³¼ ë¶„ìì ‘ì°©ì œ ê¸°ìˆ ì˜ í˜„í™©ê³¼ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ§¬ í™”í•™ ìƒë¬¼í•™", "í™”í•™ ìƒë¬¼í•™ ì ‘ê·¼ë²•ì˜ ì‹ ì•½ê°œë°œ ì ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")
            ]
        },
        "regulatory": {
            "Normal": [
                ("ğŸ“‹ ê·œì œ ê°€ì´ë“œ", "ì‹ ì•½ ìŠ¹ì¸ì„ ìœ„í•œ ê·œì œ ìš”êµ¬ì‚¬í•­ì€?"),
                ("ğŸŒ ê¸€ë¡œë²Œ ê·œì œ", "FDA, EMA, MFDSì˜ ìŠ¹ì¸ ê³¼ì • ì°¨ì´ì ì€?"),
                ("ğŸ“„ ì„œë¥˜ ì¤€ë¹„", "ì‹ ì•½ ìŠ¹ì¸ ì‹ ì²­ ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ”?"),
                ("â° ìŠ¹ì¸ ì ˆì°¨", "ì‹ ì•½ ìŠ¹ì¸ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ê³¼ ê³¼ì •ì€?")
            ],
            "Deep Research": [
                ("ğŸš€ í˜ì‹  ê²½ë¡œ", "í˜ì‹ ì  ì‹ ì•½ì„ ìœ„í•œ ê·œì œ ê²½ë¡œ(íŒ¨ìŠ¤íŠ¸íŠ¸ë™, ëŒíŒŒêµ¬)ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸŒ êµ­ì œ ì¡°í™”", "ICH ê°€ì´ë“œë¼ì¸ê³¼ êµ­ì œ ê·œì œ ì¡°í™”ì˜ í˜„í™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ“Š ì‹¤ì œ ì¦ê±°", "ì‹¤ì œ ì„ìƒ ì¦ê±°(RWE) í™œìš©ì˜ ê·œì œ ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."),
                ("ğŸ”® ë¯¸ë˜ ê·œì œ", "ë””ì§€í„¸ ì¹˜ë£Œì œì™€ AI ê¸°ë°˜ ì˜ë£Œê¸°ê¸°ì˜ ê·œì œ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")
            ]
        }
    }
    
    current_expert = st.session_state.get('expert_mode', 'default')
    questions = expert_questions.get(current_expert, expert_questions['default'])[st.session_state.mode]
    
    cols = st.columns(2)
    for i, (title, content) in enumerate(questions):
        with cols[i % 2]:
            if st.button(title):
                st.session_state.messages.append({
                    "role": "user", 
                    "content": content
                })
                st.rerun()

# ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    if message["role"] == "user":
        st.markdown(f"""
        <div class="user-message">
            <strong>ğŸ‘¤ ì‚¬ìš©ì:</strong><br>
            {message["content"]}
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="bot-message">
            <strong>ğŸ§¬ GAIA-BT:</strong><br>
            {message["content"]}
        </div>
        """, unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ì‹ ì•½ê°œë°œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”...")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # Ollama APIë¥¼ í†µí•œ ë´‡ ì‘ë‹µ ìƒì„±
    if st.session_state.ollama_status:
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        status_container = st.container()
        progress_bar = status_container.progress(0)
        status_text = status_container.empty()
        
        try:
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (ì‹œê°„ ê¸°ë°˜)
            import threading
            
            # ì‘ë‹µ ìƒì„±ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            response_result = {"completed": False, "response": None}
            
            def generate_response():
                response_result["response"] = generate_ollama_response(
                    user_input, 
                    st.session_state.model, 
                    st.session_state.mode,
                    st.session_state.expert_mode
                )
                response_result["completed"] = True
            
            # ì‘ë‹µ ìƒì„± ìŠ¤ë ˆë“œ ì‹œì‘
            response_thread = threading.Thread(target=generate_response)
            response_thread.start()
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ (5ë¶„ = 300ì´ˆ)
            status_text.text(f"ğŸ”„ ëª¨ë¸ ì¤€ë¹„ ì¤‘... ({st.session_state.model})")
            progress_bar.progress(5)
            time.sleep(1)
            
            status_text.text(f"ğŸ§  {st.session_state.mode} ëª¨ë“œë¡œ ë¶„ì„ ì¤‘...")
            progress_bar.progress(10)
            time.sleep(1)
            
            status_text.text("âš¡ ì‘ë‹µ ìƒì„± ì¤‘... (ìµœëŒ€ 5ë¶„ ì†Œìš”)")
            
            # ì‹œê°„ ê¸°ë°˜ í”„ë¡œê·¸ë ˆìŠ¤ ë°” (10% ~ 90% ê¹Œì§€)
            start_time = time.time()
            max_wait_time = 300  # 5ë¶„
            
            while not response_result["completed"]:
                elapsed_time = time.time() - start_time
                if elapsed_time >= max_wait_time:
                    break
                
                # 10%ì—ì„œ 90%ê¹Œì§€ ì‹œê°„ì— ë¹„ë¡€í•˜ì—¬ ì¦ê°€
                progress = min(10 + (elapsed_time / max_wait_time) * 80, 90)
                progress_bar.progress(int(progress))
                
                # ë‚¨ì€ ì‹œê°„ í‘œì‹œ
                remaining_time = max(0, max_wait_time - elapsed_time)
                minutes = int(remaining_time // 60)
                seconds = int(remaining_time % 60)
                status_text.text(f"âš¡ ì‘ë‹µ ìƒì„± ì¤‘... (ë‚¨ì€ ì‹œê°„: {minutes:02d}:{seconds:02d})")
                
                time.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
            
            # ì‘ë‹µ ì™„ë£Œ ëŒ€ê¸°
            response_thread.join()
            bot_response = response_result["response"]
            
            # ì™„ë£Œ ìƒíƒœ
            progress_bar.progress(100)
            status_text.text("âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ!")
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
            time.sleep(1)
            status_container.empty()
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì²˜ë¦¬
            progress_bar.progress(100)
            status_text.text(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            bot_response = f"""
âŒ **ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ**

ìš”ì²­ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì‹œë„í•´ë³¼ í•´ê²°ì±…**:
1. **ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‹œë„**: "{user_input[:30]}..." â†’ "ì‹ ì•½ê°œë°œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
2. **ì„¤ì • ì¡°ì •**: ì‚¬ì´ë“œë°”ì—ì„œ ìµœëŒ€ í† í° ìˆ˜ë¥¼ 500-800ìœ¼ë¡œ ì¤„ì´ê¸°
3. **ëª¨ë¸ ë³€ê²½**: ë‹¤ë¥¸ Ollama ëª¨ë¸ ì„ íƒí•´ë³´ê¸°
4. **ì„œë²„ í™•ì¸**: "ì„œë²„ ìƒíƒœ í™•ì¸" ë²„íŠ¼ í´ë¦­

**í˜„ì¬ ì„¤ì •**:
- ëª¨ë¸: {st.session_state.model}
- ëª¨ë“œ: {st.session_state.mode}
- ìµœëŒ€ í† í°: {st.session_state.get("max_tokens", 1000)}
            """
            
            time.sleep(2)
            status_container.empty()
    else:
        # Ollama ì„œë²„ ì—°ê²° ë¶ˆê°€ ì‹œ í´ë°± ì‘ë‹µ
        bot_response = f"""
âš ï¸ **GAIA GPT ì„œë²„ ì—°ê²° ë¶ˆê°€**

í˜„ì¬ GAIA GPT ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ìš”ì²­í•˜ì‹  ì§ˆë¬¸**: "{user_input}"

**í•´ê²° ë°©ë²•**:
1. **ì„œë²„ ì‹œì‘**: í„°ë¯¸ë„ì—ì„œ `ollama serve` ì‹¤í–‰
2. **ìƒíƒœ í™•ì¸**: ì‚¬ì´ë“œë°”ì˜ "ğŸ”„ ì„œë²„ ìƒíƒœ í™•ì¸" ë²„íŠ¼ í´ë¦­
3. **í¬íŠ¸ í™•ì¸**: http://localhost:11434/api/tags ì ‘ì† í…ŒìŠ¤íŠ¸

**ì„œë²„ ì‹œì‘ í›„ ì´ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.**
        """
    
    # ë´‡ ì‘ë‹µ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": bot_response})
    st.rerun()

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #6b7280; font-size: 0.875rem;">
    ğŸ§¬ GAIA-BT GPT v2.0 | ì‹ ì•½ê°œë°œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸<br>
    Powered by Advanced Language Models | Â© 2024 GAIA-BT Labs
</div>
""", unsafe_allow_html=True)