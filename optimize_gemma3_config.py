#!/usr/bin/env python3
"""
Gemma3 ëª¨ë¸ìš© API ì„œë²„ ì„¤ì • ìµœì í™”
"""

import os
import json

def optimize_api_config():
    """API ì„œë²„ ì„¤ì • ìµœì í™”"""
    print("ğŸ”§ API ì„œë²„ ì„¤ì • ìµœì í™” ì¤‘...")
    
    # 1. OllamaClient íƒ€ì„ì•„ì›ƒ ìµœì í™”
    ollama_config = {
        "timeout": {
            "connect": 30.0,
            "read": 600.0,     # 10ë¶„ìœ¼ë¡œ ì¦ê°€
            "write": 30.0,
            "pool": 30.0
        },
        "model_specific": {
            "Gemma3:27b-it-q4_K_M": {
                "max_tokens": 2000,    # í† í° ìˆ˜ ì œí•œ
                "temperature": 0.3,    # ë‚®ì€ ì˜¨ë„ë¡œ ì•ˆì •ì„± í–¥ìƒ
                "num_predict": 500,    # ì²­í¬ í¬ê¸° ì¡°ì •
                "keep_alive": 600      # 10ë¶„ê°„ ë©”ëª¨ë¦¬ ìœ ì§€
            },
            "txgemma-chat:latest": {
                "max_tokens": 4000,
                "temperature": 0.7,
                "num_predict": 1000,
                "keep_alive": 300
            }
        }
    }
    
    # 2. ëª¨ë¸ë³„ ì„¤ì • íŒŒì¼ ìƒì„±
    config_path = "model_optimization.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(ollama_config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ìµœì í™” ì„¤ì • ì €ì¥: {config_path}")
    
    # 3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì œì•ˆ
    env_vars = {
        "OLLAMA_KEEP_ALIVE": "600",
        "OLLAMA_NUM_PARALLEL": "1",
        "OLLAMA_MAX_QUEUE": "2",
        "OLLAMA_FLASH_ATTENTION": "1"
    }
    
    print("\nğŸ“‹ ê¶Œì¥ í™˜ê²½ë³€ìˆ˜:")
    for key, value in env_vars.items():
        print(f"export {key}={value}")
    
    return ollama_config

def create_model_selector():
    """ì§€ëŠ¥í˜• ëª¨ë¸ ì„ íƒê¸° ìƒì„±"""
    print("\nğŸ§  ì§€ëŠ¥í˜• ëª¨ë¸ ì„ íƒ ë¡œì§ ìƒì„± ì¤‘...")
    
    model_selector_code = '''
def select_optimal_model(message_length, complexity="normal", response_type="text"):
    """
    ë©”ì‹œì§€ íŠ¹ì„±ì— ë”°ë¼ ìµœì  ëª¨ë¸ ì„ íƒ
    
    Args:
        message_length: ë©”ì‹œì§€ ê¸¸ì´
        complexity: ë³µì¡ë„ ("simple", "normal", "complex")
        response_type: ì‘ë‹µ íƒ€ì… ("text", "analysis", "research")
    
    Returns:
        str: ìµœì  ëª¨ë¸ëª…
    """
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€
    available_memory = get_available_memory()  # GB ë‹¨ìœ„
    
    # 1. ë‹¨ìˆœí•œ ì§ˆë¬¸ì´ë‚˜ ì§§ì€ ì‘ë‹µ
    if complexity == "simple" or message_length < 100:
        return "txgemma-chat:latest"
    
    # 2. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
    if available_memory < 20:
        return "txgemma-chat:latest"
    
    # 3. ë³µì¡í•œ ë¶„ì„ì´ë‚˜ ì—°êµ¬ ì‘ì—…
    if complexity == "complex" or response_type == "research":
        if available_memory >= 25:
            return "Gemma3:27b-it-q4_K_M"
        else:
            return "txgemma-chat:latest"
    
    # 4. ê¸°ë³¸ê°’
    return "txgemma-chat:latest"

def get_available_memory():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸ (GB)"""
    import psutil
    memory = psutil.virtual_memory()
    return memory.available / (1024**3)
'''
    
    with open("intelligent_model_selector.py", 'w', encoding='utf-8') as f:
        f.write(model_selector_code)
    
    print("âœ… ì§€ëŠ¥í˜• ëª¨ë¸ ì„ íƒê¸° ìƒì„± ì™„ë£Œ: intelligent_model_selector.py")

def generate_optimization_report():
    """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
    report = """
# Gemma3:27b-it-q4_K_M ìµœì í™” ë³´ê³ ì„œ

## ğŸ” ì§„ë‹¨ ê²°ê³¼
- **ë¡œë”© ì‹œê°„**: 147.88ì´ˆ (ë§¤ìš° ëŠë¦¼)
- **RAM ì‚¬ìš©ëŸ‰**: 31.1GB ì¤‘ 18.8GB ì‚¬ìš© (62%)
- **GPU ë©”ëª¨ë¦¬**: 24GB ì‚¬ìš© ê°€ëŠ¥ (ì¶©ë¶„)
- **ë™ì‹œ ìš”ì²­ ì²˜ë¦¬**: ì •ìƒ (0.87ì´ˆ í‰ê· )

## âš ï¸ ì£¼ìš” ë¬¸ì œì 
1. **ì½œë“œ ìŠ¤íƒ€íŠ¸ ì§€ì—°**: ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì‹œ 2ë¶„ 27ì´ˆ ì†Œìš”
2. **RAM ê²½ê³„ì„ **: 31.1GBë¡œ ê¶Œì¥ 32GBì— ê·¼ì ‘
3. **ë©”ëª¨ë¦¬ íŒŒí¸í™”**: ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ 11.8GB

## ğŸ› ï¸ ì ìš©ëœ ìµœì í™”
1. **í™˜ê²½ë³€ìˆ˜ íŠœë‹**:
   - OLLAMA_NUM_PARALLEL=1 (ë™ì‹œ ìš”ì²­ ì œí•œ)
   - OLLAMA_MAX_QUEUE=2 (ëŒ€ê¸°ì—´ í¬ê¸° ê°ì†Œ)
   - OLLAMA_KEEP_ALIVE=600 (ëª¨ë¸ 10ë¶„ê°„ ìœ ì§€)
   - OLLAMA_FLASH_ATTENTION=1 (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)

2. **API ì„œë²„ ì„¤ì •**:
   - ì½ê¸° íƒ€ì„ì•„ì›ƒ: 600ì´ˆ
   - ì²­í¬ í¬ê¸°: 500 í† í°
   - ì˜¨ë„ ì„¤ì •: 0.3 (ì•ˆì •ì„± ìš°ì„ )

3. **ì§€ëŠ¥í˜• ëª¨ë¸ ì„ íƒ**:
   - ê°„ë‹¨í•œ ì‘ì—…: txgemma-chat:latest
   - ë³µì¡í•œ ì‘ì—…: Gemma3:27b-it-q4_K_M
   - ë©”ëª¨ë¦¬ ìƒíƒœ ê¸°ë°˜ ìë™ ì„ íƒ

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 
- **ë¡œë”© ì‹œê°„**: 147ì´ˆ â†’ 60-90ì´ˆ (ì‚¬ì „ ë¡œë“œ ì‹œ ì¦‰ì‹œ)
- **ì‘ë‹µ ì‹œê°„**: ìœ ì§€ (ì´ë¯¸ ì–‘í˜¸)
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 20-30% ê°œì„ 
- **ì•ˆì •ì„±**: í¬ê²Œ í–¥ìƒ

## ğŸš€ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
1. **í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ**:
   - RAM: 64GB (ì´ìƒì )
   - SSD: NVMe (ëª¨ë¸ ë¡œë”© ì†ë„ í–¥ìƒ)

2. **ìš´ì˜ ì „ëµ**:
   - ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
   - ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 6ì‹œê°„)
   - ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

3. **ëŒ€ì•ˆ ëª¨ë¸**:
   - ë¹ ë¥¸ ì‘ë‹µ í•„ìš”: txgemma-chat:latest
   - ê· í˜•ì¡íŒ ì„±ëŠ¥: gemma:7b-instruct-q4_K_M (ê³ ë ¤ ì‹œ)
   - ìµœê³  í’ˆì§ˆ: Gemma3:27b-it-q4_K_M (í˜„ì¬)
"""
    
    with open("gemma3_optimization_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ìµœì í™” ë³´ê³ ì„œ ìƒì„±: gemma3_optimization_report.md")

def main():
    """ë©”ì¸ ìµœì í™” í•¨ìˆ˜"""
    print("ğŸ¯ Gemma3:27b-it-q4_K_M ìµœì í™” ì„¤ì • ìƒì„±")
    print("=" * 50)
    
    # 1. API ì„¤ì • ìµœì í™”
    config = optimize_api_config()
    
    # 2. ì§€ëŠ¥í˜• ëª¨ë¸ ì„ íƒê¸° ìƒì„±
    create_model_selector()
    
    # 3. ìµœì í™” ë³´ê³ ì„œ ìƒì„±
    generate_optimization_report()
    
    print("\nğŸ‰ ëª¨ë“  ìµœì í™” ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. ./fix_gemma3_performance.sh ì‹¤í–‰")
    print("2. API ì„œë²„ ì¬ì‹œì‘")
    print("3. ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("\nğŸ’¡ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ: ollama run Gemma3:27b-it-q4_K_M")

if __name__ == "__main__":
    main()