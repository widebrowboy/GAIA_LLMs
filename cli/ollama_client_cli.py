#!/usr/bin/env python3
"""
Ollama í´ë¼ì´ì–¸íŠ¸ CLI ë„êµ¬
OllamaClient ëª¨ë“ˆì„ ìœ„í•œ ë…ë¦½ì ì¸ ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
"""

import os
import sys
import json
import asyncio
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ëª¨ë“ˆ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ëª¨ë“ˆ ì„í¬íŠ¸
from src.api.ollama_client import OllamaClient


async def generate_text(args):
    """
    Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±
    """
    try:
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = OllamaClient(
            model=args.model,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            min_response_length=args.min_length
        )
        
        # API ê°€ìš©ì„± í™•ì¸
        print("ğŸ”„ Ollama API ê°€ìš©ì„± í™•ì¸ ì¤‘...")
        status = await client.check_availability()
        
        if status["status"] != "available":
            print(f"âŒ Ollama APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {status.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return 1
            
        print(f"âœ… API ì‚¬ìš© ê°€ëŠ¥: {status.get('current_model', client.model)} ëª¨ë¸")
        
        if "models" in status:
            print(f"ğŸ§  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(status.get('models', [])[:5])}")
            if len(status.get('models', [])) > 5:
                print(f"   (ì™¸ {len(status.get('models', [])) - 5}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)")
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        prompt = args.prompt
        system_prompt = args.system
        
        # í”„ë¡¬í”„íŠ¸ê°€ íŒŒì¼ì—ì„œ ì˜¤ëŠ” ê²½ìš°
        if args.prompt_file:
            if not os.path.exists(args.prompt_file):
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.prompt_file}")
                return 1
                
            with open(args.prompt_file, 'r', encoding='utf-8') as f:
                prompt = f.read()
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ íŒŒì¼ì—ì„œ ì˜¤ëŠ” ê²½ìš°
        if args.system_file:
            if not os.path.exists(args.system_file):
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.system_file}")
                return 1
                
            with open(args.system_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read()
        
        # GPU ê°€ì† íŒŒë¼ë¯¸í„° ì¶œë ¥
        print(f"ğŸš€ GPU ê°€ì† íŒŒë¼ë¯¸í„°:")
        for key, value in client.gpu_params.items():
            print(f"  - {key}: {value}")
        
        # ë‹¨ì¼ ë˜ëŠ” ë³‘ë ¬ ìƒì„±
        start_time = datetime.now()
        
        if args.parallel > 1:
            # ë³‘ë ¬ ìƒì„±
            print(f"ğŸ”„ í”„ë¡¬í”„íŠ¸ë¥¼ {args.parallel}ê°œ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")
            
            # ë‹¤ì–‘í•œ ì˜¨ë„ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = []
            for i in range(args.parallel):
                temp_variation = args.temperature * (0.8 + 0.4 * (i / args.parallel))
                prompts.append({
                    "prompt": prompt,
                    "system": system_prompt,
                    "temperature": temp_variation
                })
                
            responses = await client.generate_parallel(prompts, max_concurrent=args.concurrent)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            print(f"âœ… ì‘ë‹µ {len(responses)}ê°œ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            
            # ê²°ê³¼ ì¶œë ¥
            for i, resp in enumerate(responses):
                print(f"\n===== ì‘ë‹µ #{i+1} =====")
                if isinstance(resp, Exception):
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(resp)}")
                else:
                    print(resp[:args.max_display] + ("..." if len(resp) > args.max_display else ""))
            
            # ê²°ê³¼ ì €ì¥
            if args.output:
                output_file = args.output
                dir_path = os.path.dirname(output_file) if os.path.dirname(output_file) else '.'
                os.makedirs(dir_path, exist_ok=True)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for i, resp in enumerate(responses):
                        f.write(f"===== ì‘ë‹µ #{i+1} =====\n\n")
                        if isinstance(resp, Exception):
                            f.write(f"ì˜¤ë¥˜: {str(resp)}\n\n")
                        else:
                            f.write(f"{resp}\n\n")
                
                print(f"ğŸ“ ì‘ë‹µì„ '{output_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        else:
            # ë‹¨ì¼ ìƒì„±
            print(f"ğŸ”„ ì‘ë‹µ ìƒì„± ì¤‘...")
            response = await client.generate(prompt, system_prompt, args.temperature)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            print(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\n===== ìƒì„±ëœ ì‘ë‹µ =====")
            print(response[:args.max_display] + ("..." if len(response) > args.max_display else ""))
            
            # ê²°ê³¼ ì €ì¥
            if args.output:
                output_file = args.output
                dir_path = os.path.dirname(output_file) if os.path.dirname(output_file) else '.'
                os.makedirs(dir_path, exist_ok=True)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(response)
                
                print(f"ğŸ“ ì‘ë‹µì„ '{output_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return 1


async def check_api(args):
    """
    Ollama API ê°€ìš©ì„±ê³¼ ëª¨ë¸ ëª©ë¡ í™•ì¸
    """
    try:
        client = OllamaClient(model=args.model)
        
        print("ğŸ”„ Ollama API ê°€ìš©ì„± í™•ì¸ ì¤‘...")
        status = await client.check_availability()
        
        if status["status"] != "available":
            print(f"âŒ Ollama APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {status.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return 1
        
        print(f"âœ… Ollama API ì‚¬ìš© ê°€ëŠ¥")
        print(f"ğŸŒ API URL: {client.ollama_url}")
        print(f"ğŸ§  í˜„ì¬ ì„ íƒëœ ëª¨ë¸: {client.model}")
        
        if "current_model_available" in status:
            if status["current_model_available"]:
                print(f"âœ… ì„ íƒëœ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")
            else:
                print(f"âš ï¸ ì„ íƒëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        
        if "models" in status:
            print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ({len(status.get('models', []))}ê°œ):")
            for i, model in enumerate(status.get("models", [])):
                print(f"{i+1}. {model}")
        
        # GPU ê°€ì† íŒŒë¼ë¯¸í„° ì¶œë ¥
        print(f"\nğŸš€ GPU ê°€ì† íŒŒë¼ë¯¸í„°:")
        for key, value in client.gpu_params.items():
            print(f"  - {key}: {value}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return 1


async def main():
    parser = argparse.ArgumentParser(
        description='Ollama í´ë¼ì´ì–¸íŠ¸ CLI ë„êµ¬',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ì„œë¸ŒíŒŒì„œ ì„¤ì •
    subparsers = parser.add_subparsers(dest='command', help='ëª…ë ¹ì–´')
    
    # í…ìŠ¤íŠ¸ ìƒì„± ëª…ë ¹ì–´
    gen_parser = subparsers.add_parser('generate', help='í…ìŠ¤íŠ¸ ìƒì„±')
    
    # í”„ë¡¬í”„íŠ¸ ì…ë ¥ ë°©ì‹ (ë‘˜ ì¤‘ í•˜ë‚˜ í•„ìš”)
    prompt_group = gen_parser.add_argument_group('í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì˜µì…˜ (í•˜ë‚˜ë¥¼ ì„ íƒ)')
    prompt_ex = prompt_group.add_mutually_exclusive_group(required=True)
    prompt_ex.add_argument('--prompt', '-p', type=str, help='ìƒì„±ì— ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸')
    prompt_ex.add_argument('--prompt-file', '-pf', type=str, help='í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ íŒŒì¼ ê²½ë¡œ')
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì˜µì…˜
    gen_parser.add_argument('--system', '-s', type=str, default='',
                          help='ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)')
    gen_parser.add_argument('--system-file', '-sf', type=str, default='',
                          help='ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ íŒŒì¼ ê²½ë¡œ')
    
    # ìƒì„± ë§¤ê°œë³€ìˆ˜
    gen_parser.add_argument('--model', '-m', type=str, default='gemma3:4b',
                          help='ì‚¬ìš©í•  Ollama ëª¨ë¸')
    gen_parser.add_argument('--temperature', '-t', type=float, default=0.7,
                          help='ìƒì„± ì˜¨ë„ (0.1~1.0, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )')
    gen_parser.add_argument('--max-tokens', '-mt', type=int, default=4000,
                          help='ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜')
    gen_parser.add_argument('--min-length', '-ml', type=int, default=1000,
                          help='ì‘ë‹µì˜ ìµœì†Œ ê¸¸ì´')
    gen_parser.add_argument('--parallel', '-pr', type=int, default=1,
                          help='ë³‘ë ¬ ìƒì„±í•  ì‘ë‹µ ìˆ˜')
    gen_parser.add_argument('--concurrent', '-c', type=int, default=2,
                          help='ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜')
    gen_parser.add_argument('--output', '-o', type=str, default='',
                          help='ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ìƒëµ ì‹œ ì €ì¥í•˜ì§€ ì•ŠìŒ)')
    gen_parser.add_argument('--max-display', '-md', type=int, default=1000,
                          help='í‘œì‹œí•  ìµœëŒ€ ë¬¸ì ìˆ˜')
    
    # API í™•ì¸ ëª…ë ¹ì–´
    check_parser = subparsers.add_parser('check', help='API ê°€ìš©ì„± ë° ëª¨ë¸ ëª©ë¡ í™•ì¸')
    check_parser.add_argument('--model', '-m', type=str, default='gemma3:4b',
                            help='ì‚¬ìš©í•  Ollama ëª¨ë¸')
    
    args = parser.parse_args()
    
    if args.command == 'generate':
        return await generate_text(args)
    elif args.command == 'check':
        return await check_api(args)
    else:
        parser.print_help()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
